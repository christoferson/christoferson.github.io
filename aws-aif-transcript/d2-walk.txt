Let's get started with our third walkthrough question from task statement 2.2, understand the capabilities and limitations of generative AI for solving business problems. There are dropdown lists in the answer area, so that tells me this is an ordering or matching question. The question reads, "A company wants to use a large language model, or LLM, to improve business productivity. The company is unsure of the tradeoffs between the various approaches to LLM customization. Select and order the LLM customization approaches from the following list, from the least operational overhead to the most operational overhead. Each LLM customization approach should be selected one time. Select and order four." Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? Remember to pause and review each question to not only identify keywords, but also what the question is asking. A few keywords I identified are select and order, which helps me understand that this is an ordering item. Also, ensure you identify what you, the learner, is ordering. For this question, you are ordering LLM customization approaches from the least operational overhead to the most operational overhead. Now that we have examined the scenario and identified keywords, let's review the responses. The responses to order include the following. Train a fine-tuned model. Perform prompt engineering on an LLM. Select and use a specific LLM. And pre-train a new LLM. Think through your answers and pause the video if you need more time. Now let's talk about the correct order from the least operational overhead to the most operational overhead to customize an LLM. First, to select and use a specific LLM requires the least operational overhead, because you can access an already-trained model without the need for any customization. This approach requires the least operational overhead because you do not need to conduct model training or modification. Next, to perform prompt engineering on an LLM requires slightly more operational overhead. You need to design and refine prompts, or inputs, that are provided to the LLM to generate your desired outputs. With this approach, you do not need to modify the underlying model. Instead, this approach focuses on designing effective prompts that guide a model's response to inputs. Third, to train a fine-tuned model requires even more operational overhead. This approach requires you to train the pre-trained model on a new dataset for a specific task. Training a fine-tuned model typically requires you to set up training procedures, select and preprocess the data, and adjust model parameters. Finally, pre-training a new LLM requires the most operational overhead. This approach is the most resource-intensive option. You must collect a large and diverse dataset, set up a training environment, and run training processes for a series of time. Let's check and see if we match the correct functionality to the AWS service. That's all for this question. Be sure to take note of any knowledge gaps that you might have identified while exploring this question. Let's get started with our fourth walkthrough question. 


Let's get started with the fourth walkthrough question, which is from task statement 2.3. Describe AWS infrastructure and technologies for building generative AI applications. The question reads, a retail company wants to start testing Amazon Bedrock foundation models, FMs, for text generation and customer-facing natural language applications. How will the company be charged for using on-demand Amazon Bedrock models? Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords are testing Amazon Bedrock foundation models, FMs, and text generation. Remember, under task statement 2.3, we ask the question, what are the pricing models for LLMs? And we talked about the costs associated with hosting your own infrastructure, the cost of pay by token pricing models, and Amazon Bedrock on-demand mode. For this question, you need to understand how you will be charged for using on-demand Bedrock models. Now that we have examined the stem, identified keywords and review the requirements, let's explore the responses. Option A, by the number of API calls. Option B, by the number of input tokens that are processed. Option C, by a monthly subscription fee. Or option D, by the number of input tokens that are received and the number of output tokens that are generated. Pause the video if you need more time. Okay, let's evaluate the options. Option A is incorrect. On-demand Amazon Bedrock models are not charged by the number of API calls that are made. On-demand Amazon Bedrock models for text generation are charged by the number of input tokens that are received and the number of output tokens that are generated. Option B is incorrect. Embedding models are charged by the number of input tokens that are processed. However, on-demand Amazon Bedrock models are not charged that way. On-demand Amazon Bedrock models for text generation are charged by the number of input tokens that are received and the number of output tokens that are generated. Option C is incorrect. On-demand Amazon Bedrock models for text generation are charged by the number of input tokens that are received and the number of output tokens that are generated. Amazon Bedrock pricing is not charged by a monthly subscription fee, so that makes Option D correct. On-demand Amazon Bedrock models for text generation are charged by the number of input tokens that are received and the number of output tokens that are generated. You pay for what you use. You do not need to commit to long-term contracts. That's all for this question. Be sure to take note of any knowledge gaps that you may have identified while exploring this question. Now is your chance to practice and dive deeper on Domain 2 topics before continuing to Domain 3. If you are taking the enhanced course, you'll move on to bonus questions, flashcards, and a lab. Whether you're taking the standard or enhanced course, you'll see a list of additional resources to learn more about the topics covered. 