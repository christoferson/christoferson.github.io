Let's get started with a seventh walkthrough question, which is from task statement 4.1. Explain the development of AI systems that are ethical and fair. The question reads, "A company has a foundation model, FM, in Amazon Bedrock that provides answers to employee questions. The company wants to prevent inappropriate user input and model output. Which feature of Amazon Bedrock can the company use to meet these requirements?" Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords identified are foundation model, FM, in Amazon Bedrock, and you need to prevent inappropriate user input and model output. You need to choose a feature of Amazon Bedrock to meet these requirements. Now that we have examined the stem, identified keywords, and reviewed the requirements, let's explore the responses. Option A, FMs, option B, guardrails, option C, knowledge bases, or option D, agents. Pause the video if you need more time. Okay, let's evaluate the options. Option A is incorrect. FMs in Amazon Bedrock are models that have been trained on large amounts of data with many parameters. You can train an FM to avoid inappropriate or harmful information. However, the FM on its own does not prevent inappropriate user input or output. Option B is a possible correct answer. You can use Amazon Bedrock guardrails to prevent or filter inappropriate content from user input and output. You can customize Amazon Bedrock guardrails with policies to implement responsible AI policies. Remember to review all of the answer options to choose your best choice answer. Option C is incorrect. You can use Amazon Bedrock knowledge bases to provide contextual information based on the retrieval augmented generation, RAG, technique. You can use a knowledge base to create a RAG application that provides responses based on the information that is received. You cannot use Amazon Bedrock knowledge bases to prevent inappropriate user input or output. Option D is incorrect. You can use Amazon Bedrock agents to create generative AI applications that can accomplish complex tasks. Agents can orchestrate interactions between model components, such as the FM, data source, and user conversations. You cannot use Amazon Bedrock agents to prevent inappropriate user input or output. So that makes option B the correct answer. That's all for this question. Be sure to take note of any knowledge gaps that you have identified while exploring this question. Let's get started with our eighth walkthrough question. 

Let's get started with our eighth walkthrough question, which is from task statement 4.2. Recognize the importance of transparent and explainable models. The question reads, A company must ensure that it has a mechanism to observe the inner mechanics of a model. The company must understand exactly how the model generates a prediction. Which concept matches this description? Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords identified are, mechanism to observe the inner mechanics of a model to understand how the model generates a prediction. You need to choose a concept that matches this description. Now that we have examined the stem, identify the keywords and reviewed the requirements, let's explore the responses. Option A, Interpretability. Option B, Explainability. Option C, Guardrails. Or Option D, Model evaluation. Pause the video if you need more time. Okay, let's evaluate the options. Option A is a possible correct answer. Interpretability is the process of understanding the inner mechanics of a model. The goal is to explain how the model generates a prediction. To explain model behavior in human terms is not the goal. The intent is to be transparent about the model's inner mechanics. Option B is incorrect. Explainability is the process of explaining a model's behavior in human terms. Explainability does not focus on exactly how each model weight and input data contributes to the behavior of the model. Therefore, this concept does not focus on understanding the inner mechanics of a model. Option C is incorrect. Guardrails are policies or methods to secure ML and AI resources. You can implement guardrails to secure resources that are related to ML environments. You can use guardrails to filter harmful or inappropriate user input or model output. Guardrails are not related to understanding the inner mechanics of a model. Option D is incorrect. Model evaluation is a stage in the ML development lifecycle, where you test the model to determine how well the model performs. You typically evaluate a model after the model has been trained. You can evaluate for performance and success metrics. Model evaluation is not related to understanding the inner mechanics of a model. So, that makes option A the correct answer. That's all for this question. Be sure to take note of any knowledge gaps that you might have identified while exploring this question. Now is your chance to practice and dive deeper on Domain 4 topics before continuing to Domain 5. If you are taking the enhanced course, you'll move on to bonus questions, flashcards, and a lab. Whether you're taking the standard or enhanced course, you'll see a list of additional resources to learn more about the topics covered. 