Let's get started with the second task statement from Domain 3 which is to choose effective prompt engineering techniques. This task statement is split into two lessons. We have talked about prompt engineering a few times already, especially in task statement 2.1, but let's step back and ask a question. What is a prompt? Prompts are a specific set of inputs provided by you the user. They guide LLMs to generate an appropriate response or output for a given task or instruction. Let's dive deeper into prompts and talk about the components of a prompt. A prompt contains components that you want the LLM to perform such as the task or instruction. You might also need the context of that task or instruction and the input text that you want for the response or output. Depending on your use case, the availability of the data and the task, your prompt should combine one or more of these components. Here is another question. What is few-shot prompting? It is when you provide a few examples to help the LLM models better perform and calibrate their output to meet your expectations. What is zero-shot prompting? It is a sentiment classification prompt with no examples provided to the prompt. You can also use a prompt template. Templates might include instructions, few-shot examples, and specific content and questions for different use cases. And if you have more complex tasks, then you might use chain-of-thought prompting to break down the reasoning process into intermediate steps. This kind of prompting can improve the quality and coherence of the final output. A more advanced technique is prompt tuning, where the actual prompt text is replaced with a continuous embedding backer that is optimized during training. This technique helps the prompt to be fine-tuned for a specific task. At the same time, it keeps the rest of the model parameters frozen, which can be more efficient than full fine-tuning. I like to think that prompt engineering is the way we talk and communicate with an LLM. AWS defines it as the practice of crafting and optimizing input prompts. It selects appropriate words, phrases, sentences, punctuation, and separator characters to effectively use LLMs for a wide variety of applications. The quality of prompts that you provide to LLMs can impact the quality of their responses. For the exam, ensure that you understand guidelines that you provide with all the necessary information and tools. These tools will help you find the best possible prompt format for your use case when using LLMs on Amazon Bedrock. Consider this. Your prompt engineering strategy for your use case depends on both your task and the data. What are common tasks supported by LLMs on Amazon Bedrock? A few are classification, question and answer with and without context, summarization, open-ended text generation, code generation, math, and reasoning or logical thinking. Let's jump back to prompt engineering and talk about model latent space. Lots of different prompting strategies are available that you can use. Latent space is the encoded knowledge of language in a large language model. It's the stored patterns of data that capture relationships and, when prompted, reconstruct language from those patterns. For example, let's say that you want to build a scuba vacation model so that AI can recommend different scuba vacations to your customers. You could input data about scuba diving vacation destinations and the specific dives to get a database of scuba diving destinations. So your model might be trained on the destination, depth of dives, visibility, average water temperature, average weather, temperature, and more, and you would end up with a database of scuba vacations. If someone inputs a prompt about wanting to snorkel with manatees, then your model would be able to refer to the catalog of statistics. They could query it for recommendations. That database of statistics is the latent space. It's an understanding of patterns that the model can use to generate new outputs, and it's a statistical database. I'm going to pause this lesson here, and in the next lesson we will continue with task statement 3.2. 

Let's continue with task statement 3.2, and start off by asking a question from the end of the last lesson. What does latent space have to do with prompting a language model? All language models are trained from large text databases, such as RefinedWeb, Common Crawl, StarCoder data, BookCorpus, Wikipedia, C4, and more. Those big databases contain varying amounts of knowledge on significant number of topics, and the quality of knowledge varies. Just because it's in Wikipedia doesn't make it correct or incorrect. When you write a prompt for a language model, that prompt is ingested by the model and refers to its latent space against its database of statistics. It returns a pile of statistics that then get assembled as words. If you prompt a language model and get dissatisfactory or a negative response, then your prompt might be insufficient for the model. But it's also possible, especially if your model is smaller, that the model's latent space might not have enough information about the topic of your prompt. That situation can cause the model to hallucinate a model that doesn't know the exact specifics of a prompt because the knowledge isn't in its latent space will choose the closest match. This result might be interpreted as a mistake, but the model is actually functioning correctly. That is how the model works. For example, in the past, LLMs were trained with small amounts of data and not fine-tuned to follow instructions based on millions of examples. You could ask a model, who was the first person to dive below 25 feet when dinosaurs walked the earth? You know from history and our own reasoning that probably no one was diving during that time. But models don't reason. They generate a sentence one word at a time, choosing a word from a pool based on its conditional probability, given its surrounding context. And a key part of advanced prompt engineering is knowing the limitations of a language model's latent space. You must assess its latent space for a given topic to know what it knows about that topic before you can start constructing prompts. Otherwise, you'll prompt it for things that it doesn't know well, and the answers you will get back will have a high chance of hallucination. They will be statistically correct, but factually wrong from the reasoning standpoint. Prompt engineering is an important skill for getting the most out of generative AI models. It involves designing and refining the input prompts that are fed into the model to guide it towards producing the desired outputs. Effective prompt engineering can make the difference between mediocre and outstanding results, especially with large language models that have broad knowledge but need direction to apply it. Prompt engineering has several key techniques. First, be specific and provide clear instructions or specifications for the task at hand. For example, include the desired format, examples, comparison, style, tone, output length, and detailed context. Second, include examples of the desired behavior and direction, such as sample texts, data formats, templates, code, graphs, charts, and more. Third, experiment and use an iterative process to test prompts and understand how the modifications alter the responses. Fourth, know the strengths and weaknesses of your model. Fifth, balance simplicity and complexity in your prompts to avoid vague, unrelated, or unexpected answers. Six, specifically for your prompt engineers, use multiple comments to offer more context without cluttering your prompt. Seventh, add guardrails. And this subject leads us to the last objective for this task statement, which is to identify potential risks and limitations of prompt engineering. For example, consider exposure, poisoning, hijacking, and jailbreaking. Guardrails provide safety and privacy controls to manage interactions in your generative AI applications. You can define topics within the context of your application that are not desirable. You can set words to be blocked. You can configure thresholds to filter across categories that might be harmful and prompt attacks such as jailbreak and prompt injections. You can also filter inputs that might contain sensitive data. Here is a question, what is prompt injection? It describes attacks of prompt manipulation. One example involves a trusted prompt, usually created by the developer of an LLM. In this case, it occurs along with an untrusted input that is created by a user to produce malicious, undesired, or elicit response. When an attacker tries to bypass the guardrails that you have established, this is called jailbreaking. In this case, it is different because jailbreaking targets the safety measures put in place such as guardrails. Hijacking is an attempt to change or manipulate the original prompt with new instructions. Poisoning is another risk of prompt engineering where harmful instructions are embedded in messages, emails, web pages, and more. AWS services such as Amazon Bedrock and Amazon Titan offer pre-trained language models that can be customized and controlled through prompt engineering. They provide APIs and tools for constructing and refining prompts, along with monitoring and analyzing the resulting outputs. You can use these services to build applications that generate high-quality text for use cases such as content creation summarization, question answering and chatbots. Alright, let's get started with task statement 3.3 in the next lesson. 

