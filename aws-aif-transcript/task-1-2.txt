Now let's review the second task statement from Domain 1, which is to identify practical use cases for AI. This task statement is split into five lessons. Let's start by reviewing some use cases and scenarios for which AI and ML should be considered. Unlike humans, AI technology can work all day every day without decreasing rates of performance. You can use AI to focus on repetitive, tedious tasks that employees struggle with or find boring. AI can decrease employees' workloads while streamlining all business-related operations. AI technology can use ML and deep learning networks to solve complex problems with humanlike intelligence. AI is a great choice for solving complex problems that involve analyzing vast amounts of data at high velocity that humans could not be able to do. Because AI is very good at recognizing patterns, it is great at detecting deviations in patterns and uncovering fraud. It helps reduce waste by forecasting the demand for products or resources. Companies can make better choices, become more efficient, and better address their customer needs. We have seen that there are a lot of useful applications for artificial intelligence. However, it isn't the solution to every problem. Let's go over some use cases where AI is not the best option. Training AI with machine learning consumes vast resources. Processing power can be costly, and models might need to be re-trained frequently. Before embarking on a costly AI project, you should be certain that the benefits to the business will outweigh the cost of the AI solution. For example, have targets for the dollar amounts of fraud and waste reduction, and then estimate the cost to build a model to meet those targets. If the costs exceed the savings, then you probably shouldn't proceed. AI models can be used to make decisions that impact customers. For example, when someone applies for a loan, it's important to make sure that models are trustworthy and understandable. However, complex neural networks model the human brain. You cannot fully understand how and why the inner mechanics impact the prediction, which is known as models' interpretability. Complex models generally present a tradeoff of compatibility compared with interpretability. If these are business or compliance requirements for complete transparency, then less complex models will need to be used, which will usually result in lower performance. An alternative is to use a rule-based system that doesn't require AI. For example, a rule might be that people with a credit score about 750 should be automatically approved for a loan of $10,000 or less. If a software application always produces the same output for the same input, it is said to be deterministic. A rule-based application is deterministic unless someone changes the rules. Machine learning models, however, are probabilistic. That is, they determine the likelihood of something. They learn and adapt over time and incorporate randomness into their approach. As a result, identical sets of input values will result in a variety of results that aren't consistent. If determinism is necessary, then a rule-based system is a better option. I'm going to pause this lesson here, and in the next lesson we will continue talking about task statement 1.2. 

Let's continue with task statement 1.2, which is to identify practical use cases for AI, and talk about different ML problem types and how to identify them. If your dataset consists of features or attributes as inputs with labeled target values as outputs, then you have a supervised learning problem. In this type of problem, you train your model with data containing known inputs and outputs. If your target values are categorical, for example, one or more discrete values, then you have a classification problem. If these target values you're trying to predict are mathematically continuous, then you have a regression problem. If your dataset consists of features or attributes as inputs that do not contain labels or target values, then you have an unsupervised learning problem. In this type of problem, the output must be predicted based on the pattern discovered in the input data. The goal in unsupervised learning problems is to discover patterns, such as groupings, within the data. When your data needs to be separated into discrete groups, you have a clustering problem. If you are seeking to spot outliers in your data, then you have an anomaly detection problem. Classification problems are normally distinguished as binary or multiclass. Binary classification assigns an input to one of two predefined and mutually exclusive classes based on its attributes. A medical diagnosis for whether an individual has a disease or not, based on the results of diagnostic tests is an example of binary classification. We also saw an example of binary classification earlier with our fish or not fish example. Multiclass classification assigns an input to one of several classes based on the input attributes. An example is the prediction of the topic most relevant to a tax document. A document might be classified as being about religion, politics, or finance, or as about one of several other predefined topic classes. If we extend our fish example to identify multiple categories of sea creatures, then we have made it into a multiclass classification problem. When your target values are mathematically continuous, then you have a regression problem. Regression estimates the value of dependent target variable based on one or more other variables, or attributes that are correlated with it. Linear regression is when there is a direct linear relationship between the inputs and output. Simple linear regression uses a single independent variable, such as weight, to predict someone's height. If we have multiple independent variables, such as weight and age, then we have a multiple linear regression problem. Another example is the prediction of house prices using features like the number of bathrooms, bedrooms, the square footage of the house and garden. Regression analysis can create a model that takes one or more of these features as an input to predict the price of a house. Logistic regression is used to measure the probability of an event occurring. The prediction is a value between zero and one, where zero indicates an event that is unlikely to happen, and one indicates a maximum likelihood that it will happen. Logistic equations use logarithmic functions to compute the regression line. It can use one or multiple independent variables. An example is predicting if a person will get heart disease based on body mass index, BMI, smoking status, and genetic predisposition. Another example would be predicting whether a financial transaction is fraud when there are transactions labeled as fraud and not fraud used for training. Both logistic regression and linear regression require a significant amount of labeled data for the models to become accurate in predictions. Cluster analysis is a class of techniques that are used to classify data objects into groups, called clusters. It attempts to find discrete groupings within data. Members of a group are similar as possible to one another, and as different as possible from members of other groups. You define the features or attributes that you want the algorithm to use to determine similarity. Then you select a distance function to measure similarity and specify the number of clusters, or groups, you want for the analysis. An example of clustering is segmenting customers into groups by purchase history or clickstream activity. Anomaly detection is the identification of rare items, events, or observations in the data, which raise suspicions, because they differ significantly from the rest of the data. The identification of anomalous items can be used, for example, to detect failed sensors or medical errors. I'm going to pause this lesson here. And in the next lesson we will continue talking about task statement 1.2. 


Let's continue with task statement 1.2, which is to identify practical use cases for AI. For many common use cases, it isn't necessary to build and train your own custom model. AWS offers several pre-trained AI services that are accessible through APIs. Before embarking on the effort and cost to build a custom model, you should investigate whether an existing service for your use case already exists. Amazon Rekognition is a pre-trained deep learning service for computer vision. It meets the needs of several common computer vision use cases without requiring customers to train their own models. It works with both images and videos, including streaming videos. One of its capabilities is face recognition. This service can be used to verify someone's identity by comparing their image with a reference image, like an employee's badge or a driver's license. One of the things you can do with Amazon Rekognition is give it a collection of labeled images of faces. For example, a company's employees. It'll automatically recognize and find them in images or stored in streaming videos. Amazon Rekognition can detect and label objects, which can be used to make an image or video library searchable. It can also be used in security system to detect and identify objects in real-time streaming video and send out alerts. You can get Amazon Rekognition to recognize custom or proprietary objects by giving it some labeled images to learn from. It can also add labels for any text it sees like street signs. Typically, companies that allow users to upload content to their application need to employ people to screen content before letting it get published. Amazon Rekognition can detect and filter explicit, inappropriate, and violent content in images and videos, and also flag content that should be reviewed by humans. Here we can see Amazon Rekognition facial recognition in action. We give it the reference image of a face and it can identify that person in other images and videos. Here, it found a match with 99.8% confidence. It also detected other faces in the photo and shows that there are not a match. More than just optical character recognition, Amazon Textract extracts text, handwriting, forms, and tabular data from scanned documents. Amazon Comprehend is a natural language processing service that helps discover insights and relationships in text. A common use case to classify the sentiment of customer feedback. For example, AWS uses Amazon Comprehend to analyze the comments left on Certification exams. Frequently, Amazon Textract and Amazon Comprehend are used together. Content extracted by Amazon Textract can be given to Amazon Comprehend for sentiment analysis. A common ML use case is detecting personal identifiable information, PII, in text. If you were collecting data for training an ML model to detect spam emails, you would want to be able to find PII and remove it from training data. Amazon Comprehend is pre-trained to find PII. In this example, Amazon Comprehend found a person's name, address, email, phone, and credit card number inside an email. Notice it also returns a confidence score. Suppose you are creating a job to remove PII from the data. You would set a threshold for the minimum confidence level over which you would automatically remove the associated entity. Amazon Lex helps build voice and text interfaces to engage with customers. It uses the same technology that powers Amazon Alexa devices. Some common use cases are customer service chatbots and interactive voice response systems, that route calls to the proper agent in a call center. Amazon Transcribe is an automatic speech recognition service that supports over 100 languages. Transcribe is designed to process live and recorded audio or video input to provide high quality transcriptions for search and analysis. A common use case is to caption streaming audio in real time. I'm going to pause this lesson here and in the next lesson, we will continue talking about task statement 1.2. 


Let's continue with task statement 1.2, which is to identify practical use cases for AI and pick up right where we left. Amazon Polly turns text into natural-sounding speech in dozens of languages. It uses deep learning technologies to synthesize human speech. Common use cases include converting articles to speech and prompting callers in interactive voice response systems. The ability of AI to create natural-sounding voices is enabling companies to increase engagement with their products and be more accessible to visually impaired customers. For example, media companies like Washington Post and USA Today use Amazon Polly to create audio from breaking news and headlines. Amazon Kendra uses machine learning to perform an intelligent search of enterprise systems to quickly find content. It uses natural language processing to understand questions like, "How do I connect my Echo Plus to my network?" It responds with results based on an intelligent understanding of the question. Amazon Personalize allows businesses to automatically generate personalized recommendations for their customers in industries such as retail, media and, entertainment. For example, an ecommerce app can include a section called "you might also like" with personalized product recommendations to customers who will likely be interested in those products. Businesses can also use Amazon Personalize to run more effective marketing campaigns by segmenting customers according to their preferences. Amazon Translate fluently translates text between 75 different languages. It is built on a neural network that considers the entire context of the source sentence and the translation it has generated so far. It uses this information to create more accurate and fluent translations. One use case is real-time translation in an online chat application. Amazon Forecast is an AI service for time series forecasting. By providing Amazon Forecast with historical time series data, you can predict future points in the series. Time series forecasting is useful in multiple domains, including retail, financial planning, supply chain, and healthcare. For example, you can use it to project sales and manage inventory levels. Amazon Fraud Detector helps to identify potentially fraudulent online activities such as online payment fraud and creation of fake accounts. It features pre-trained data models to detect fraud in online transactions, product reviews, checkout and payments, new accounts, and account takeovers. Amazon Bedrock is a fully managed service to build generative AI applications on AWS. Amazon Bedrock lets you choose from high performing foundation models trained by Amazon, Meta, and leading AI startups. You can customize a foundation model by providing your own training data or creating a knowledge base for the model to query. When a generative AI model calls an external knowledge system to retrieve information outside its training data, this is called Retrieval Augmented Generation or RAG for short. This example uses the Titan Image Generator foundation model from Amazon to generate an image in response to a prompt. Use the Amazon SageMaker family of services when you need more customized machine learning models or workflows that go beyond the prebuilt functionalities offered by the core AI services. Amazon SageMaker provides machine learning capabilities for data scientists and developers to prepare, build, train, and deploy high-quality ML models efficiently. It comprises several services that are optimized for building and training custom machine learning models. These include data preparation and labeling, large-scale parallel training on multiple instances or GPU clusters, model deployment, and real-time inference endpoints To accelerate the development process, SageMaker offers pre-trained models that you can use as a starting point and reduce the resources needed for data preparation and model training. I'm going to pause this lesson here, and in the next lesson, we will continue talking about task statement 1.2. 


Let's continue with task statement 1.2, which is to identify practical use cases for AI and talk about real-world AI applications. MasterCard International Inc. is the second largest credit card network by purchasing volume. Each transaction that comes through their network is instantly given a score to determine the probability of fraud using AI. By training a fraud detection model with SageMaker, MasterCard has been able to triple the number of detected fraudulent transactions. It also has reduced the number of false positives by a factor of 10. In 2024, MasterCard announced that they have now added generative AI to help improve fraud detection by 20% on average. The large language model is given the customer's transaction history as a prompt. The model predicts if the business involved in the transaction is a place the customer would likely go and factors this in the score. DoorDash needed to replace an outdated interactive voice response, IVR system, which required customers to navigate prompts using touch tones. Customers were frustrated and often just pressed 0 to speak to a live agent, who had to route them to someone else. DoorDash implemented a new system that uses Amazon Lex natural language processing to let customers just speak instead of pressing buttons. This improved the customer experience, decreased hold times, and increased self-service adoption. Laredo Petroleum operates more than 1,300 oil and gas wells in west Texas. The wells use pressure, temperature, and flow rate sensors to measure important operational parameters. Laredo implemented a data streaming solution on AWS and built ML models using Amazon SageMaker to monitor the data in real time. This allows their operations teams to know where to focus their maintenance efforts and avoid potential issues. Proactive monitoring on AWS helps them quickly identify and remediate issues that could result in flaring or venting of natural gas. This is an important way of reducing environmental impact. They also deployed models to detect leaks in their storage tanks and corridor lines. Booking.com is a travel marketplace for hotels, flights, rental cars, and attractions. Booking.com provides over 28 million accommodation listings and flight locations in over 54 countries, managing over 150 terabytes of data. They use Amazon SageMaker to build machine learning models to provide booking recommendations. For the best customer experience, they created an application called the AI Trip Planner, which uses generative AI to engage with customers using natural language. As soon as the AI Trip Planner understands what the customer is looking for, it calls the booking recommendation API and retrieves customer reviews before making recommendations to the customer. This is an example of retrieval augmented generation that is what makes generative AI models more accurate and current with their responses. Pinterest is a visual discovery engine that hosts billions of images for over 450 million users to explore, save, and share as pins to personalized digital inspiration boards. The Pinterest app offers a feature called Pinterest Lens, which lets the user take a picture of an object. Then, Pinterest will immediately show them similar items for sale, linking them directly to the product in online catalogs. Pinterest maintains a massive collection of labeled product images in Amazon S3 and frequently re-trains their machine learning model to learn new objects. They label the images using Amazon Mechanical Turk and SageMaker Ground Truth. AffordableTours.com is one of the largest travel sellers for escorted tours, cruises, river cruises, and active vacations in the United States. They differentiate themselves in the travel industry by providing live consultants on the phone while still offering travel packages at low cost. Before asking machine learning, the company often had too many or too few customer service agents to handle the customer call volumes. This would lead to inconsistent customer experiences, increased missed call rates, and wasted operational expense. Affordable Tours now uses Amazon Forecast to generate time series forecasts. This enables them to better predict call volumes and have the right number of agents available. Since using Amazon Forecast, their missed calls rates have improved by 20%. Amazon Forecast supports a number of built-in forecast algorithms, including neural networks and more traditional statistical algorithms. In the next lesson, we will get started with task statement 1.3. 


