Let's get started with the first task statement from Domain 2, which is to explain the basic concepts of generative AI. This task statement is split into five lessons. Let's pick up from task statement 1.1 and talk about the concepts of generative AI. What is generative AI? Generative AI is a subset of deep learning. Like deep learning, generative AI is a multipurpose technology that helps to generate new original content rather than finding or classifying existing content. Generative AI focuses on creating new content, such as text, images, audio, video, and even code. Remember that AI is focused on classifying or making predictions based on data that you input to make predictions. The models of generative AI learn patterns and representations from a large amount of training data. They then use that knowledge to generate outputs that resemble the training data. In Domain 1, we talked about how generative AI uses foundation models that have been trained on vast amounts of data. They look for statistical patterns in modalities, such as natural language and images. These foundation models are very large and complex neural network models with billions of parameters that are learned during the training phase or pre-training. The size of the models have increased by increasing the number of trainable parameters. And the more parameters a model has, the more memory it has, so the model can perform more advanced tasks. You can either use these models as they are or apply fine-tuning techniques to adopt them to your specific use case. Models are at the core of generative AI. And again, models are built with neural networks, system resources, data, and prompts, all working together. When you build a model, you train it with the knowledge it needs to generate unique output based on what it has learned. The models take the data or text that you input and provide an output. This output is a guess of what the next word or token should be. Let's dive deeper into how the models work. The current core element of generative AI is the transformer network. Transformers were introduced in a 2017 paper called "Attention Is All You Need." Some LLMs, such as ChatGPT, are built on the transformer architecture. These LLMs are pre-trained on massive amounts of the text data from the internet. They can use this pre-training process to build up a broad knowledge base. And they can be fine-tuned for specific tasks with relatively little additional data. Generative AI can be used for multiple content tasks and use cases, which we'll cover in a few minutes. Generative AI large language models can take natural language or human-written instructions and perform those tasks as a human would. For the exam, ensure you understand generative AI concepts, such as prompt, inference, completion, context window, tokens, LLMs' vocabulary, tokenizer, prompt engineering, and more. Most ML models, AI models, and generative AI models rely on statistics and linear algebra for their computations, including probability modeling, loss function, and matrix multiplication. These calculations are used in deep learning operations because machine learning prefers to work with numbers and not raw text, images, or videos. Remember, in Domain 1, we used a prompt to generate a song and we included lyrics with a prompt. You might encounter situations where the model doesn't produce the outcome that you want on the first try, and you can use prompt engineering to understand and apply generative models to your tasks and use cases. One strategy you can use to get the model to produce better completions is to include examples of the task that you want the model to carry out. These examples can be incorporated inside the prompt. Providing examples inside the context window is called in-context learning. With in-context learning, you can help LLMs learn more about the task being asked by including examples or additional data in the prompt. The input that you sent into your generative model is called the prompt. A text prompt is used for LLMs and other modalities, such as images, videos, and more. The prompt is passed into the model during the inference time to generate an output or completion. The inference configuration parameters influence the model's completion to the prompt. So the prompt consists of instruction and content, but you can add in-context learning with few-shot, zero-shot, and one-shot inference. I'm going to pause this lesson. And in the next lesson, we will continue with task statement 2.1.

Let's continue with task statement 2.1, which is to explain basic concepts of generative AI. Let's step back to the beginning of our AI project and the data selection stage. There are publicly available pre-trained foundation models such as SageMaker JumpStart. We'll talk about them more in task statement 2.3. Remember that every language-based generative AI model has a tokenizer that converts human text into a vector that contains token IDs or input IDs. Each input ID represents a token in the model's vocabulary. Let's pause and ask a question. What is a vector? A vector is an ordered list of numbers that represent features or attributes of some entity or concept. In the context of generative AI, vectors might represent words, phrases, sentences, or other units. The power of vector representations is in the ability to encode related relationships between the items and capture meaningful associations, analogies, and hierarchies. For example, the vector difference between a manatee and dugong might be similar to the difference between a great hammerhead shark and a hammerhead shark. Let's dive deeper here. We said that a vector is a list of numbers, but it also indicates where that list of numbers is located within a space. I like to think of a vector as an Excel spreadsheet, so the list of numbers or vector would indicate a specific location in a particular space or in a vector database. In the same way, the column and row number in the Excel spreadsheet indicate a certain cell in that spreadsheet. The model uses the tokenizer to convert the input text into a vector. The embedding vector is this set of input IDs necessary to obtain each token's high dimensional representation. Embedding vectors are also called embeddings. Embeddings are a numerical vectorized representation of any entity. Embeddings capture the semantic meaning of tokens such as text, image, video, or audio. For example, the vectors encode the meaning and context of tokens within a large body of text. In this way, the model can statistically represent and understand human language. The closer the tokens are to each other in the vector space, the more similar they are in the semantic meaning. In this way, the model can generate text based on relationships between these vectors. Embeddings are passed to this self-attention layers, which are another key component of the transformer. At the beginning of this lesson, I said that current core element of generative AI is the transformer network. At its core, generative AI is a machine learning technique that creates content that mimics what a human could do. An innovation of transformers is this self-attention mechanism. This mechanism helps the model to weigh the importance of different parts of the input when generating each output token. As a result, the model can capture long-range dependencies and contextual relationships that were difficult to learn with previous architectures, such as recurrent neural networks, or RNNs. Self-attention works by computing a set of query, key and value vectors for each input token. Then it uses the dot products between these vectors to determine the attention weights. The output for each token is a weighted sum of the value vectors where the weights come from the attention scores. This process is repeated in multiple layers so that the model can build up complex representations of the input. Transformers also introduce the concept of position embeddings, which encode the relative position of each token in the sequence. They help the model to distinguish between identical tokens that appear in different positions, which is important for understanding sentence structure and word order. During the model inference, the transformer aims to help the model generate the completion to an input prompt. The transformer model uses self-attention to compute representations of input sequences, which helps the model to capture long-term dependencies and parallelized computation. I have added a link to an article, "Attention is all you need." The authors demonstrate that their model achieves excellent performance on several machine translation tasks and outperforms previous models that rely on RNNs or convolutional neural networks, CNNs. The article talks about how the transformer architecture consists of an encoder and a decoder, each of which has several layers. Each layer consists of two sublayers. The transformer model uses residual connections and layer normalization to facilitate training and prevent overfitting. In addition, the authors introduce the positional encoding scheme that encodes the position of each token in the input sequence. In this way, it helps the model to capture the order of the sequence without the need for recurrent or convolutional operations. During the model pre-training and fine-tuning, the transformer is helping the model gain contextual understanding of the language from the input training or tuning. You don't need to understand the low-level details of the transformer architecture, but it helps to understand that the complex implementation has occurred behind the scenes. And again, I added flashcards to cover more concepts such as input, context window, embedding layer, encoder, self-attention, decoder, soft max output, and more. I'm going to pause this lesson, and in the next lesson, we will continue with task statement 2.1. 

Let's continue with task statement 2.1. Researchers have found that the larger a model is, the more likely it is to work without additional in-context learning or further training. Because the model's capability increases with size, it has supported the development of larger and larger models. The result is the introduction of highly scalable transformer architecture, access to enormous amounts of data for training, and development of more powerful compute resources. You might be asking, can we just keep adding parameters to increase performance and make models smarter? Well, training these large models is difficult and expensive. It might be hard to continuously train larger and larger models. Another interesting question is, where might this model growth lead? Let's step back and repeat a few key points. LLMs encode a deep statistical representation of a language. This understanding is developed during the pre-training phase when the model learns from vast amounts of unstructured data. This data can be gigabytes, terabytes, and even petabytes of text. This data is pulled from many sources, including the internet and larger texts that have been assembled specifically for training language models. In this self-supervised learning step, the model internalizes the patterns and structures in the language. These patterns then help the model to complete its training objective, which depends on the architecture of the model. During pre-training, the model weights get updated to minimize the loss of the training objective. The encoder generates an embedding or vector representation for each token. Pre-training also requires a large amount of compute and the use of graphic processing units, GPUs. Remember that when you collect training data from public sites such as the internet, you'll need to process the data. The processing is to increase quality, address bias, or remove other harmful content. From what I could find, 1% to 3% of tokens are used for pre-training after the data quality curation step. This estimate is important to include when you are trying to determine how much data you need to collect to pre-train your model. Generative AI can be unimodal or multimodal. Unimodal models work with one data modality. LLMs are an example of unimodal generative AI because the input and the output, or completion, are text. Multimodal is adding another modality such as image, video, or audio. Multimodal models can understand diverse data sources and can provide more robust forecasts. Multimodal generative AI use cases are marketing, image captioning, product design, customer service, chatbots, and avatars. Multimodal models and diffusion models are two important classes of generative AI that go beyond text-only applications. Multimodal models can process and generate multiple types of data, but they can also do this type of operation in combination with each other. This collaborative capability adds cross-model reasoning, translation, search, and creation that more closely mirrors human intelligence. What are some examples of multimodal tasks? These are image captioning, where the model is generating text descriptions of images, visual question answering, where the model answers questions about image content. Another example is text to image synthesis, which is generating images from textual descriptions. Models such as DALL-E, Stable Diffusion, and Midjourney can create realistic and diverse images from natural language prompts. There are also diffusion models, which support a variety of tasks for multimodal models such as image generation, upscaling, and inpainting. Diffusion models are a class of generative models that learn to reverse a gradual noising process. Diffusion-based architectures offer a higher degree of control in quality and diversity of images generated. For the exam, ensure you understand the three main components, forward diffusion, reverse diffusion, and stable diffusion. The idea is to start with random noise and iteratively de-noise it to produce a coherent output, such as high-quality image or audio clip. The model is trying to predict the noise that was added at each step conditioned on the partially denoised output from the previous step. Stable diffusion differs from many other image generation models. In principle, diffusion models use Gaussian noise to encode an image, then they use a noise predictor together with a reverse diffusion process to recreate the image. Stable diffusion doesn't use the pixel space of the image. Instead, it uses a reduced definition latent space. You can easily generate images from text using stable diffusion models through Amazon SageMaker JumpStart. What are advantages that diffusion models have over other generative approaches, such as generative adversarial networks and variational autoencoders? Diffusion models tend to produce higher quality outputs with more diversity and consistency, and they're more stable and easier to train. Some examples of diffusion models are Stable Diffusion for image generation, Whisper for speech recognition and translation, and AudioLM for audio generation. AWS provides a range of services and tools for building and deploying multimodal and diffusion models. For example, SageMaker supports deep learning frameworks such as TensorFlow and PyTorch, which have prebuilt modules for working with multimodal data. AWS also offers pre-trained models like Stable Diffusion that can be fine-tuned and deployed with just a few lines of code. All right, let's pause here, and in the next lesson we will continue with task statement 2.1. 

Let's continue with task statement 2.1, and talk about generative tasks and use cases. Large language models, LLMs, are a type of generative AI that can be applied to many different problem domains or tasks without being fine-tuned. What are the main use cases for generative AI models? First, generative AI generates text, and might be useful for writing or rewriting pieces of text to adapt to different audiences. For example, you could adapt or convert a technical document that is written about the design and specifications for a scuba diving buoyancy device. To be more specific, you could have it use less technical terms for people who are beginning their scuba diving certifications. Generative AI is also good at text summarization. You can give it a relatively long piece of information, and have it generate a short output, while retaining the main idea. Examples could include summarizing technical documentation, financial reports, legal documents, news articles, and more. AWS provides a range of services and tools for building generative AI applications for content creation. For example, Amazon Bedrock and Amazon Titan offer pre-trained models for text, image, audio generation, that could be fine-tuned for specific use cases. SageMaker and Amazon Q Developer, formerly known as Amazon CodeWhisper, support code generation and completion. Amazon Nimble Studio and Amazon Sumerian offer virtual production and 3D content creation. There are more examples of other use cases, such as information extraction, question answering, classification, identifying harmful content, translation, recommendation engines, personalized marketing and ads, chatbots, customer service agents, and search. Another use case is the use of generative AI as a developer tool for source code generation. Code generation is an application of generative AI to accelerate software development. Models can generate functional code snippets, and even entire programs from natural language descriptions or examples, to help automate routine programming tasks, suggest code completions, and even translate code between different languages. Amazon Q Developer generates real-time code suggestions, that range from snippets to full functions, based on your comments and existing code. AWS handles the underlying infrastructure, data management, model training, and inference. You can focus on your specific use cases and applications. For the exam, ensure that you understand that various architectures exist, such as generative adversarial networks, GANs, variational autoencoders, VAEs, and transformers. Each architecture has unique advantages and limitations, so evaluate the objective and dataset before selecting the appropriate one. I'm going to pause this lesson here, and in the next lesson, we will continue with task statement 2.1. 

Let's wrap up this task statement and walk through a generative AI project lifecycle. This framework maps out the tasks required to take your project from conception to launch. The stages are: identify use case; experiment and select; adapt, align, and augment; evaluate; deploy and iterate; and monitor. The first stage of the AI cycle is to define objectives, collect data, process data, select your model, and train and develop your model. The next stage is to develop your model by using feature engineering, building, testing, validating, optimizing, and scaling. Last is the stage to deploy and maintain, which includes model evaluation, deployment, feedback, updates, security, and scalability. The exam guide lists the foundation model lifecycle as data selection, model selection, pre-training, fine-tuning, evaluation, deployment, and feedback. The most important step in any project is to define the scope as accurately and narrowly as you can. You should think about what function the LLM will have in your specific application. Do you need the model to be able to carry out many different tasks, including long-form text generation, or is the task much more specific, like named entity recognition, so that your model only needs to be good at one thing. Getting specific about what you need your model to do, You can save time and perhaps more importantly, compute costs. As soon as you are happy and you have scoped your model requirements enough to begin development, you are ready to get started. Your first decision will be whether to train your own model from scratch or work with an existing base model. The next step is to assess the model's performance and complete any additional training if needed for your application. Prompt engineering can sometimes be good to get your model to perform well. Therefore, you'll likely start by trying in-context learning, using examples suited to your task and your use case. In some cases, however, the model might not perform as well as you need, even with one- or few-shot inferences. In that case, you can try fine-tuning your model. This process is a supervised learning process. As models become more capable, it's becoming increasingly important to ensure that they behave well in a way that aligns with human preferences in deployment. So include an additional fine-tuning technique called reinforcement learning from human feedback, which can help make sure that your model behaves well. An important aspect of all of these techniques is evaluation. Also, try different metrics and benchmarks that can determine how well your model is performing and how well it aligns with your preferences. This stage of development with adapting and aligning is usually highly iterative. I prefer to start by trying prompt engineering and evaluating the outputs, and then using fine-tuning to improve performance. And of course, always revisit and evaluate the prompt engineering to get the performance that you need. Finally, when you've got a model that is meeting your performance needs and is well aligned, you can deploy it. Your model can be deployed to your infrastructure and integrated with your application. Ensure that you are optimizing your model for deployment and your compute resources too, and that application provides great experience to users. The last but very important step is to consider any additional infrastructure that your application will require to work. Remember that some fundamental limitations of LLMs can be difficult to overcome through training alone. These issues might include hallucinations, inventing information when the answer is unknown, and the limited ability with complex reasoning and mathematics. Okay, let's continue with task statement 2.2 in the next lesson. 

