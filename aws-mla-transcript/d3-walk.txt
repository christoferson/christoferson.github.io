Let's get started with our fifth walkthrough question, which is from task statement 3.1. Select deployment infrastructure based on existing architecture and requirements. The question reads, "A company wants to deploy multiple models based on the scikit-learn framework by using Amazon SageMaker. The models will need to serve traffic with low latency. Which deployment option will meet these requirements most cost-effectively?" Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords are, deploy multiple models of scikit-learn framework, Amazon SageMaker, and serve traffic with low latency. And you need to choose a deployment option to meet the requirements of both low latency and the most cost-effective. Now that we have examined the stem, identified keywords, and reviewed the requirements, let's explore the responses. Option A, use Amazon SageMaker Serverless Inference and a company-provided docker container. Option B, use Amazon SageMaker Real-Time Inference by hosting multiple models in one container behind one endpoint option. Option C, use Amazon SageMaker Real-Time Inference by hosting multiple models in multiple containers behind one endpoint option. And Option D, use Amazon SageMaker Serverless Inference and built-in containers. Pause the video if you need more time. Okay, let's evaluate the options. Option A is incorrect. SageMaker Serverless Inference is an inference option that you can use to deploy ML models without the need to provision or manage infrastructure. This serverless option is cost-effective because the solution automatically scales endpoints up or down depending on inference requests. With SageMaker Serverless Inference, you can use the bring your own container option. However, SageMaker Serverless Inference is not suitable to serve low-latency requests. You should use SageMaker Serverless Inference only for traffic that can tolerate cold starts. Option B is a possible correct answer. SageMaker Real-Time Inference is an inference option that you can use for real-time low-latency use cases. You can use SageMaker Real-Time Inference to serve traffic with low latency. To host multiple models behind one endpoint is a cost-effective way to meet the requirements. You can use and pay for only one endpoint instead of using several single model endpoints. But remember, we need to read all the responses before we choose our best-choice answer. Option C is incorrect. Option B talked about how you can use SageMaker Real-Time Inference to serve traffic with low latency, and to host multiple models behind one endpoint is a cost-effective way to meet the requirements. You can use and pay for only one endpoint instead of using several single-model endpoints. However, multiple models and multiple containers is less cost-effective than multiple models behind one container. And option D is incorrect. SageMaker Serverless Inference is an inference option that you can use to deploy machine learning models without the need to provision or manage infrastructure. And this serverless option is cost-effective because the solution automatically scales endpoints up or down depending on inference requests. And with SageMaker Serverless Inference, you can use a built-in container option. However, SageMaker Serverless Inference is not suitable to serve low-latency requests. You should use SageMaker Serverless Inference only for traffic that can tolerate cold starts. So that makes option B the correct answer. That's all for this question. Be sure to take note of any knowledge gaps that you may have identified while exploring this question. Let's get started with the sixth walkthrough question. 

Let's get started with our sixth walkthrough question, which is from task statement 3.3, "You use automated orchestration tools to set up continuous integration and continuous delivery, CI/CD, pipelines." The question reads, "A company stores its code repositories in AWS CodeCommit. After code is approved, code can be incorporated into the main branch in CodeCommit for production. The main branch is protected so that changes cannot be committed without approval. An ML engineer has recently developed, tested, and validated a new feature. The ML engineer wants to incorporate the new feature into the main branch. What is the first step the ML engineer should take to deploy the feature to the main branch for production?" Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords are "store the code repositories," "AWS CodeCommit," and "that the main branch is protected and needs approval." The question is asking, "What is the first step to deploy the feature to the main branch for production?" So, we need a solid understanding of Git principles and AWS CodeCommit to answer this question. Now that we have examined the stem, identified keywords and reviewed the requirements, let's explore the responses. Option A, "Commit the changes directly into the main branch in CodeCommit." Option B, "Merge the changes directly into the main branch in CodeCommit." Option C, "Create a pull request in CodeCommit. Set the main branch as the destination." Option D, "Create a pull request in CodeCommit and set the feature branch as the destination." Pause the video if you need more time. Okay, let's evaluate the options. Option A is incorrect. CodeCommit is a version control service that stores source code and binary files. Because the main branch is protected, you must first open a pull request to validate the feature branch before you can make commits into the main branch. Option B is incorrect. Because the main branch is protected, you must first open a pull request to validate the feature branch before you can make commits into the main branch. After the changes are approved, you can merge the feature branch into the main branch. Option C is a possible correct answer. CodeCommit is a Git-based service and follows Git principles to incorporate changes from a feature branch into a main branch. Because the main branch is protected, you must first open a pull request to validate the feature branch before you can make commits into the main branch. In a pull request, the destination is the branch where you intend to merge the code changes after the pull request is approved. And option D is incorrect. Again, because the main branch is protected, you must first open a pull request to validate the feature branch before you can make commits into the main branch. In a pull request, the destination is the branch where you intend to merge the code changes after the pull request is approved. In this scenario, the intended destination is the main branch, not the feature branch. Therefore, this solution does not meet the requirements. So, that makes option C the correct answer. That's all for this question. Be sure to take note of any knowledge gaps that you may have identified while exploring this question. Now is your chance to practice and dive even deeper into Domain 3 topics before continuing to Domain 4. If you are taking the Enhanced course, you'll move on to bonus questions and flashcards. Whether you are taking the Standard or Enhanced course, you'll see a list of additional resources to learn more about the topics covered. 

Let's get started with another walkthrough question for the course from Domain 3 and from task statement 3.2. Create and script infrastructure based on existing architecture and requirements. There are dropdown lists in the answer area, so that tells me this is an ordering or matching question. The question reads, "A company wants to use scikit-learn scripts for data processing and model evaluation. The company wants to use its own processing Docker container that is stored in Amazon Elastic Container Registry, or Amazon ECR, and Amazon SageMaker to process the scripts. Select and order the steps from the following list to process the input data and save the process data in Amazon S3. Each step should be selected one time. Select and order four." Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords I identified are, their own processing Docker container, stored in Amazon ECR, and SageMaker to process the scripts. This is an ordering question, and you need to select and order the four steps to process the input data and save the process data in Amazon S3, and each step should be selected only one time. Now that we've examined the stem, identified keywords, and reviewed the requirements, let's explore the step answers that we need to order from step 1 to step 4 to input and save the process data in Amazon S3. The steps that we need to order are, build the container and push the image to an Amazon ECR repository. Create a Docker directory and add the Docker file. Run the script by indicating the script processor and setting Amazon S3 as the output. Set up a script processor by using the SageMaker Python SDK and indicating the URL of the image. Think through your answers, and pause the video if you need more time. Let's first talk about SageMaker. SageMaker is a service that you can use to build, test, and deploy ML models. In SageMaker, you can install libraries to run scripts in your own processing containers. Here are the correct answers. First, you would create a Docker directory and add the Docker file. Second, you would build the container and push the image to an Amazon ECR repository. To build and push a Docker image that uses specific libraries, you might store the image in an Amazon ECR repository. Third, set up a script processor by using the SageMaker Python SDK and indicating the URL of the image. This ensures that the SageMaker IAM role has the necessary policies to pull the image from Amazon ECR. After the image is available in the Amazon ECR repository, SageMaker can invoke the image from a script processor and run the script. Fourth, you run the script by indicating the script processor and setting Amazon S3 as the output. Let's check and see if we matched the correct functionality to the AWS service. That's all for this question. Be sure to take note of any knowledge gaps that you might have identified while exploring this question. Let's get started with Domain 4. 