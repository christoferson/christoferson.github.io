Let's get started with the second task statement from Domain 2, which is to train and refine models. This task statement is split into two lessons. In previous lessons, we have talked about ways you can prepare the data for the training process. We mentioned training and testing data splits to have our model trained on the training data, and then evaluating that model using testing data that it's never seen before. Another option is cross-validation. K-fold cross-validation is a process where you use all of the data to test our model without removing or splitting off any of the data. All of the training data is used to train your model, and all of the data is used to test your model. Here is a question. How do you choose the right hyperparameter values when using cross-validation? One solution is to use regularization to minimize the adjusted loss function and prevent overfitting. For the exam, ensure you understand regularization, both Lasso regularization, or L1 regularization, and Ridge Regression regularization, or L2 regularization, and the use cases. Parameters are obtained from the data, and are the values inside the algorithm which influence how the calculations of weights, biases, coefficient, and so on actually work. When we pass in our data, the algorithm completes some calculations and comes up with numbers, weights, biases, coefficients, and so on, and then creates the model. Hyperparameters are how we control a training model, improve model performance and the model outcome. Hyperparameters are set by us to define how the training and learning process takes place. But all of this depends on the algorithm we use and the implementation of that algorithm. What are examples of hyperparameters? Epoch count, batch size, learning rate, regression type, number of nodes, layers, and more. Here is a question. You are developing an image classification model in Amazon SageMaker using 20 epochs. During training, you observed that the validation loss starts to increase from the 15th epoch onwards and results in poor model performance. How can you prevent this from adding expense and also reduce the training times? One solution is to use the early stopping option in Amazon SageMaker to stop the training jobs earlier if the results are not improving based on a particular metric to reduce compute time and avoid overfitting your model. And we'll talk more about compute under task statement 3.1. For the exam, also ensure you understand optimizers. Optimizers are one of the hyperparameters we can set on our algorithms as we train our models. A few examples of optimizers are Gradient Descent, Stochastic Gradient Descent, Adam, and many others. The optimizer that is best for your scenario and data depend on your requirements and the needed performance, and we'll dive deeper into performance under Domain 3. Here is a question. You are using Amazon SageMaker to train an image classification model using a large training dataset stored in Amazon S3. But you need to experiment with different hyperparameter tunings to achieve the best result, and you want fast startup and training times to run iterations. One solution might be to use Amazon FSx for Lustre to serve the Amazon S3 training data to Amazon SageMaker. Amazon FSx for Lustre file system is integrated with Amazon S3 and can speed up your training jobs by serving your Amazon S3 data to Amazon SageMaker at high speeds. What if you apply Principal Component Analysis on the Amazon S3 training data to reduce the file size? Will this work? No, this solution might reduce the overall size of the training data and improve the model performance, but it won't speed up the training job start and training times because you will need to load the data from the S3 bucket to Amazon SageMaker for iterations. What if you use Amazon EFS to serve the Amazon S3 training data to SageMaker? Will this solution work? It would, but Amazon FSx has a higher throughput than Amazon EFS, so using FSx would be the better choice. I'm gonna stop this lesson here. And in the next lesson, we'll continue talking about task statement 2.2. 

Let's continue with task statement 2.2, which is to train and refine models, and talk about algorithms and hyperparameters for SageMaker. SageMaker has an automatic model tuning, or AMT, that uses the algorithm and ranges of hyperparameters that you can specify. You can use SageMaker AMT with built-in algorithms, custom algorithms, or SageMaker prebuilt containers for machine learning frameworks. Let's dive deeper into how hyperparameter tuning works in SageMaker. When using grid search, hyperparameter tuning chooses combinations of values for the range of categorical values that you specify when you create the job. When using random cert, hyperparameter tuning chooses a random combination of values from within the range that you specify for hyperparameters for each training job it launches. For Bayesian optimization, hyperparameter tuning uses regression to choose the next set of hyperparameter values to test. Hyperband uses the median and final results of training jobs to reallocate epochs to effective hyperparameter configurations, and automatically stops those that underperform. This can help reduce compute time and avoid overfitting your model. And you can bring your own model into SageMaker using script mode, and use the automated hyperparameter optimization to select the best training job. For the exam, ensure you understand that it can take a long time to train a model, such as a large neural network, and then to process all of the data, which might be millions of calculations for each epoch. What is the solution to fine-tune and train a pre-trained model on a new dataset without training from scratch? You can use transfer learning and a generalized model from a generalized dataset that can be transferred into your training process with your new dataset. Then you can train your model, and the algorithm already knows some information and understand what it needs to do. But the model still needs to figure out how the information it understands maps to your new dataset. You can also use SageMaker Studio. SageMaker Studio does not run on a single instance server in your account like SageMaker Notebooks. SageMaker Studio is in an environment on a server managed by AWS with an Amazon EFS share to store the data. SageMaker Studio also has SageMaker JumpStart, which can help you find projects that are already built with datasets, models, algorithm types, and solutions that are based on industry best practices. And SageMaker Studio also has SageMaker resources, with projects, pipelines, experiments and trials, model registry, endpoints, compilation jobs, and more. And you can use SageMaker ML Lineage Tracking to create and store information of your machine learning workflow from data preparation to model deployment, to help reproduce the workflow steps, track the model and data lineage, and establish model governance and audit standards. And to manage your model versions for repeatability and audits, you can use the SageMaker Model Registry, which helps to catalog models for production, manage model versions, associate metadata, manage the approval status of a model, deploys models to production, and automate model deployment with continuous integration and continuous delivery. As machine learning and artificial intelligence has evolved, we've seen different approaches in types of machine learning models. For the exam, ensure you understand ensemble modeling, and how you can train, optimize, and deploy your custom ensembles using SageMaker. What is a use case for ensemble learning? Cybersecurity, fraud detection, remote sensing, predicting best next steps in financial decision making, medical diagnosis, and even computer vision and natural language processing tasks. There are also several libraries and frameworks that provide implementations of ensemble models such as eXtreme Gradient Boost, CatBoost, or the scikit-learn random forest, And again, with SageMaker, you can also bring your own models and use them as stacking ensemble, and then deploy a custom ensemble using a single SageMaker training job, and a single tuning job, to deploy a single endpoint which might reduce cost and operational overhead. Here's a question. You've created a data pipeline that extracts input features from a Redshift cluster and saves it as CSV files in Amazon S3. The exported data is then fed into a SageMaker instance to make inferences. You need to lower costs and generate predictions quicker. What is your solution? One solution is to use Amazon Redshift ML to create a recommender system model and perform in-database local inference. You can use SageMaker and SQL statements to create and train SageMaker machine learning models using your Redshift data and then use these models to make predictions. For this scenario question, the data is already in a Redshift cluster. Using Redshift ML, the input data can be processed directly within the Redshift cluster, reducing the time and resources required to move data between Redshift and Amazon S3. Additionally, Redshift ML is designed to perform fast in-database machine learning, which can help speed up the prediction generation process and reduce the overall cost of inference. Here is a related question. Can you use Amazon Redshift streaming ingestion to feed inference data directly to the SageMaker instance? No. Redshift streaming ingestion is a feature for consuming data directly from streaming sources such as Amazon Data Streams and Amazon Managed Streaming for Apache Kafka. It cannot be used to send inference requests to a SageMaker instance. Let's get started with the third task statement from Domain 2, which is to analyze model performance. 

