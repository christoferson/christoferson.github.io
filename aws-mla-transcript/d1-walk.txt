
Question 1
Thanks, Julie. Welcome to our first walkthrough question for the course. My name is Aaron Hunter, and I'm a Principal Developer Advocate on the AWS Training and Certification team. These walkthroughs are meant to assist you in a few ways. First, this will be an opportunity to see the type of questions that you'll encounter on the actual exam. Although the questions aren't pulled directly from the certification exam, they are of similar quality and difficulty, and will give you exposure to this style of questioning. Second, I wanna show you the methods I consider to be helpful when you're working with multiple-choice questions. These methods help you focus on what you're looking for, and help you identify any distractors you might encounter on the exam. And third, these questions will provide you with additional information. Any questions you feel confident in will reinforce your knowledge in that area, and any questions that reveal gaps in your knowledge will help you identify where to focus in your studies going forward. As I go through each of the questions, I'll generally follow a particular format. I'll start by reading through the question, then I'll identify keywords and phrases in the question that indicate what you're looking for in the answers. After that, I'll go through the answers and give you time to figure out if you can identify the correct answer yourself. After you've been given a chance to figure it out yourself, I'll go through the answers and discuss why they're correct or incorrect. Okay. Now that I've given you a background information, let's get started with a question, which is from task statement 1.1, ingest and store data. The question reads, "A bank developed a mobile app for customers to manage financial products. User interaction data from the app is continuously streamed to an Amazon Kinesis Data Streams application. An ML engineer must implement an automated solution with low latency that aggregates the data minute by minute to calculate ML features. Which solution will meet these requirements with the least operational overhead?" Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords identified are user data and continuously streamed to Kinesis Data Streams application. And you need to choose a solution with the least operational overhead for an automated solution with low latency that aggregates the data minute by minute to calculate ML features. We talked about streaming data in the first and last lesson for this task statement. Now that we have examined the STEM, identified keywords, and reviewed the requirements, let's explore the responses. Option A, create an application in Amazon Managed Service for Apache Flink that continuously reads from the data stream and applies the aggregations by using a 1-minute time window. Option B, use Amazon Data Firehose to deliver the streaming data to Amazon S3, create a scheduled AWS Glue job that reads the S3 data and calculates the aggregated features every minute. Option C, create an application in an Amazon EC2 instance that invokes the GetRecords API of Kinesis Data Streams once every minute to read the streaming data and apply the aggregations. And option D, configure an Amazon EMR cluster to continuously read from the data stream and apply the aggregations by using a 1-minute time window. Pause the video if you need more time. Okay, let's evaluate the options. Option A is a possible correct answer! Managed Service for Apache Flink is a fully managed service that you can use to build applications to process and transform data from streaming sources. For example, you can process and transform data from Kinesis Data Streams. You can write processing code in SQL, Python, Java, or Scala. You can use the required time aggregations and run the code against the stream. This solution requires the least operational overhead because you do not need to configure or manage infrastructure, servers, or clusters. Option B is incorrect. You can use Firehose to write streaming data into specific destinations. However, you must configure Firehose, an S3 bucket, and an AWS Glue job for data aggregations. Therefore, this solution requires additional operational overhead. Option C is incorrect. You can use Kinesis Data Streams APIs to receive records from a stream. However, you must create a loop to continuously pull data from the stream, respond to failover, handle re-sharding, and checkpoint processed records. Therefore, this solution requires additional operational overhead. Additionally, a solution that uses a single EC2 instance might not be enough to process all the incoming real-time data. Therefore, you would need to add additional operational overhead to configure load balancing across multiple instances. And option D is incorrect. Amazon EMR is a service that can run large-scale data processing. EMR clusters can process and analyze data from Kinesis Data Streams by using Apache Hadoop tools. However, you must configure the hardware and networking in the clusters. For example, you must configure node types, EC2 instances, instance grouping, and more. Therefore, this solution requires additional operational overhead. So, that makes option A the correct answer. That's all for this question. Be sure to take note of any knowledge gaps that you may have identified while exploring this question. Let's get started with our second walkthrough question. 

Question 2
Let's get started with our second walkthrough question, which is from task statement 1.3: Ensure data integrity and prepare data for modeling. The question reads, a company uses Amazon SageMaker to build, train and deploy its ML models. An ML engineer must identify features that can help generate reports for model explainability and detect possible biases. The company wants to analyze datasets stored in Amazon S3 to detect possible biases and analyze models deployed as SageMaker inference endpoints. Which solution meets these requirements with the least operational effort? Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords identified are SageMaker to build, train, and deploy its ML models. You must identify features for model explainability. Detect possible biases. The company wants to analyze datasets stored in Amazon S3 to detect possible biases and to analyze models deployed as SageMaker inference endpoints, and you need to identify the solution that meets the requirements with the least operational effort. In task statement 1.3, we discussed bias, variance, pre-training bias and metrics and how to ensure that the model makes predictions and whether bias impacts the prediction during training or inference. Do you remember the AWS service that can help less biased models that are also more understandable and explainable? Now, that we have examined the stem, identified the keywords and reviewed the requirements, let's explore the responses. Option A, use a single AWS Lambda function to analyze the datasets stored in Amazon S3 and the models deployed as SageMaker inference endpoints. Option B, use an AWS Glue job to analyze the datasets stored in Amazon S3. Use an Amazon SageMaker Clarify processing job to analyze the models deployed as SageMaker inference endpoints. Option C, use an AWS Lambda function to analyze the datasets stored in Amazon S3. Use an Amazon SageMaker Clarify processing job to analyze the models deployed as SageMaker inference endpoints. Option D, use two separate Amazon SageMaker Clarify processing jobs to analyze the datasets stored in Amazon S3 and the models deployed as SageMaker inference endpoints. Pause the video if you need more time. Okay, let's evaluate the options. Option A is incorrect. SageMaker is a fully managed service that is used to build, test, and deploy ML models. Lambda is a serverless service to run code that can connect with other services such as Amazon S3. This solution would work, but it creates additional overhead to create and maintain Lambda functions, and remember, we need to choose a solution with the least operational effort. Option B is incorrect. AWS Glue is a serverless data integration service. AWS Glue jobs create scripts to process data from sources to targets, such as Amazon S3. SageMaker Clarify is a feature of SageMaker that gives you the ability to generate reports for model explainability by the definition of processing jobs to compute bias metrics and feature attributions. SageMaker Clarify processing jobs can also interact with Amazon S3 containing input datasets. This solution adds unnecessary operational overhead to create an AWS Glue job because there is a solution already available within SageMaker Clarify to analyze datasets stored in Amazon S3. Option C is incorrect. Lambda is a serverless service to run code that can connect with other services such as Amazon S3. SageMaker Clarify is a feature of SageMaker that gives you the ability to generate reports for model explainability by the definition of processing jobs to compute bias metrics and feature attributions. SageMaker Clarify processing jobs can also interact with Amazon S3 containing input datasets. Although you could create a Lambda function to analyze a dataset stored in Amazon S3, this solution adds unnecessary operational overhead. You would need to create and manage the AWS Lambda function instead of using a solution already available within SageMaker Clarify to analyze datasets stored in Amazon S3. So that makes option D correct. Model explainability and detection of possible biases can help you generate interpretable explanations during a model's decision-making process. SageMaker Clarify is a feature of SageMaker that gives you the ability to generate reports for model explainability by the definition of processing jobs to compute bias metrics and feature attributions. SageMaker Clarify processing jobs can also interact with Amazon S3 containing input datasets. This solution is the most operationally efficient because you do not need to configure other services to meet the requirements. That's all for this question. Be sure to take note of any knowledge gaps that you have identified while exploring this question. Now is your chance to practice and dive deeper on Domain 1 topics before continuing to Domain 2. If you are taking the enhanced course, you'll move onto bonus questions, flashcards, and a lab. Whether you are taking the standard or enhanced course, you'll see a list of additional resources to learn more about the topics covered. 

Question 3
Let's get started with another walkthrough question for the course, which is from Domain 1 and task statement 1.2, transform data and perform feature engineering. There are dropdown lists in the answer area, so that tells me this is an ordering or matching question. The question reads, Select the correct functionality from the following list for each AWS service or feature that would allow an ML engineer to prepare data for ML model training. Each functionality should be selected one or more times. Select six. The functionality options are, Query and discover the data. Extract, transform, and load or ETL, the data to storage. Store and make the data available to Amazon SageMaker. Reading this question, can you identify any keywords or phrases? Also, what exactly is the question asking? A few keywords I identified are service, feature, and to prepare data for model training. You need to identify and match the correct functionality to the AWS service by choosing one response answer option from each dropdown list. You can select each response one or more times. Now that we have examined the stem, identified keywords, and reviewed the requirements, let's explore the AWS services that we need to match to the correct functionality. The AWS services that you need to match the functionality to are the following. Amazon EMR, Amazon Athena, Amazon S3, AWS Glue Data Catalog, Amazon Elastic File System, or Amazon EFS, and AWS Glue ETL. Think through your answers, and pause the video if you need more time. Amazon EMR matches the functionality to ETL the data to storage. It is a big data platform that you can use to run large-scale data processing jobs from multiple data sources. Amazon EMR can ETL the data to storage. Athena matches the functionality to query and discover the data. It is a serverless analytics service that provides a streamlined way to query data from multiple sources. You can use Athena to query and discover the data for this use case. Amazon S3 matches the functionality to store and make the data available to SageMaker. Amazon S3 is an object storage service that you can use to deliver training data to SageMaker. AWS Glue Data Catalog matches the functionality to query and discover the data. You can use Data Catalog to store schemas for data that is located in multiple data sources. Data Catalog can query and discover the data for this use case. Amazon EFS matches the functionality to store and make the data available to SageMaker. Amazon EFS provides a fully managed elastic NFS file system that you can use to deliver training data to SageMaker. AWS Glue ETL matches to the functionality to ETL the data to storage. AWS Glue ETL is a serverless ETL service that you can use to prepare, move, and integrate data from multiple sources. AWS Glue ETL meets the requirements to ETL the data to storage. Let's check and see if we matched the correct functionality to the AWS service. That's all for this question. Be sure to take note of any knowledge gaps that you might have identified while exploring this question. Let's get started with Domain 2. 