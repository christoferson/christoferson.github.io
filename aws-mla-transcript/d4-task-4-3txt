Let's get started with the last task statement from Domain 4, which is to secure AWS resources. This task statement is split into three lessons. Before we begin, I want to emphasize that security is one of the biggest and earliest considerations you will make when designing an architecture. This includes defining how the people, tools, pipelines, applications, and model you build will access the necessary AWS services and your data. For the exam, ensure you understand the Shared Responsibility Model, the AWS global infrastructure, and especially the resilience of different AWS services. One area that is often overlooked when studying for AWS Certification exams is AWS accounts. AWS accounts may seem too basic to take the time to understand, but it is considered part of the required need-to-know fundamentals for AWS. You must understand what accounts are and how they work to ensure you can secure access to your AWS resources. For the exam, review the best practices regarding the account root user security, applying the principle of least privilege, and multi-factor authentication. IAM identities start with no permission, but permissions can be granted. SageMaker support specific actions, resources, and condition keys for IAM identity-based policies to allow or deny actions and resources as well as the conditions under which actions are allowed or denied. Amazon SageMaker performs operations on your behalf using other AWS services, and you must grant SageMaker permissions with an IAM execution role to use these services and the resources they act upon. And to ensure least privilege permissions with SageMaker, you can use Amazon SageMaker Role Manager to build and manage IAM roles for your machine learning needs. For the exam, dive deeper into identity-based policies that are attached to an IAM user, group, or role and resource-based policies that are attached to a resource. I'm gonna stop this lesson here, and in the next lesson, we'll continue talking about task statement 4.3. 


Let's get started with the second lesson for task statement 4.3, which is to secure AWS resources. SageMaker components including the studio, notebooks, training jobs, and hosting instances can be provisioned in an isolated Amazon VPC with no internet access. SageMaker Studio also gives you the ability to control the network access and internet connectivity of SageMaker Studio notebooks. Direct internet access can be disabled to add an additional layer of security. And this same concept can be applied to SageMaker notebooks by choosing to launch the notebook instance in an Amazon VPC and the notebook instance can be configured with or without direct internet access. Traffic to public endpoints, such as Amazon S3 or SageMaker APIs, can be configured to travel over VPC endpoints to ensure that the traffic stays within the AWS network. You can also use VPC endpoint and endpoint policies to further limit access. You can have your AWS resources directly with public endpoints, such as S3, CloudWatch, and SageMaker API, and SageMaker Runtime through an interface endpoint in your Amazon VPC instead of connecting over the internet. For example, access to an S3 bucket can be restricted only to a SageMaker Studio domain or set of users, and each studio domain can be restricted to have access only to a specific S3 bucket to secure SageMaker Studio connectivity using a private Amazon VPC. You can also grant access from only within the Amazon VPC with an IAM policy that can be created to prevent users outside the Amazon VPC from accessing SageMaker Studio or SageMaker notebooks over the internet. For example, this policy can help restrict connections made only through specific VPC endpoints or a specific set of source IP addresses. This policy can be added to every user, group, or role used to access SageMaker Studio or Jupyter notebooks. For the exam, ensure you understand IAM users, groups, and roles, and what goes into deciding between which to use and how they might be combined. Also, dive into the best practices for controlling your access to the AWS APIs. And lastly, understand the methods, service, and tools available that help you create traceability for access to AWS resources. You need to understand the performance and behaviors of your application components and have insights into who and what has access to your account resources and data. Do you know how to design and implement the principle of least privilege throughout the whole machine learning pipeline to ensure only those who need access have just the access they need? Let's move on and talk about data security and ask a security question. How do you automate a solution to keep your data secure and prevent manual logins into instances in an auto scaling group that your application hosts? One solution is to install the CloudWatch agent on the instances. Set up a CloudWatch Logs subscription. Create a Lambda function to add a FOR_DELETION tag to any EC2 instance that produces an SSH login event. Create a second Lambda function using an EventBridge rule to terminate all instances that have the custom FOR_DELETION tag. I added this question even though it relates more to architecting than machine learning because I wanted to make this distinction. By default, when you create a SageMaker notebook instance, users that log into that notebook instance have root access. And the reasoning is that data science is an iterative process and you might need to test and use different software tools and packages, and many notebook instance users need to have root access to be able to install these tools and packages. Because users with root access have administrator privileges, users can access and edit all files on a notebook instance with root access enabled. To secure shared SageMaker notebook instances, you can use IAM policies when granting individuals access to notebook instances that are attached to an Amazon VPC that contains sensitive information. So, the distinction is for Amazon EC2 instances, you have to grant all access, but for SageMaker notebook instances, you need to secure the access. And also, for both you need to ensure you are using the principle of least privilege. SageMaker Studio uses file system and container permissions for access control and isolation of SageMaker Studio users and notebooks. And this ties into adding security based on access patterns. Certain services, such as Amazon S3, give you the ability to manage security for the entire buckets, but also to add controls based on specific paths or objects. You can also add intrusion detection and prevention with AWS Gateway Load Balancer. AWS Network Firewall can be used to filter outbound traffic and supports inbound and outbound web filtering for unencrypted web traffic. For encrypted web traffic, Server Name Indication is used for blocking access to specific sites. In addition, AWS Network Firewall can filter fully qualified domain names. If access is needed to an AWS service that does not support interface VPC endpoints or to a resource outside of AWS, you can configure a NAT gateway with security groups to allow outbound connections. I'm going to stop this lesson here, and in the next lesson we'll continue talking about task statement 4.3. 

Let's continue with task statement 4.3, which is to secure AWS resources. Let's move on and talk about data retention, classification, and data recovery. There are best practices for securing sensitive data in AWS data stores. Ensure you understand general data security patterns and a clear mapping of these patterns to cloud security controls. AWS provides the AWS Cloud Adoption Framework with a specific security perspective to help. I added a link for you to dive deeper under the additional resources. It is important to understand what data protection looks like for your architecture and requirements. Here are some questions to consider. How do you design data protection when using a VPN over the internet, or a private connection through AWS Direct Connect, or connections between Amazon VPCs, or for the transfer of your data between services, and how do you protect the data in transit when reaching end users over the public internet? Ensure you understand how to protect data at rest with the AWS Key Management Service, or AWS KMS, that can be used to encrypt your machine learning data, studio notebook, and SageMaker notebook instances. SageMaker uses KMS keys by default for more control on encryption and key management. For studio notebooks, the machine learning data is primarily stored in multiple locations. An Amazon S3 bucket host notebook snapshots and metadata, Amazon EFS volumes contains studio notebook and data files, and Amazon Elastic Block Store, Amazon EBS, volumes are attached to the instance that the notebook runs on. KMS can be used for encryption for all of these storage locations. To protect data in transit, AWS uses HTTPS communication for its APIs. Requests to the SageMaker API and console are made over a secure sockets layer connection. In addition to passing all API calls through a transport layer security encrypted channel, AWS APIs also require that requests are signed using the Signature Version 4 signing process. Additionally, communication between instances in a distributed training job can be further protected and another level of security can be added to protect your training containers and data by configuring a private Amazon VPC. SageMaker can be instructed to encrypt inter-node communication automatically for the training job. The data passed between nodes is then passed over an encrypted tunnel without the algorithm having to take on responsibility for encrypting and decrypting the data. What are the recommended best practices of using version control to track code or model artifacts? If model artifacts are stored in Amazon S3, versioning should be enabled and also paired with multi-factor authentication, or MFA, delete to ensure that only users authenticated with MFA can permanently delete an object version, or change the versioning state of the bucket. You can also associate Git repositories with new or existing SageMaker notebook instances and add version control. SageMaker supports AWS CodeCommit, GitHub, and other Git-based repositories. And if you use AWS CodeCommit, the repository can be further secured by rotating credentials and enabling MFA. Additionally, the SageMaker Model Registry can also be used to register, deploy, and manage models. And to tie in logging, monitoring, and compliance, Amazon SageMaker is part of multiple AWS compliance programs, including FedRAMP, HIPAA, and others. And third-party audit reports can be downloaded using AWS Artifact. For the exam, ensure you dive deeper into the security pillar of the AWS Well-Architected Framework. AWS provides tools and features to help you secure your AWS environment. We've talked about these before, such as AWS CloudTrail, CloudWatch, Amazon GuardDuty, and more. Prioritize security at every layer, study with security in mind, and evaluate how protection can be added both in original designs as well as reinforcing existing deployments. Let's get started with our seventh walkthrough question. 