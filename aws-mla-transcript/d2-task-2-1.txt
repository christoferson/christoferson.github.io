Let's get started with the first task statement from Domain 2, which is to choose a modeling approach. This task statement is split into three lessons. For the exam, ensure you understand different machine learning algorithms such as logistic regression, linear regression, gradient descent, support vector machines, decision trees, random forests, K-means clustering, K-nearest neighbor, latent Dirichlet allocation, principal component analysis, sequence-to-sequence, word2vec, multinominal logistic regression, and more. You'll also need to ensure you understand deep learning, neural networks, artificial neurons, activation functions, weights and bias. Neural networks such as convolutional neural networks use sophisticated layers inside of the networks and introduce different layer types. Convolutional neural networks are mostly used for image classification. Recurrent neural networks, which is a different type of architecture for neural networks, are used for sequence or time-based inputs. For example, recurrent neural networks are great at translation because some languages with the different dialects have words in different order to each other. And having the order of the words in the context of the sentence it is in is important for translation, audio analysis, sentiment analysis, and time series data analysis. For the exam, ensure you understand most of the machine learning models and how to mitigate problems in neural networks. Here's a question. What is the solution to help mitigate the vanishing gate problem? Well, there is long short term memory, or LSTM, which is a type of recurrent neural network. LSTM uses gates to be able to determine what is important and should be remembered, but also, what it should forget. It introduces multiple neural networks inside of the LSTM node inside of the neural network. And this does add time to train because there are multiple neural networks at all stages along the process. And if you don't have the extra time needed to train, then you can use a gated recurrent units, or GRU, which is another type of recurrent neural network and also uses gates. But it is structured differently, it is not as complicated, needs less processing than an LSTM, and it might be faster to train. Here is another question. How do you consider interpretability during model selection or your algorithm selection? The higher the interpretability of a machine learning model, the easier it is to comprehend the model's predictions. But there is a tradeoff to be considered between what the model has predicted, which is the model performance, and why the model has made such a prediction, which is the model interpretability. And there are methods for model interpretability that can be classified into intrinsic analysis and post hoc analysis. Intrinsic analysis can be applied to interpret models that have low complexity or simple relationships between the input variables and the predictions. These models are based on algorithms such as linear regression where the prediction is the weighted sum of the inputs and decision trees, where the prediction is based off of a set of if-then rules. Post hoc analysis can be applied to interpret simple relationship models and more complex models such as neural networks, which have the capability to capture non-linear interactions. These methods are often model-agnostic and provide mechanisms to interpret a trained model based on the inputs and output predictions. Post hoc analysis can be performed at a local level and zoom in on a single data point or at a global level and zoom out and view the overall behavior of the model. I'm gonna stop this lesson here. And in the next lesson, we'll continue talking about task statement 2.1. 

Let's continue with task statement 2.1 which is to choose a modeling approach. Let's talk more about the AWS machine learning stack. On the bottom of the stack, there is the infrastructure and machine learning frameworks. Everything is built on top of this infrastructure layer. Then on top of the infrastructure layer is the AWS machine learning services such as Amazon SageMaker. And then on top of the machine learning layer are the AWS artificial intelligence services. These are prebuilt models already trained by AWS, and you can use these services and integrate them into your application. Let's quickly cover some of the AWS AI services and their use cases. Amazon Lex is a chatbot engine and is the service that powers Alexa. Use cases for Amazon Lex are call center bot, informational bot, and travel bot. Amazon Polly is a text-to-speech service that supports using the Speech Synthesis Markup Language and you can choose how you want your speech to be rendered such as shouting or whispering. It also supports custom lexicons which you can use to explain how words are pronounced. I added a link to a blog that talks about how to customize pronunciation using lexicons in Amazon Polly. You give it a text file and Polly will convert it to a stream of audio or an audio file. Amazon Textract and Amazon Comprehend are both text-based services powered by machine learning and you can use these services through an API. Amazon Textract extracts text from images and is an example of Optical Character Recognition. It grabs meaning out of documents and can work with both printed characters and handwriting. As far as use cases, Textract can import documents and forms, automated document processing, and extract text for Natural Language Processing. Amazon Comprehend is a natural language processing tool. Use cases are language detection and extraction of things from text such as key phrases, places, people, events, brands, and so on. It can also be used for sentiment analysis, topics analysis, Comprehend Medical which has been trained for NLP with medical terminology and the way that the notes are taken. Amazon Transcribe and Translate are text-based services. Amazon Transcribe is a speech-to-text service and it is the reverse of Amazon Polly that takes text and outputs audio files. Transcribe takes audio files and outputs text files. Amazon Transcribe uses Automatic Speech Recognition. A few use cases are call transcription, meeting transcription, subtitles, and indexing of audio. Amazon Translate is a service that translates text files A use case is mass automated translations. Amazon Rekognition is a machine learning service to analyze images and videos that are prebuilt and pre-trained. There are different services under Amazon Rekognition, Amazon Rekognition Image and Amazon Rekognition Video. I added links for Amazon Rekognition under additional resources. Ensure that you understand the image classification and object detection for Rekognition Image and Video is detecting a standard set of things that it can find. But with the Rekognition Custom Labels, you input the image and object data in an Amazon S3 bucket to train Rekognition with your custom images and objects. It integrates with Amazon SageMaker Ground Truth to help you label your data. Amazon Forecast is a time series forecasting service. It supports various models and can be used with recurrent neural networks such as inputting historical data, creating a model, and then making predictions. Use cases are sales forecasting inventory levels, website impression, and more. Forecast is different from the other AWS AI services because you do have to provide training data to train the model to make that inference. Amazon Personalize is a personalized recommendation service. Use cases are recommending items that other customers bought, similar items, sharing promotions and notifications. Fraud Detector is a service to detect fraud and fraudulent online activities. A few use cases are to help mitigate online payment fraud and fake accounts. And to wrap up our coverage of AWS AI services, let's talk about Amazon Bedrock. Amazon Bedrock is a fully managed service to build generative AI applications on AWS with foundation models and the tools to customize. You can choose foundation models from various providers and pick the one that is better suited for your use case. Here is a question. You are using Amazon Polly to translate plaintext documents to speech. After testing, some acronyms and business-specific terms are being pronounced incorrectly. What is your solution to ensure the terms are pronounced correctly? With Amazon Polly, you can use the pronunciation lexicons and you can modify the pronunciation of the words and customize the pronunciations too. Here is another question. You're building a crawler to analyze social media posts using sentiment analysis. You need to index the posts and the sentiment as metadata in an Amazon OpenSearch Service cluster. Which AWS AI services will help? Amazon Comprehend uses machine learning to discover the insights and relationships in your unstructured data. It extracts key phrases, places, people, brands, or events, understands the positive or negative of the text, analyzes the text using tokenization and parts of speech, and automatically organizes text files by topic. So my first thought of a design would be to expose the application through a RESTful endpoint, have it invoke a Lambda function that will call Amazon Comprehend for sentiment analysis, and index data into the cluster. I'm gonna stop this lesson here and in the next lesson, we'll continue talking about task statement 2.1. 

Let's continue with task statement 2.1, which is to choose a modeling approach and dive deeper into SageMaker. So, let's pause and talk about what makes SageMaker different from other AWS services. SageMaker is a fully managed machine learning service to help build, train, and deploy machine learning models into a production-ready hosted environment. And we've talked about how SageMaker is a collection of elements, features, and different services, and it sits in the machine learning layer of the different levels of machine learning services in AWS. It has its own dedicated SDK, and the Boto3 Python library can configure SageMaker. Also, SageMaker has data labeling services, SageMaker Notebooks, containers, pre-written document samples, algorithms, infrastructure management, and more capabilities. And these are all tools under the machine learning operations. SageMaker also has Docker containers. For the exam, ensure you understand that SageMaker does not use containers to scale, but as containers for application code, scripts, datasets, models, libraries associated with the environment that you'll be using for training, inference, or data pre-processing, and as a method to deploy code, binaries, and the environment to EC2 instances running inside AWS. AWS has prebuilt containers, and these containers are ready to accept development workloads from the framework. And there are also containers that contain algorithms which have been tuned to do specific things, such as image and object classification, linear regression, and more. And you can create your own custom containers with your own image, Docker files, and more. And SageMaker will manage the deployment and lifecycle, along with the infrastructure for you. Let's also talk about the different services and features of SageMaker. SageMaker Ground Truth is a service to help you label data. There are notebook instances to run your Jupyter notebooks. You can also use SageMaker to train algorithms, and the way that works is that you deploy a Docker container to run a training job. And you can run hyperparameter tuning jobs, which is a batch way to run multiple training jobs and monitor the performance metrics and tune the jobs and tweak your hyperparameters as much as it can for you. SageMaker can also perform inference and host models and create endpoints. You can use SageMaker Neo to optimize your machine learning models to train once and then run anywhere in the cloud and at the edge. Augmented AI provides a streamlined way of incorporating human reviews of AI applications, such as Amazon Rekognition, Amazon Textract, or your own models. And then there is also AWS Marketplace to be able to call pre-trained algorithms and more that you can buy and bring into your account. Then with the SageMaker machine interface and the AWS API, you have the ability to interact with SageMaker, but also Amazon EC2, Amazon S3, and so on. And SageMaker can be broken down into even more layers or a SageMaker stack. And as a user, you can use the AWS console, SageMaker notebooks, or SageMaker Studio to interact with the SageMaker user interface. And then there are multiple AWS SDKs for multiple different languages. And on top of the infrastructure layer is a Docker container infrastructure, and these containers are run to support machine learning workloads. And you can use your own containers or containers created by AWS with built-in algorithms. And SageMaker manages that orchestration. For the exam, ensure you understand how to use SageMaker Clarify to detect bias in machine learning models and understand model predictions. SageMaker Clarify provides a scalable and efficient implementation of SHapley Additive exPlanations, which is a method based on cooperative game theory and used to increase transparency and interpretability of machine learning models. And a focus for this task statement is the SageMaker built-in algorithms, pre-trained models, and prebuilt solution templates to help get started on training and deploying machine learning models. SageMaker JumpStart provides pre-trained models, prebuilt solution templates, and examples for problem types that use the SageMaker SDK and more. I also add a link under additional resources for JumpStart. For the exam, ensure that you dive deep into SageMaker. Let's get started with the second task statement from Domain 2, which is to train and refine models. 