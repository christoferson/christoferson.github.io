<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning</h1>
  
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">At a Glance - Notes</h3>

  <div class="row">
    <div class="col-sm-12">
      <p style="color:blue;"></p>
      <ul>
        <li>Introduction to Machine Learning: <ul>
            <li>Types of ML:
              <ul>
                <li>Supervised Learning: The algorithm learns from labeled data to predict outcomes for unseen data. e.g. Classification, Regression</li>
                <li>Unsupervised Learning: The algorithm finds patterns in unlabeled data. e.g. Clustering, Dimensionality Reduction</li>
                <li>Reinforcement Learning: The algorithm learns by interacting with an environment, receiving feedback in the form of rewards or penalties. e.g. Game playing AI, Robotics</li>
              </ul>
            </li>
            <li>Bias-Variance tradeoff
              <ul>
                <li>Bias: The error due to overly simplistic assumptions in the learning algorithm.</li>
                <li>Variance: The error due to the model's sensitivity to small fluctuations in the training set.</li>
                <li>The tradeoff involves finding the right balance between a model that is too simple (high bias) and one that is too complex (high variance).</li>
              </ul>
            </li>
            <li>Overfitting and Underfitting
              <ul>
                <li>Overfitting: The model learns the training data too well, including noise, leading to poor generalization on new data.</li>
                <li>Underfitting: The model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and new data.</li>
              </ul>
            </li>
            <li>Cross-validation
              <p>A technique used to assess how well a model will generalize to an independent dataset. It involves partitioning the data into subsets, training the model on a subset, and validating it on the remaining data. Common methods include k-fold cross-validation and leave-one-out cross-validation.</p>
            </li>
          </ul>
        </li>
        <li>Traditional Machine Learning Algorithms: <ul>
            <li>Linear Regression - models the linear relationship between a dependent variable and one or more independent variables.</li>
            <li>Logistic Regression - classification algorithm that predicts the probability of an instance belonging to a particular class using a logistic function.</li>
            <li>Decision Trees - A tree-like model of decisions where each internal node represents a feature, each branch represents a decision rule, and each leaf represents an outcome.</li>
            <li>Random Forests - An ensemble learning method that constructs multiple decision trees and combines their outputs for improved accuracy and reduced overfitting.</li>
            <li>Support Vector Machines (SVM) - finds the hyperplane that best separates classes in high-dimensional space, maximizing the margin between the classes.</li>
            <li>K-Nearest Neighbors (KNN) - non-parametric method that classifies data points based on the majority class of their k nearest neighbors in the feature space.</li>
            <li>K-Means Clustering - unsupervised algorithm that partitions data into k clusters, where each data point belongs to the cluster with the nearest mean.</li>
            <li>Principal Component Analysis (PCA) - dimensionality reduction technique that transforms the data into a new coordinate system, preserving the maximum variance in the data.</li>
          </ul>
        </li>
        <li>Deep Learning Fundamentals: <ul>
            <li>Neural Networks - Computational models inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers to process and transmit information.</li>
            <li>Activation functions - Mathematical functions applied to the output of a neuron to introduce non-linearity, enabling the network to learn complex patterns. Common examples include ReLU, sigmoid, and tanh.</li>
            <li>Backpropagation - An algorithm for efficiently computing gradients in neural networks by propagating the error backward through the network, allowing for weight updates during training.</li>
            <li>Gradient Descent and its variants
              <p>An optimization algorithm that iteratively adjusts the model's parameters in the direction of steepest descent of the loss function to minimize errors.</p>
              Gradient Descent variants:
              <ul>
                <li>Stochastic Gradient Descent (SGD): Updates parameters using one random sample at a time, offering faster iterations but noisier updates.</li>
                <li>Mini-batch Gradient Descent: A compromise between batch and stochastic methods, updating parameters using small batches of samples.</li>
                <li>Adam: An adaptive learning rate optimization algorithm that combines ideas from momentum and RMSprop, offering efficient and stable updates.</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>Deep Learning Architectures: <ul>
            <li>Convolutional Neural Networks (CNNs) - Specialized neural networks designed for processing grid-like data (e.g., images), using convolutional layers to automatically learn spatial hierarchies of features.</li>
            <li>Recurrent Neural Networks (RNNs) - Neural networks with loops that allow information to persist, making them suitable for processing sequential data like time series or natural language.</li>
            <li>Long Short-Term Memory (LSTM) -  A type of RNN architecture designed to address the vanishing gradient problem, capable of learning long-term dependencies in sequential data.</li>
            <li>Autoencoders - Unsupervised learning models that aim to learn efficient data encodings by training the network to reconstruct its input from a compressed representation.</li>
            <li>Generative Adversarial Networks (GANs) - A framework consisting of two neural networks (generator and discriminator) that compete against each other, typically used for generating new, synthetic instances of data.</li>
          </ul>
        </li>
        <li>Natural Language Processing (NLP): <ul>
            <li>Text preprocessing techniques - Methods to clean and standardize raw text data, including tokenization, lowercasing, removing punctuation, stemming, and lemmatization.</li>
            <li>Word embeddings (Word2Vec, GloVe) - Dense vector representations of words that capture semantic relationships, learned from large text corpora using neural network-based techniques.</li>
            <li>Sequence-to-sequence models - Neural network architectures designed to transform one sequence into another, commonly used for tasks like machine translation and text summarization.</li>
            <li>Attention mechanisms - Techniques that allow models to focus on different parts of the input when producing each element of the output, improving performance on tasks with long-range dependencies.</li>
            <li>Transformer architecture - A neural network architecture that relies entirely on attention mechanisms, dispensing with recurrence and convolutions, and achieving state-of-the-art results in various NLP tasks.</li>
          </ul>
        </li>
        <li>Advanced Deep Learning: <ul>
            <li>Transfer Learning</li>
            <li>Few-shot and Zero-shot Learning</li>
            <li>Meta-learning</li>
          </ul>
        </li>
        <li>Generative AI: <ul>
            <li>Variational Autoencoders (VAEs)</li>
            <li>Advanced GAN architectures</li>
            <li>Diffusion Models</li>
            <li>Large Language Models (e.g., GPT, BERT)</li>
          </ul>
        </li>
        <li>Reinforcement Learning: <ul>
            <li>Markov Decision Processes</li>
            <li>Q-Learning</li>
            <li>Policy Gradient Methods</li>
            <li>Deep Reinforcement Learning</li>
          </ul>
        </li>
        <li>ML/AI Tools and Frameworks: <ul>
            <li>Scikit-learn</li>
            <li>TensorFlow</li>
            <li>PyTorch</li>
            <li>Keras</li>
            <li>Hugging Face Transformers</li>
          </ul>
        </li>
        <li>Data Processing and Visualization: <ul>
            <li>Pandas</li>
            <li>NumPy</li>
            <li>Matplotlib</li>
            <li>Seaborn</li>
          </ul>
        </li>
      </ul>
    </div>
  </div>

</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Metrics</h3>

  <div class="row">
    <div class="col-sm-12">
      <p style="color:blue;"></p>

      <ul>
        <li>Accuracy: <ul>
            <li>Definition: The proportion of correct predictions (both true positives and true negatives) among the total
              number of cases examined.</li>
            <li>Formula: (TP + TN) / (TP + TN + FP + FN)</li>
            <li>Use case: Good for balanced datasets.</li>
          </ul>
        </li>
        <li>Precision: <ul>
            <li>Definition: The proportion of true positive predictions compared to the total number of positive predictions.
            </li>
            <li>Formula: TP / (TP + FP)</li>
            <li>Use case: When the cost of false positives is high.</li>
          </ul>
        </li>
        <li>Recall (Sensitivity or True Positive Rate): <ul>
            <li>Definition: The proportion of actual positive cases that were correctly identified.</li>
            <li>Formula: TP / (TP + FN)</li>
            <li>Use case: When the cost of false negatives is high.</li>
          </ul>
        </li>
        <li>F1 Score: <ul>
            <li>Definition: The harmonic mean of precision and recall, providing a single score that balances both metrics.
            </li>
            <li>Formula: 2 * (Precision * Recall) / (Precision + Recall)</li>
            <li>Use case: When you need a balance between precision and recall.</li>
          </ul>
        </li>
        <li>Specificity (True Negative Rate): <ul>
            <li>Definition: The proportion of actual negative cases that were correctly identified.</li>
            <li>Formula: TN / (TN + FP)</li>
            <li>Use case: When correctly identifying negative cases is important.</li>
          </ul>
        </li>
        <li>ROC (Receiver Operating Characteristic) Curve: <ul>
            <li>Definition: A graph showing the performance of a classification model at all classification thresholds.</li>
            <li>Use case: Evaluating the trade-off between true positive rate and false positive rate.</li>
          </ul>
        </li>
        <li>AUC (Area Under the ROC Curve): <ul>
            <li>Definition: Represents the degree or measure of separability between classes.</li>
            <li>Use case: Comparing different models' performance.</li>
          </ul>
        </li>
        <li>Mean Squared Error (MSE): <ul>
            <li>Definition: The average of the squared differences between predicted and actual values.</li>
            <li>Formula: Σ(y_pred - y_true)² / n</li>
            <li>Use case: Regression problems, sensitive to outliers.</li>
          </ul>
        </li>
        <li>Root Mean Squared Error (RMSE): <ul>
            <li>Definition: The square root of MSE.</li>
            <li>Formula: √(Σ(y_pred - y_true)² / n)</li>
            <li>Use case: Regression problems, same units as the predicted value.</li>
          </ul>
        </li>
        <li>Mean Absolute Error (MAE): <ul>
            <li>Definition: The average of the absolute differences between predicted and actual values.</li>
            <li>Formula: Σ|y_pred - y_true| / n</li>
            <li>Use case: Regression problems, less sensitive to outliers than MSE.</li>
          </ul>
        </li>
        <li>R-squared (Coefficient of Determination): <ul>
            <li>Definition: The proportion of the variance in the dependent variable that is predictable from the independent
              variable(s).</li>
            <li>Use case: Regression problems, measuring goodness of fit.</li>
          </ul>
        </li>
        <li>Log Loss (Cross-Entropy Loss): <ul>
            <li>Definition: A performance measurement for a classification model where the prediction input is a probability
              value between 0 and 1.</li>
            <li>Use case: When probabilistic predictions are important.</li>
          </ul>
        </li>
        <li>Confusion Matrix: <ul>
            <li>Definition: A table layout of prediction results, showing the numbers of correct and incorrect predictions
              broken down by class.</li>
            <li>Use case: Detailed analysis of classification performance.</li>
          </ul>
        </li>
        <li>Cohen's Kappa: <ul>
            <li>Definition: A statistic that measures inter-rater agreement for categorical items.</li>
            <li>Use case: Evaluating the agreement between two raters or between a model and human rater.</li>
          </ul>
        </li>
      </ul>

    </div>
  </div>
</div>





<div class="container mt-5">
	<h3 class="text-primary h4">Common Hyperparameters</h3>

  <div class="row">
    <div class="col-sm-12">
      <p style="color:blue;"></p>

      <ul>
        <li>Learning Rate: <ul>
            <li>Definition: Controls the step size at each iteration while moving toward a minimum of the loss function.</li>
            <li>Importance: Affects the speed and quality of learning.</li>
          </ul>
        </li>
        <li>Number of Estimators/Trees: <ul>
            <li>Definition: The number of trees or models in ensemble methods.</li>
            <li>Importance: Affects model complexity and performance.</li>
          </ul>
        </li>
        <li>Max Depth: <ul>
            <li>Definition: The maximum depth of a tree in tree-based models.</li>
            <li>Importance: Controls model complexity and potential overfitting.</li>
          </ul>
        </li>
        <li>Batch Size: <ul>
            <li>Definition: The number of training examples used in one iteration.</li>
            <li>Importance: Affects training speed and the accuracy of the estimate of the gradient.</li>
          </ul>
        </li>
        <li>Regularization Parameters: <ul>
            <li>Definition: Parameters that prevent overfitting by adding a penalty to the loss function.</li>
            <li>Importance: Controls the trade-off between model complexity and training error.</li>
          </ul>
        </li>
        <li>Number of Hidden Layers/Units: <ul>
            <li>Definition: The number of layers and neurons in neural networks.</li>
            <li>Importance: Determines the model's capacity and complexity.</li>
          </ul>
        </li>
      </ul>

    </div>
  </div>
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Convolutional Neural Networks</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<div class="row">
		<div class="col-sm-12">
      A class of deep learning neural networks primarily used for analyzing visual imagery. They are particularly effective in image recognition, classification, and computer vision tasks. 
      CNNs have revolutionized the field of computer vision and continue to be a cornerstone of many AI applications dealing with image and video data. Their ability to automatically learn hierarchical features from raw pixel data has made them indispensable in modern machine learning and AI systems.
      Feature Location Invariant - Detect and classify objects and features that are not in a specific location.
		</div>
	</div>
  <br/>
	<div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">CNN Process Flow</p>
      <ul>
      <li>Input: Takes in an image as a matrix of pixel values</li>
      <li>Convolution: Applies filters to detect features (edges, textures, etc.)</li>
      <li>Activation: Applies non-linearity (e.g., ReLU)</li>
      <li>Pooling: Reduces dimensionality while retaining important information</li>
      <li>Fully Connected Layers: Combine features for final classification</li>
      </ul>
		</div>
	</div>

  <div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">What is a Convolution</p>
      <p>Convolution is the process of applying a filter (or kernel) to an input to create a feature map.</p>
      <ul>
        <li>Input: Usually a 2D or 3D tensor (for images)</li>
        <li>Filter/Kernel: A small matrix of weights
        <li>Output: A feature map</li>
      </ul>
		</div>
	</div>

  <div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">What is a Kernel</p>
      <p>
        In mathematics, a kernel is a function used to define a convolution operation.
        The convolution operation is fundamental to CNNs.
      </p>
      <p>
        In image processing, kernels have been used for operations like blurring, sharpening, and edge detection.
        These operations involve sliding a small matrix (kernel) over an image, which is similar to how CNNs operate.
        Both in traditional image processing and CNNs, these small matrices perform a similar function of feature extraction or transformation.
        In essence, the term "kernel" emphasizes the mathematical operation and origins of the concept, while "filter" emphasizes its purpose in feature extraction. Both terms are correct and widely used in the field of deep learning and computer vision. The choice of term often depends on the context or the background of the person using it.
      </p>
		</div>
	</div>
</div>

<!-- -->


<div class="container mt-5">
	<h3 class="text-primary h4">Recurrent Neural Networks</h4>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<div class="row">
		<div class="col-sm-12">
      A class of artificial neural networks designed to work with sequential data or time series. They are particularly useful for tasks involving inputs or outputs of varying lengths, such as natural language processing, speech recognition, and time series prediction.
      RNNs have loops in them, allowing information to persist. They can process sequences of inputs, maintaining an internal state (memory).
      Feedforward Neural Networks do not have time dependency or memory effect. RNNs take temporal dimension into consideration by having a memory (internal state) (feedback loop)
		</div>
	</div>
  <br/>
	<div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">RNN Process Flow</p>
      <ul>
        <li>Input: Takes in a sequence of data (e.g., words, time series)</li>
        <li>Recurrent Layer: Processes input sequentially, maintaining hidden state</li>
        <li>Activation: Applies non-linearity (e.g., tanh, sigmoid)</li>
        <li>State Update: Updates hidden state based on current input and previous state</li>
        <li>Output: Generates output for each time step or final sequence output</li>
        <li>Backpropagation Through Time: Computes gradients and updates weights</li> 
      </ul>
		</div>
	</div>

  <div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">What is a Long Short-Term Memory (LSTM)</p>
      <p>type of Recurrent Neural Network (RNN) architecture designed to address the vanishing gradient problem that standard RNNs face when dealing with long-term dependencies. .</p>
      <p>LSTMs can selectively remember or forget information over long periods. They use a gating mechanism to control the flow of information</p>
      <p>LSTMs have been crucial in advancing the field of sequence modeling and continue to be widely used in various applications. Their ability to handle long-term dependencies makes them particularly useful in tasks where context over long sequences is important. While newer architectures like Transformers have surpassed LSTMs in some NLP tasks, LSTMs remain relevant and effective for many sequence modeling problems.</p>
		</div>
	</div>

  <div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">What is Vanishing Gradient Problem</p>
      <p>The vanishing gradient problem occurs when gradients become extremely small as they are backpropagated through time or layers, making it difficult for the network to learn long-term dependencies.</p>
      <p>
        Cause: Repeated multiplication of small numbers (< 1) during backpropagation. Use of certain activation functions (e.g., sigmoid, tanh) that squash values into a small range
      </p>
      <p>
        The vanishing gradient problem is a fundamental challenge in deep learning, particularly affecting RNNs and very deep feedforward networks. Recognizing and mitigating this issue is essential for developing effective deep learning models, especially those dealing with sequential or temporal data.
      </p>
      <p>
        Mitigations:
        <uL>
        <li>Use of LSTM or GRU architectures in RNNs</li>
        <li>Residual connections (as in ResNet)</li>
        <li>Careful initialization of weights</li>
        <li>Use of ReLU activation function</li>
        <li>Gradient clipping</li>
        <li>Batch normalization</li>
        </uL>
      </p>
      <p>
        Imagine you're passing a message in a very long game of telephone. By the time the message reaches the end, it might be completely different from the original. This is similar to what happens in regular neural networks with long sequences - important information from the beginning gets lost as it passes through many layers.
        - Think of LSTMs as having a special "memory highway" that runs alongside the regular path.
        - This highway allows important information to travel long distances without getting distorted.
        - LSTMs have "gates" that control what information goes on the highway.
        - These gates decide what's important enough to remember for a long time and what can be forgotten.
        - The memory highway provides a direct path from earlier parts of the sequence to later parts.  
        - It's like being able to whisper the original message directly to people further down the line in the telephone game.
        LSTMs give the network a better memory, allowing it to remember important stuff from long ago, which regular networks often forget. This improved memory helps the network learn and perform better on tasks involving long sequences of data.
      </p>
		</div>
	</div>


</div>





<div class="container mt-5">
	<h3 class="text-primary h4">Confusion Matrix</h4>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<div class="row">
		<div class="col-sm-12">
      A table used in machine learning and statistics to evaluate the performance of a classification model. It provides a summary of the model's predictions compared to the actual outcomes.
      <p>Confusion Matrix</p>
      <ul>      
      <li>True Positives (TP): Correctly predicted positive instances</li>
      <li>True Negatives (TN): Correctly predicted negative instances</li>
      <li>False Positives (FP): Incorrectly predicted positive instances (Type I error)</li>
      <li>False Negatives (FN): Incorrectly predicted negative instances (Type II error)</li>
      </ul>
      <p>Accuracy vs Loss</p>
      <ul>
        <li>Purpose: <ul>
            <li>Accuracy measures how often the model's predictions are correct</li>
            <li>Loss quantifies the model's errors and is used to optimize the model during training</li>
          </ul>
        </li>
        <li>Granularity: <ul>
            <li>Accuracy is a coarse measure (correct or incorrect)</li>
            <li>Loss provides a more nuanced view of how far off predictions are</li>
          </ul>
        </li>
        <li>Training use: <ul>
            <li>Loss is typically used to train the model (gradient descent minimizes loss)</li>
            <li>Accuracy is often used to evaluate the model's performance after training</li>
          </ul>
        </li>
        <li>Applicability: <ul>
            <li>Accuracy is primarily used for classification tasks</li>
            <li>Loss can be used for both classification and regression tasks</li>
          </ul>
        </li>
        <li>Sensitivity: <ul>
            <li>Accuracy doesn't distinguish between slight misses and major errors</li>
            <li>Loss captures the degree of error in predictions</li>
          </ul>
        </li>
        <li>Optimization: <ul>
            <li>Models are typically optimized to minimize loss, not maximize accuracy directly</li>
          </ul>
        </li>
      </ul>
		</div>
	</div>
  <br/>
	<div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">Key Performance Indicators</p>
      Classification Models:      
      Accuracy: Overall correctness of predictions
      Precision: Proportion of true positive predictions among all positive predictions
      Recall (Sensitivity): Proportion of true positive predictions among all actual positives
      F1 Score: Harmonic mean of precision and recall
      Specificity: Proportion of true negative predictions among all actual negatives
      ROC Curve and AUC: Receiver Operating Characteristic curve and Area Under the Curve
      Confusion Matrix: Table showing true positives, false positives, true negatives, and false negatives

		</div>
	</div>
  <br/>
	<div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">Specificity</p>
      Specificity is a key performance metric used in binary classification problems, particularly in medical testing and machine learning.
      Specificity is the proportion of actual negative cases that were correctly identified as negative by the model or test.
      Specificity = True Negatives / (True Negatives + False Positives)
      Key Points:
      Interpretation: It measures how well a model can identify negative cases.
      Range: 0 to 1 (or 0% to 100%)
      Perfect specificity: A score of 1 (100%) means the model correctly identified all negative cases.

      Specificity of 0.95 means 95% of healthy people are correctly identified as not having the disease.
      Complements sensitivity (recall): While sensitivity measures the ability to detect positive cases, specificity measures the ability to detect negative cases.
      Use Cases:      
      Medical screening: High specificity is crucial when false positives have significant consequences.
      Spam detection: High specificity ensures legitimate emails aren't marked as spam.
      Fraud detection: High specificity prevents flagging legitimate transactions as fraudulent.    

		</div>
	</div>



</div>



<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
