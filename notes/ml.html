<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning</h1>
  
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Convolutional Neural Networks</h4>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<div class="row">
		<div class="col-sm-12">
      A class of deep learning neural networks primarily used for analyzing visual imagery. They are particularly effective in image recognition, classification, and computer vision tasks. 
      CNNs have revolutionized the field of computer vision and continue to be a cornerstone of many AI applications dealing with image and video data. Their ability to automatically learn hierarchical features from raw pixel data has made them indispensable in modern machine learning and AI systems.
      Feature Location Invariant - Detect and classify objects and features that are not in a specific location.
		</div>
	</div>
  <br/>
	<div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">CNN Process Flow</p>
      <ul>
      <li>Input: Takes in an image as a matrix of pixel values</li>
      <li>Convolution: Applies filters to detect features (edges, textures, etc.)</li>
      <li>Activation: Applies non-linearity (e.g., ReLU)</li>
      <li>Pooling: Reduces dimensionality while retaining important information</li>
      <li>Fully Connected Layers: Combine features for final classification</li>
      </ul>
		</div>
	</div>

  <div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">What is a Convolution</p>
      <p>Convolution is the process of applying a filter (or kernel) to an input to create a feature map.</p>
      <ul>
        <li>Input: Usually a 2D or 3D tensor (for images)</li>
        <li>Filter/Kernel: A small matrix of weights
        <li>Output: A feature map</li>
      </ul>
		</div>
	</div>

  <div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">What is a Kernel</p>
      <p>
        In mathematics, a kernel is a function used to define a convolution operation.
        The convolution operation is fundamental to CNNs.
      </p>
      <p>
        In image processing, kernels have been used for operations like blurring, sharpening, and edge detection.
        These operations involve sliding a small matrix (kernel) over an image, which is similar to how CNNs operate.
        Both in traditional image processing and CNNs, these small matrices perform a similar function of feature extraction or transformation.
        In essence, the term "kernel" emphasizes the mathematical operation and origins of the concept, while "filter" emphasizes its purpose in feature extraction. Both terms are correct and widely used in the field of deep learning and computer vision. The choice of term often depends on the context or the background of the person using it.
      </p>
		</div>
	</div>
</div>

<!-- -->


<div class="container mt-5">
	<h3 class="text-primary h4">Recurrent Neural Networks</h4>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<div class="row">
		<div class="col-sm-12">
      A class of artificial neural networks designed to work with sequential data or time series. They are particularly useful for tasks involving inputs or outputs of varying lengths, such as natural language processing, speech recognition, and time series prediction.
      RNNs have loops in them, allowing information to persist. They can process sequences of inputs, maintaining an internal state (memory).
      Feedforward Neural Networks do not have time dependency or memory effect. RNNs take temporal dimension into consideration by having a memory (internal state) (feedback loop)
		</div>
	</div>
  <br/>
	<div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">RNN Process Flow</p>
      <ul>
        <li>Input: Takes in a sequence of data (e.g., words, time series)</li>
        <li>Recurrent Layer: Processes input sequentially, maintaining hidden state</li>
        <li>Activation: Applies non-linearity (e.g., tanh, sigmoid)</li>
        <li>State Update: Updates hidden state based on current input and previous state</li>
        <li>Output: Generates output for each time step or final sequence output</li>
        <li>Backpropagation Through Time: Computes gradients and updates weights</li> 
      </ul>
		</div>
	</div>

  <div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">What is a Long Short-Term Memory (LSTM)</p>
      <p>type of Recurrent Neural Network (RNN) architecture designed to address the vanishing gradient problem that standard RNNs face when dealing with long-term dependencies. .</p>
      <p>LSTMs can selectively remember or forget information over long periods. They use a gating mechanism to control the flow of information</p>
      <p>LSTMs have been crucial in advancing the field of sequence modeling and continue to be widely used in various applications. Their ability to handle long-term dependencies makes them particularly useful in tasks where context over long sequences is important. While newer architectures like Transformers have surpassed LSTMs in some NLP tasks, LSTMs remain relevant and effective for many sequence modeling problems.</p>
		</div>
	</div>

  <div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">What is Vanishing Gradient Problem</p>
      <p>The vanishing gradient problem occurs when gradients become extremely small as they are backpropagated through time or layers, making it difficult for the network to learn long-term dependencies.</p>
      <p>
        Cause: Repeated multiplication of small numbers (< 1) during backpropagation. Use of certain activation functions (e.g., sigmoid, tanh) that squash values into a small range
      </p>
      <p>
        The vanishing gradient problem is a fundamental challenge in deep learning, particularly affecting RNNs and very deep feedforward networks. Recognizing and mitigating this issue is essential for developing effective deep learning models, especially those dealing with sequential or temporal data.
      </p>
      <p>
        Mitigations:
        <uL>
        <li>Use of LSTM or GRU architectures in RNNs</li>
        <li>Residual connections (as in ResNet)</li>
        <li>Careful initialization of weights</li>
        <li>Use of ReLU activation function</li>
        <li>Gradient clipping</li>
        <li>Batch normalization</li>
        </uL>
      </p>
      <p>
        Imagine you're passing a message in a very long game of telephone. By the time the message reaches the end, it might be completely different from the original. This is similar to what happens in regular neural networks with long sequences - important information from the beginning gets lost as it passes through many layers.
        - Think of LSTMs as having a special "memory highway" that runs alongside the regular path.
        - This highway allows important information to travel long distances without getting distorted.
        - LSTMs have "gates" that control what information goes on the highway.
        - These gates decide what's important enough to remember for a long time and what can be forgotten.
        - The memory highway provides a direct path from earlier parts of the sequence to later parts.  
        - It's like being able to whisper the original message directly to people further down the line in the telephone game.
        LSTMs give the network a better memory, allowing it to remember important stuff from long ago, which regular networks often forget. This improved memory helps the network learn and perform better on tasks involving long sequences of data.
      </p>
		</div>
	</div>


</div>





<div class="container mt-5">
	<h3 class="text-primary h4">Confusion Matrix</h4>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<div class="row">
		<div class="col-sm-12">
      A table used in machine learning and statistics to evaluate the performance of a classification model. It provides a summary of the model's predictions compared to the actual outcomes.
      <p>Confusion Matrix</p>
      <ul>      
      <li>True Positives (TP): Correctly predicted positive instances</li>
      <li>True Negatives (TN): Correctly predicted negative instances</li>
      <li>False Positives (FP): Incorrectly predicted positive instances (Type I error)</li>
      <li>False Negatives (FN): Incorrectly predicted negative instances (Type II error)</li>
      </ul>
      <p>Accuracy vs Loss</p>
      <ul>
        <li>Purpose: <ul>
            <li>Accuracy measures how often the model's predictions are correct</li>
            <li>Loss quantifies the model's errors and is used to optimize the model during training</li>
          </ul>
        </li>
        <li>Granularity: <ul>
            <li>Accuracy is a coarse measure (correct or incorrect)</li>
            <li>Loss provides a more nuanced view of how far off predictions are</li>
          </ul>
        </li>
        <li>Training use: <ul>
            <li>Loss is typically used to train the model (gradient descent minimizes loss)</li>
            <li>Accuracy is often used to evaluate the model's performance after training</li>
          </ul>
        </li>
        <li>Applicability: <ul>
            <li>Accuracy is primarily used for classification tasks</li>
            <li>Loss can be used for both classification and regression tasks</li>
          </ul>
        </li>
        <li>Sensitivity: <ul>
            <li>Accuracy doesn't distinguish between slight misses and major errors</li>
            <li>Loss captures the degree of error in predictions</li>
          </ul>
        </li>
        <li>Optimization: <ul>
            <li>Models are typically optimized to minimize loss, not maximize accuracy directly</li>
          </ul>
        </li>
      </ul>
		</div>
	</div>
  <br/>
	<div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">Key Performance Indicators</p>
      Classification Models:      
      Accuracy: Overall correctness of predictions
      Precision: Proportion of true positive predictions among all positive predictions
      Recall (Sensitivity): Proportion of true positive predictions among all actual positives
      F1 Score: Harmonic mean of precision and recall
      Specificity: Proportion of true negative predictions among all actual negatives
      ROC Curve and AUC: Receiver Operating Characteristic curve and Area Under the Curve
      Confusion Matrix: Table showing true positives, false positives, true negatives, and false negatives

		</div>
	</div>
  <br/>
	<div class="row">
		<div class="col-sm-12">
      <p style="color:blue;">Specificity</p>
      Specificity is a key performance metric used in binary classification problems, particularly in medical testing and machine learning.
      Specificity is the proportion of actual negative cases that were correctly identified as negative by the model or test.
      Specificity = True Negatives / (True Negatives + False Positives)
      Key Points:
      Interpretation: It measures how well a model can identify negative cases.
      Range: 0 to 1 (or 0% to 100%)
      Perfect specificity: A score of 1 (100%) means the model correctly identified all negative cases.

      Specificity of 0.95 means 95% of healthy people are correctly identified as not having the disease.
      Complements sensitivity (recall): While sensitivity measures the ability to detect positive cases, specificity measures the ability to detect negative cases.
      Use Cases:      
      Medical screening: High specificity is crucial when false positives have significant consequences.
      Spam detection: High specificity ensures legitimate emails aren't marked as spam.
      Fraud detection: High specificity prevents flagging legitimate transactions as fraudulent.    

		</div>
	</div>



</div>



<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
