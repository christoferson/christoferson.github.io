<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>

    
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
    <style>
        details {
            border: 1px solid #aaa;
            border-radius: 2px;
            padding: .5em .5em 0;
            color: indigo;
            font-size: 12px;
        }
    
        summary {
            font-weight: bold;
            margin: -.5em -.5em 0;
            padding: .5em;
            cursor: pointer;
        }
    
        details[open] {
            padding: .5em;
        }
    
        details[open] summary {
            border-bottom: 1px solid #aaa;
            margin-bottom: .5em;
        }
    </style>

</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning Engineer Associate (MLA) - Services SageMaker - Explain</h1>  
</div>



<div style="color: darkmagenta;font-size: 20px;padding:5px;">Model Explain</div>
<hr style="height: 12px;background-color:#0066cc"/>




<div class="container mt-5">
	<h3 class="text-primary h4">Evaluate, explain, and detect bias in models</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
			<p style="color: #333333; font-size: 16px;">Amazon SageMaker offers features to improve your machine learning (ML) models by detecting potential bias and helping to explain the predictions that your models make from various types of datasets. It helps identify bias in pre-training data and post-training that can emerge during model training or when the model is in production. You can also evaluate language models for quality and responsibility metrics using foundation model evaluations.</p> <p style="color: #333333; font-size: 16px;">The main topics covered are:</p> <ul> <li>Use SageMaker Clarify to evaluate large language models</li> <li>Use SageMaker Clarify to explain and detect bias</li> <li>Use SageMaker Clarify explainability with SageMaker Autopilot</li> </ul> <p style="color: #333333; font-size: 16px;">SageMaker Clarify for evaluating large language models:</p> <ul> <li>Allows evaluation of text-based foundation models from JumpStart</li> <li>Supports models already deployed to an endpoint</li> <li>Offers three approaches for creating model evaluation jobs: <ol> <li>Automated model evaluation jobs in Studio</li> <li>Model evaluation jobs using human workers in Studio</li> <li>Automated model evaluation jobs using the fmeval library</li> </ol> </li> <li>Supports common LLM use cases: <ul> <li>Open-ended generation</li> <li>Text summarization</li> <li>Question answering</li> <li>Classification</li> </ul> </li> </ul> <p style="color: #333333; font-size: 16px;">Foundation Model Evaluations:</p> <ul> <li>Help quantify model risks such as inaccurate, toxic, or biased content</li> <li>Assist in complying with international guidelines for responsible generative AI</li> <li>Offer various evaluation tasks and metrics</li> <li>Allow updating inference parameters to influence model output</li> </ul> <p style="color: #333333; font-size: 16px;">Automatic Model Evaluation Jobs:</p> <ul> <li>Use metrics based on benchmarks to measure model responses</li> <li>Support single model evaluation</li> <li>Can use built-in datasets or custom prompt datasets</li> <li>Generate reports with visualizations and examples</li> <li>Results are saved in a specified Amazon S3 bucket</li> </ul> <p style="color: #333333; font-size: 16px;">Prompt Templates:</p> <ul> <li>SageMaker Clarify automatically augments input prompts for optimal performance</li> <li>Default prompt templates are provided based on the selected model and evaluation dimensions</li> <li>Custom prompt templates can be created by modifying the default templates</li> </ul> <p style="color: #333333; font-size: 16px;">Model Evaluation Jobs with Human Workers:</p> <ul> <li>Allow manual evaluation of model responses for subjective dimensions</li> <li>Can compare responses from up to two JumpStart models</li> <li>Require a custom prompt dataset stored in Amazon S3</li> <li>Allow defining evaluation criteria and instructions for human workers</li> <li>Support creation of work teams in Studio</li> </ul> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Automatic Evaluation</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Human Worker Evaluation</strong></td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Number of models</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Single model</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Up to two models</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Dataset options</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Built-in or custom</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Custom only</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Evaluation method</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Automated metrics</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Human judgment</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Best for</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Objective measurements</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Subjective dimensions</td> </tr> </table>

			<hr />

			<p style="color: #333333; font-size: 16px;">SageMaker provides three options for evaluating Large Language Models (LLMs):</p> <ul> <li>Set up manual evaluations for a human workforce using Studio</li> <li>Evaluate your model with an algorithm using Studio</li> <li>Automatically evaluate your model with a customized workflow using the fmeval library</li> </ul> <p style="color: #333333; font-size: 16px;">Human Work Team Evaluations:</p> <ul> <li>Can evaluate and compare up to two models concurrently</li> <li>Use metrics indicating preference for one response over another</li> <li>Workflow, metrics, and instructions can be tailored to specific use cases</li> <li>Provide more refined evaluation than algorithmic methods</li> </ul> <p style="color: #333333; font-size: 16px;">Algorithmic Evaluations in Studio:</p> <ul> <li>Use benchmarks to rapidly score model responses</li> <li>Provide a guided workflow for evaluating JumpStart model responses</li> <li>Use pre-defined metrics specific to generative AI tasks</li> <li>Can use built-in or custom datasets</li> </ul> <p style="color: #333333; font-size: 16px;">Customized Evaluations with fmeval Library:</p> <ul> <li>Allows for more customized workflows than Studio</li> <li>Uses Python code and the fmeval library</li> <li>Can evaluate any text-based LLM, including non-JumpStart models</li> </ul> <p style="color: #333333; font-size: 16px;">Available Evaluation Dimensions and Datasets:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Evaluation Dimension</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Metrics</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Datasets</strong></td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Human Evaluation</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Preference rate, Preference strength, Preference rank, Approval rate, Approval strength</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Custom datasets</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Factual Knowledge</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Semantic robustness</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">TREX, BOLD, WikiText</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Prompt Stereotyping</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">-</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">CrowS-Pairs</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Toxicity</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">-</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">RealToxicityPrompts, BOLD</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Text Summarization</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Accuracy (ROUGE-N, BERTScore), Semantic robustness, Toxicity</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Government Report Dataset, Gigaword</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Question Answering</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Accuracy (Exact match, Quasi exact match, F1 over words), Semantic robustness, Toxicity</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">BoolQ, NaturalQuestions, TriviaQA</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Text Classification</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Accuracy (Classification accuracy, Precision, Recall, Balanced classification accuracy), Semantic robustness</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Women's Ecommerce Clothing Reviews</td> </tr> </table> <p style="color: #333333; font-size: 16px;">These evaluation options provide a comprehensive approach to assessing LLMs, allowing users to choose between human-based and algorithmic evaluations, or to create custom evaluation workflows using the fmeval library.</p>

			<hr />

			<p style="color: #333333; font-size: 16px;">Accuracy evaluation measures how accurately a model performs in a task by comparing the model output to the ground truth answer included in the dataset. SageMaker supports running accuracy evaluations from Amazon SageMaker Studio or using the fmeval library.</p> <p style="color: #333333; font-size: 16px;">Supported task types and their associated built-in datasets:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Task Type</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Built-in Datasets</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Notes</strong></td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Text summarization</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Gigaword, Government Report Dataset</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">English language only, but some metrics are language-agnostic</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Question answering</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">BoolQ, NaturalQuestions, TriviaQA</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">English language only, but some metrics are language-agnostic</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Classification</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Women's E-Commerce Clothing Reviews</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"></td> </tr> </table> <p style="color: #333333; font-size: 16px;">Computed values for each task type:</p> <p style="color: #333333; font-size: 16px;">1. Summarization:</p> <ul> <li>ROUGE score (0-1, higher is better)</li> <li>METEOR score (higher is better)</li> <li>BERTScore (uses BERT-family model for sentence embeddings)</li> </ul> <p style="color: #333333; font-size: 16px;">2. Question answering:</p> <ul> <li>Precision Over Words score (0-1)</li> <li>Recall Over Words score (0-1)</li> <li>F1 Over Words score (0-1)</li> <li>Exact Match (EM) score (0 or 1)</li> <li>Quasi Exact Match score (0 or 1)</li> </ul> <p style="color: #333333; font-size: 16px;">3. Classification:</p> <ul> <li>Accuracy score (0 or 1)</li> <li>Precision score (0-1)</li> <li>Recall score (0-1)</li> <li>Balanced classification accuracy (0-1)</li> </ul> <p style="color: #333333; font-size: 16px;">Key points:</p> <ul> <li>SageMaker samples 100 random prompts by default for accuracy evaluation</li> <li>Users can bring their own datasets with ground truth components</li> <li>The fmeval library offers more configuration options for evaluations</li> <li>Some metrics have limitations, especially for abstractive summarization tasks</li> <li>Classification metrics can be configured for different multiclass averaging strategies</li> </ul>
            
			<hr />

			<p style="color: #333333; font-size: 16px;">This section covers three types of evaluations for language models: Factual Knowledge, Prompt Stereotyping, and Semantic Robustness. Here's a summary of each:</p> <p style="color: #333333; font-size: 16px;"><strong>1. Factual Knowledge Evaluation</strong></p> <ul> <li>Evaluates the model's ability to reproduce facts about the real world</li> <li>Supported task type: Open-ended generation</li> <li>Built-in dataset: T-REx (English only)</li> <li>Computed value: Binary metric (0 or 1) averaged across prompts</li> </ul> <p style="color: #333333; font-size: 16px;"><strong>2. Prompt Stereotyping Evaluation</strong></p> <ul> <li>Measures the probability that the model encodes biases in its response</li> <li>Supported task type: Open-ended generation</li> <li>Built-in dataset: CrowS-Pairs (English only, US-centric)</li> <li>Computed values: <ul> <li>Is_biased: Binary metric (0 or 1) averaged over the dataset</li> <li>log_probability_difference: Numerical score indicating stereotyping intensity</li> </ul> </li> </ul> <p style="color: #333333; font-size: 16px;"><strong>3. Semantic Robustness Evaluation</strong></p> <ul> <li>Evaluates how much model output changes due to small, semantic-preserving input changes</li> <li>Supported task types: Text summarization, Question answering, Classification, Open-ended generation</li> <li>Perturbation types: Butter Fingers, Random Upper Case, Whitespace Add Remove</li> <li>Computed values vary by task type: <ul> <li>Summarization: Delta ROUGE, METEOR, and BERTScore</li> <li>Question answering: Delta F1, Exact Match, Quasi Exact Match, Precision, and Recall scores</li> <li>Classification: Delta Accuracy score</li> <li>Open-ended generation: Word Error Rate (WER) and BERTScore Dissimilarity (BSD)</li> </ul> </li> </ul> <p style="color: #333333; font-size: 16px;">Key points:</p> <ul> <li>Evaluations can be run from Amazon SageMaker Studio or using the fmeval library</li> <li>fmeval library offers more configuration options</li> <li>Users can bring their own datasets for all evaluations</li> <li>Default sample size is 100 random datapoints, adjustable with fmeval</li> <li>Semantic Robustness for open-ended generation must use the fmeval library</li> <li>Non-deterministic models are accounted for in Semantic Robustness evaluation</li> </ul> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Evaluation Type</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Main Purpose</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Key Metric</strong></td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Factual Knowledge</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Assess ability to reproduce real-world facts</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Binary score (0 or 1)</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Prompt Stereotyping</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Detect biases in model responses</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Is_biased score (0-1)</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Semantic Robustness</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Measure output stability with small input changes</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Various Delta scores</td> </tr> </table>

			<hr />

			<p style="color: #333333; font-size: 16px;">Toxicity Evaluation in Amazon SageMaker assesses generated text for various forms of toxic content. Here's a summary of the key points:</p> <p style="color: #333333; font-size: 16px;"><strong>Supported Task Types and Datasets:</strong></p> <ul> <li>Text summarization: Gigaword, Government Report Dataset</li> <li>Question answering: BoolQ, NaturalQuestions, TriviaQA</li> <li>Open-ended generation: Real toxicity prompts, Real toxicity prompts-challenging, BOLD</li> </ul> <p style="color: #333333; font-size: 16px;"><strong>Toxicity Detectors:</strong></p> <ul> <li>UnitaryAI Detoxify-unbiased (default in Studio and fmeval library)</li> <li>Toxigen-roberta (available in fmeval library)</li> </ul> <p style="color: #333333; font-size: 16px;"><strong>UnitaryAI Detoxify-unbiased Scores:</strong></p> <ul> <li>Main score: Toxicity (0-1)</li> <li>Additional scores: severe_toxicity, obscene, threat, insult, sexual_explicit, identity_attack</li> </ul> <p style="color: #333333; font-size: 16px;"><strong>Toxigen-roberta Score:</strong></p> <ul> <li>Single score: Toxicity (0-1)</li> </ul> <p style="color: #333333; font-size: 16px;"><strong>Key Points:</strong></p> <ul> <li>Default sample size: 100 random datapoints (adjustable with fmeval)</li> <li>Custom datasets are supported</li> <li>Toxicity detection is limited to English language</li> <li>Toxicity concept is culturally and contextually dependent</li> <li>Scores may be biased or unreliable due to model-based evaluation</li> <li>User-provided toxicity detectors are not supported</li> </ul> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Studio Evaluations</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>fmeval Library Evaluations</strong></td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Default Toxicity Detector</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">UnitaryAI Detoxify-unbiased</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">UnitaryAI Detoxify-unbiased</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Toxicity Detector Options</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Fixed</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Configurable (UnitaryAI or Toxigen-roberta)</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Customization</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Limited</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">More flexible</td> </tr> </table> <p style="color: #333333; font-size: 16px;">When interpreting toxicity evaluation results, it's important to consider the limitations of the toxicity detectors and the cultural context of the content being evaluated.</p>

			<hr />

			<p style="color: #333333; font-size: 16px;">Human evaluation in Amazon SageMaker allows you to use human workers to assess and compare model responses. Here's a summary of the key points:</p> <p style="color: #333333; font-size: 16px;"><strong>Setting up a Human Evaluation:</strong></p> <ul> <li>Requires proper environment setup with correct permissions</li> <li>Uses the model evaluation job wizard in Amazon SageMaker Studio</li> <li>Allows selection of models, definition of parameters, and workforce selection</li> </ul> <p style="color: #333333; font-size: 16px;"><strong>Key Features:</strong></p> <ul> <li>Can evaluate and compare up to two models concurrently</li> <li>Supports inference data from models hosted outside of SageMaker and AWS</li> <li>Results are saved in Amazon S3 as a jsonlines output file</li> <li>Provides a report to understand how the workforce evaluated the models</li> </ul> <p style="color: #333333; font-size: 16px;"><strong>Important Considerations:</strong></p> <ul> <li>Custom IAM policies must grant permissions to add tags to SageMaker resources</li> <li>AWS Managed Policies for Amazon SageMaker include necessary tagging permissions</li> <li>"AccessDenied" errors may occur if tagging permissions are not granted</li> </ul> <p style="color: #333333; font-size: 16px;"><strong>Process Overview:</strong></p> <ol> <li>Set up environment with correct permissions</li> <li>Use Studio's model evaluation job wizard to: <ul> <li>Select models for evaluation</li> <li>Define job parameters</li> <li>Choose the workforce</li> </ul> </li> <li>Run the evaluation job</li> <li>View the evaluation report</li> <li>Access detailed results in Amazon S3</li> </ol> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Model Comparison</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Up to two models can be evaluated and compared</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Model Source</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Supports models from SageMaker, outside SageMaker, and outside AWS</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Result Storage</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Amazon S3 (jsonlines format)</td> </tr> <tr> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Reporting</td> <td style="border: 1px solid #dddddd; text-align: left; padding: 8px;">Provides a summary report of workforce evaluations</td> </tr> </table> <p style="color: #333333; font-size: 16px;">Human evaluations offer a way to get subjective assessments of model performance, which can be particularly useful for tasks that require nuanced understanding or where automated metrics may not capture all aspects of model quality.</p>

		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Use SageMaker Clarify to explain and detect bias</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>





<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
