


Organize the following transcript into easy to understand article and study material. 
Emphasize the output contents with the following exam guide:

Knowledge of: 
 IAM roles, policies, and groups that control access to AWS services (for 
example, AWS Identity and Access Management [IAM], bucket policies, 
SageMaker Role Manager) 
 SageMaker security and compliance features 
 Controls for network access to ML resources 
 Security best practices for CI/CD pipelines 
 


For each topic, output a separate set of html content like in the following example.
Topic-1:
<p>xxxxxx</p><ul><li>foo</li><li>bar</li></ul>

Topic-2:
<p>xxxxxx</p><ul><li>foo</li><li>bar</li></ul>

Topic-3:
<p>xxxxxx</p><ul><li>foo</li><li>bar</li></ul>

Also, add information regarding the gotchas and insights regarding the services that can invalidate it as a solution.
For example, sagemaker random cut forest has high operational overhead compared to apache flink random cut to detect anomaly.
Another example is that if the solution needs real time processing, then we need kinesis data stream and not kinesis data firehose.
Final example is that if the question assumes use of kinesis firehose, then we need to consider only supported destinations.
Make sure to provide supplementary information e.g. don't just say the support destinations, enumerate what are they.

Even if not mentioned in the transcript, feel free to supplement information in order to explain the concepts and hard to understand.

Format the output using <ul> <li> <p> tags with inline styles to provide font colors. 
Do not use headers e.g. h2, and use inline styles to set the font color for emphasis. 
Also, do not set the font size unless necessary and set a max font size of 16px.
Wrap section headers using <p style="color: goldenrod; font-size:14px;"><strong>.

Here is the transcript:

Let's get started with the third task statement from Domain 2, which is to analyze model performance. This task statement is split into two lessons. After you built your model, you can evaluate how well your model performed on your data before using it to make predictions. You can use information to determine whether your model can make accurate predictions for your data. For the exam, ensure you understand the different calculations that can be used to evaluate the performance of models such as precision, accuracy, recall, specificity, false positive rate, receiver operating characteristics, area under the curve, the F1 score, root mean squared error, mean absolute percentage error, and more. Here's a question. You have deployed an Amazon SageMaker endpoint in productions to generate predictions. You need a solution to visualize the prediction's precision recall curve. What is your solution? One solution is to generate the precision recall data by executing a daily Amazon EMR workflow, storing the results in an S3 bucket, and using Amazon QuickSight to produce a dashboard of the results. Here's another question. You want to load test the model you have developed using Amazon SageMaker notebook instance to right size the instance before deploying to production. What is your solution to visualize the load tests? One solution is to create a CloudWatch dashboard to build a unified operational view of the metrics generated by the notebook instance. You can monitor SageMaker using Amazon CloudWatch and build customized dashboards for your CloudWatch metrics. Let's also talk about how to create a model quality baseline in AWS to compare your model predictions in AWS. You can use Amazon SageMaker Ground Truth with Ground Truth labels in a baseline dataset that you have stored in Amazon S3. You can also use model quality monitoring jobs to monitor the performance of a model and compare the predictions that the model makes with the actual Ground Truth labels that the model attempts to predict. To measure model quality, SageMaker Model Monitor uses metrics that depend on the machine learning problem type. For example, if your model is for a regression problem, one of the metrics evaluated is mean squared error. I added a link to all of the metrics used for the different machine learning problem types in SageMaker. I'm gonna stop this lesson here, and in the next lesson we'll continue talking about task statement 2.3. 

Let's continue with task statement 2.3, which is to analyze model performance. How do you gain insights into your machine learning training data and models in AWS? You can use SageMaker Clarify that has post-training data and model bias metrics to help quantify various conceptions of fairness. You can also use SageMaker Canvas, which provides an overview and scoring information for the different types of models to help you determine how accurate your model is when it makes predictions. And if you use Amazon QuickSight, you can use SageMaker Canvas in your QuickSight visualizations. Also to help identify convergence issues and gain visibility into your models, SageMaker provides two debugging tools. First, the SageMaker Debugger, which provides tools to register hooks to callbacks to extract model output tensors and save them in Amazon S3. It provides built-in rules for detecting model convergence issues such as overfitting, saturated activation functions, vanishing gradients, and more. You can also set up the built-in rules with Amazon CloudWatch Events and AWS Lambda for taking automated actions against detected issues and set up Amazon's simple notification service to receive email or text notifications. And the second debugging tool is SageMaker with TensorBoard, which hosts TensorBoard as an application in SageMaker domain. I added links for both of these under additional resources. SageMaker also provides a shadow testing to evaluate any changes to your model serving infrastructure by comparing its performance against the currently deployed infrastructure. Using shadow testing, you can monitor the progress, help identify potential configuration errors and performance issues, and take actions on the results. We know that machine learning is an iterative process, and when we're solving a new use case, we iterate through various parameters to find the best model hyperparameters that can be used to solve the identified challenge. Over time, after experimenting with multiple models and hyperparameters, it becomes difficult for machine learning teams to manage model runs to find the optimal one without a tool to keep track of the different experiments. SageMaker Experiments helps to log your model metrics, parameters, files, artifacts, and can plot charts from the different metrics, capture various metadata, search and support model reproducibility. Then you can compare the performance and hyperparameters for model evaluation through visual charts and tables. You can also use SageMaker Experiments to download the creative charts and share the model evaluation with stakeholders. Let's wrap up this task statement. Model fit is important for understanding the root cause for poor model accuracy. You can determine if a predictive model is underfitting or overfitting the training data using the prediction error on the training data and evaluation data. What are things you can do to increase your model accuracy? If your model is underfitting, you might increase the model flexibility and add new domain-specific features or more feature Cartesian products, and you might change the type of feature processing such as increasing your n-gram size and decreasing the amount of regularization used. If your model is overfitting the training data, you might reduce the model flexibility and consider using fewer feature combinations, decrease the n-gram size and decrease the amount of regularization use. Also, you might have poor accuracy on the training and test data because the algorithm did not have enough data to learn from. To improve performance, you might increase the amount of training data examples and increase the number of passes on the existing training data. Ensure you understand model fits and how the model fit is important for understanding the root cause if your model has poor accuracy. For your data collection, increase the number of training examples. For feature processing, add more variables and better feature processing. And for model parameter tuning, use different values for the training parameters used by your learning algorithm. Let's get started with our third walkthrough question. 



---


----
You are an expert in creating certification exam study material. 
Can you create a detailed output for Topic 1. Feel free to use your own knowledge and intelligence to supplement the explanations as you see fit.



-----

Looking back at all the information. Think step by step and come up with the best set of explanation, comparison tables etc. that you think would be very useful. Decide on the format by yourself and to he best of your ability provide the best output.


--
Can you create 8 example exam questions. Place the answers inline immediately after every question.
You can use an expander to initially hide the answer. 
Wrap answer choices in <li> to make sure they are presented in separate line.