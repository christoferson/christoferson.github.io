<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>

    
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
    <style>
        details {
            border: 1px solid #aaa;
            border-radius: 2px;
            padding: .5em .5em 0;
            color: indigo;
            font-size: 12px;
        }
    
        summary {
            font-weight: bold;
            margin: -.5em -.5em 0;
            padding: .5em;
            cursor: pointer;
        }
    
        details[open] {
            padding: .5em;
        }
    
        details[open] summary {
            border-bottom: 1px solid #aaa;
            margin-bottom: .5em;
        }
    </style>

</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning Engineer Associate (MLA) - Services SageMaker</h1>  
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">SageMaker Model Monitor</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="font-size: 16px; color: #333;">Amazon SageMaker Model Monitor is a powerful tool for monitoring machine learning models in production. It offers several key features:</p> <ul> <li>Continuous monitoring for real-time endpoints and batch transform jobs</li> <li>On-schedule monitoring for asynchronous batch transform jobs</li> <li>Automated alerts for quality deviations</li> <li>Prebuilt monitoring capabilities and flexibility for custom analysis</li> </ul> <p style="font-size: 16px; color: #333;">SageMaker Model Monitor provides four main types of monitoring:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Monitoring Type</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Data Quality</td> <td style="border: 1px solid #ddd; padding: 8px;">Monitors drift in data quality</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Model Quality</td> <td style="border: 1px solid #ddd; padding: 8px;">Monitors drift in model quality metrics (e.g., accuracy)</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Bias Drift</td> <td style="border: 1px solid #ddd; padding: 8px;">Monitors bias in model predictions</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Feature Attribution Drift</td> <td style="border: 1px solid #ddd; padding: 8px;">Monitors drift in feature attribution</td> </tr> </table> <p style="font-size: 16px; color: #333;"><strong>Data Quality Monitoring:</strong></p> <ul> <li>Automatically monitors ML models in production</li> <li>Uses rules to detect data drift</li> <li>Steps include enabling data capture, creating a baseline, scheduling monitoring jobs, and viewing metrics</li> <li>Integrates with Amazon CloudWatch for alerts</li> </ul> <p style="font-size: 16px; color: #333;"><strong>Model Quality Monitoring:</strong></p> <ul> <li>Compares model predictions with actual Ground Truth labels</li> <li>Uses metrics specific to the ML problem type (e.g., mean square error for regression)</li> <li>Requires merging of actual labels with captured prediction data</li> <li>Follows similar steps to data quality monitoring with additional label ingestion</li> </ul> <p style="font-size: 16px; color: #333;"><strong>Bias Drift Monitoring:</strong></p> <ul> <li>Helps detect bias in deployed ML models over time</li> <li>Uses bias metrics (e.g., DPPL) to evaluate model fairness</li> <li>Employs confidence intervals for statistical significance</li> <li>Allows setting of thresholds for automated alerts</li> </ul> <p style="font-size: 16px; color: #333;"><strong>Feature Attribution Drift Monitoring:</strong></p> <ul> <li>Monitors changes in feature importance over time</li> <li>Uses Normalized Discounted Cumulative Gain (NDCG) to compare feature attribution rankings</li> <li>Raises alerts if NDCG value falls below 0.90</li> <li>Helps identify significant changes in feature importance between training and live data</li> </ul> <p style="font-size: 16px; color: #333;">SageMaker Model Monitor provides sample notebooks for hands-on experience with bias drift and feature attribution drift monitoring. These notebooks demonstrate how to capture inference data, create baselines, and inspect results using SageMaker Studio.</p> <p style="font-size: 16px; color: #333;">By leveraging these monitoring capabilities, data scientists and ML engineers can ensure the ongoing quality, fairness, and reliability of their deployed machine learning models in production environments.</p>

            <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">1. Data Quality Monitoring</strong></p> <p style="font-size: 14px;">Data quality monitoring automatically tracks the statistical properties of a model's input data over time.</p> <ul> <li><strong>What is monitored:</strong> Statistical properties of input features, such as mean, median, standard deviation, and distribution.</li> <li><strong>Example:</strong> For a credit scoring model, you might monitor the average income, age distribution, or the proportion of categorical values in the 'employment_type' feature.</li> </ul> <p style="font-size: 14px;">Additional details:</p> <ul> <li>Uses the Deequ library (built on Apache Spark) to compute baseline statistics and constraints.</li> <li>Supports both numerical and categorical features.</li> <li>Can detect schema changes, such as new or missing columns.</li> <li>Allows for custom preprocessing and postprocessing scripts to transform data.</li> </ul> <p style="font-size: 14px;">Steps for implementation:</p> <ol> <li>Enable data capture for your endpoint or batch transform job.</li> <li>Create a baseline using a representative dataset.</li> <li>Define and schedule monitoring jobs.</li> <li>View metrics and results in Amazon SageMaker Studio or CloudWatch.</li> </ol> <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">2. Model Quality Monitoring</strong></p> <p style="font-size: 14px;">Model quality monitoring assesses the performance of a model by comparing its predictions against actual ground truth labels.</p> <ul> <li><strong>What is monitored:</strong> Performance metrics specific to the model type, such as accuracy, F1 score, mean squared error, etc.</li> <li><strong>Example:</strong> For a binary classification model predicting customer churn, you might monitor the AUC-ROC score or the precision-recall curve.</li> </ul> <p style="font-size: 14px;">Additional details:</p> <ul> <li>Requires a mechanism to obtain ground truth labels for comparison.</li> <li>Supports various problem types: binary classification, multiclass classification, regression.</li> <li>Can compute confusion matrix for classification problems.</li> <li>Allows for custom metrics definition.</li> </ul> <p style="font-size: 14px;">Implementation process:</p> <ol> <li>Set up data capture for model inputs and outputs.</li> <li>Create a baseline with initial performance metrics.</li> <li>Configure a system to ingest ground truth labels.</li> <li>Schedule regular monitoring jobs to compare predictions with ground truth.</li> <li>Set up CloudWatch alerts for performance degradation.</li> </ol> <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">3. Bias Drift Monitoring</strong></p> <p style="font-size: 14px;">Bias drift monitoring helps detect changes in model fairness over time, ensuring that the model doesn't discriminate against certain groups.</p> <ul> <li><strong>What is monitored:</strong> Bias metrics such as Demographic Parity Difference (DPD), Equal Opportunity Difference (EOD), or Disparate Impact (DI).</li> <li><strong>Example:</strong> In a loan approval model, you might monitor the approval rate difference between male and female applicants to ensure it stays within an acceptable range.</li> </ul> <p style="font-size: 14px;">Additional details:</p> <ul> <li>Uses SageMaker Clarify for bias detection and monitoring.</li> <li>Supports both pre-training and post-training bias metrics.</li> <li>Employs statistical techniques like bootstrap intervals for robust bias estimation.</li> <li>Allows setting custom thresholds for bias alerts.</li> </ul> <p style="font-size: 14px;">Key steps:</p> <ol> <li>Identify sensitive attributes in your data.</li> <li>Create a bias baseline using a representative dataset.</li> <li>Configure regular bias monitoring jobs.</li> <li>Set up alerts in CloudWatch for significant bias drift.</li> <li>Analyze bias reports in SageMaker Studio.</li> </ol> <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">4. Feature Attribution Drift Monitoring</strong></p> <p style="font-size: 14px;">Feature attribution drift monitoring tracks changes in the importance of different features to model predictions over time.</p> <ul> <li><strong>What is monitored:</strong> Changes in feature importance rankings and magnitudes compared to the baseline.</li> <li><strong>Example:</strong> In a house price prediction model, you might monitor if the importance of 'location' decreases while 'square footage' increases significantly over time.</li> </ul> <p style="font-size: 14px;">Additional details:</p> <ul> <li>Uses the Normalized Discounted Cumulative Gain (NDCG) score to compare feature attribution rankings.</li> <li>Supports various feature attribution methods like SHAP (SHapley Additive exPlanations).</li> <li>Can detect both changes in ranking order and absolute attribution values.</li> <li>Helps identify potential data drift or concept drift issues.</li> </ul> <p style="font-size: 14px;">Implementation approach:</p> <ol> <li>Compute baseline feature attributions using training data.</li> <li>Set up regular jobs to calculate feature attributions on recent data.</li> <li>Use NDCG to compare current attributions with the baseline.</li> <li>Configure alerts for significant changes (e.g., NDCG < 0.90).</li> <li>Analyze attribution drift reports in SageMaker Studio.</li> </ol> <p style="font-size: 14px;">By implementing these comprehensive monitoring strategies, data scientists and ML engineers can ensure their models remain accurate, fair, and reliable throughout their lifecycle in production environments.</p>
            
            <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">Data Quality Checks in Amazon SageMaker Model Monitor</strong></p> <p style="font-size: 14px;">SageMaker Model Monitor performs several checks to ensure the quality of your data remains consistent over time. Here's a simplified explanation of each check:</p> <ol> <li><strong>Data Type Check</strong> <p>Ensures that the data types of your features haven't changed.</p> <p><em>Example:</em> If 'age' was originally a number, an alert is raised if it suddenly becomes text.</p> </li> <li><strong>Completeness Check</strong> <p>Monitors the percentage of non-null values in each feature.</p> <p><em>Example:</em> If 'income' typically has 98% non-null values, an alert is raised if it drops to 85%.</p> </li> <li><strong>Baseline Drift Check</strong> <p>Compares the current data distribution to the baseline to detect significant changes.</p> <p><em>Example:</em> If the average 'transaction_amount' shifts from $50 to $500, it triggers an alert.</p> </li> <li><strong>Missing Column Check</strong> <p>Alerts you if any expected columns are missing from the current dataset.</p> <p><em>Example:</em> If 'customer_id' column disappears from your data, you'll be notified.</p> </li> <li><strong>Extra Column Check</strong> <p>Notifies you if new, unexpected columns appear in the data.</p> <p><em>Example:</em> If a new 'loyalty_score' column suddenly appears, you'll be alerted.</p> </li> <li><strong>Categorical Values Check</strong> <p>Monitors for an increase in unknown categories in categorical features.</p> <p><em>Example:</em> If 'product_category' starts showing many values not seen in the baseline data, it raises an alert.</p> </li> </ol> <p style="font-size: 14px;">These checks help you maintain data quality by alerting you to unexpected changes in your data. You can adjust thresholds for each check to fine-tune sensitivity based on your specific needs.</p> <p style="font-size: 14px;">The results of these checks are stored in a 'constraint_violations.json' file, which lists any violations found during the monitoring execution. This file helps you quickly identify and address data quality issues, ensuring your model continues to receive appropriate input data.</p>

            <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">Model Quality Metrics by Solution Type</strong></p> <p style="font-size: 14px;"><strong>1. Regression Metrics</strong></p> <ul> <li><strong>MAE (Mean Absolute Error):</strong> Average of absolute differences between predictions and actual values.</li> <li><strong>MSE (Mean Squared Error):</strong> Average of squared differences between predictions and actual values.</li> <li><strong>RMSE (Root Mean Squared Error):</strong> Square root of MSE, in the same unit as the target variable.</li> <li><strong>RÂ² (R-squared):</strong> Proportion of variance in the dependent variable predictable from the independent variable(s).</li> </ul> <p style="font-size: 14px;"><strong>2. Binary Classification Metrics</strong></p> <ul> <li><strong>Confusion Matrix:</strong> Table showing True Positives, False Positives, True Negatives, and False Negatives.</li> <li><strong>Accuracy:</strong> Proportion of correct predictions (both true positives and true negatives) among the total number of cases examined.</li> <li><strong>Precision:</strong> Proportion of true positive predictions among all positive predictions.</li> <li><strong>Recall (True Positive Rate):</strong> Proportion of actual positives correctly identified.</li> <li><strong>F1 Score:</strong> Harmonic mean of precision and recall.</li> <li><strong>AUC (Area Under the ROC Curve):</strong> Measure of the ability of a classifier to distinguish between classes.</li> <li><strong>ROC (Receiver Operating Characteristic) Curve:</strong> Graphical plot illustrating the diagnostic ability of a binary classifier system.</li> </ul> <p style="font-size: 14px;"><strong>3. Multiclass Classification Metrics</strong></p> <ul> <li><strong>Confusion Matrix:</strong> Table showing predicted vs. actual class assignments.</li> <li><strong>Accuracy:</strong> Proportion of correct predictions among the total number of cases examined.</li> <li><strong>Weighted Precision:</strong> Average of precision for each class, weighted by the number of true instances for each class.</li> <li><strong>Weighted Recall:</strong> Average of recall for each class, weighted by the number of true instances for each class.</li> <li><strong>Weighted F1 Score:</strong> Harmonic mean of precision and recall, calculated for each class and weighted by the number of true instances for each class.</li> </ul> <p style="font-size: 14px;">For all metric types, SageMaker Model Monitor also provides:</p> <ul> <li><strong>Standard Deviation:</strong> Measure of variability in the metric (when at least 200 samples are available).</li> <li><strong>Best Constant Classifier Metrics:</strong> Metrics for a hypothetical classifier that always predicts the most common class, serving as a baseline for comparison.</li> </ul> <p style="font-size: 14px;">These metrics are automatically sent to Amazon CloudWatch if enabled, allowing you to set up alarms and monitor your model's performance over time. You can create CloudWatch alarms to alert you when any of these metrics deviate from expected values, helping you detect model drift or performance degradation quickly.</p>

            <hr/>
            <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">Model Quality Monitoring in Amazon SageMaker</strong></p> <p style="font-size: 14px;">Amazon SageMaker Model Monitor provides comprehensive tools for evaluating and monitoring the quality of machine learning models in production. Here's an overview of the key components and processes:</p> <p style="font-size: 15px; color: #0066cc;">Creating a Model Quality Baseline</p> <ol> <li>Use the <code>ModelQualityMonitor</code> class from the SageMaker Python SDK.</li> <li>Create an instance of <code>ModelQualityMonitor</code> with appropriate parameters (role, instance count, type, etc.).</li> <li>Call the <code>suggest_baseline</code> method to run a baseline job, specifying: <ul> <li>Baseline dataset URI</li> <li>Dataset format</li> <li>Output S3 URI</li> <li>Problem type (e.g., 'BinaryClassification')</li> <li>Inference, probability, and ground truth attribute columns</li> </ul> </li> <li>Review and modify the generated constraints as needed before using them for monitoring.</li> </ol> <p style="font-size: 15px; color: #0066cc;">Model Quality Metrics</p> <p style="font-size: 14px;">SageMaker calculates different metrics based on the type of machine learning problem:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Problem Type</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Key Metrics</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Regression</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Mean Absolute Error (MAE)</li> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R-squared (R2)</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Binary Classification</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Confusion Matrix</li> <li>Accuracy, Precision, Recall</li> <li>F1, F0.5, F2 scores</li> <li>AUC (Area Under the ROC Curve)</li> <li>True/False Positive/Negative Rates</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Multiclass Classification</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Confusion Matrix</li> <li>Accuracy</li> <li>Weighted Precision, Recall, F-scores</li> </ul> </td> </tr> </table> <p style="font-size: 14px;">For each metric, SageMaker provides both the value and its standard deviation (when at least 200 samples are available).</p> <p style="font-size: 15px; color: #0066cc;">Best Constant Classifier Metrics</p> <p style="font-size: 14px;">SageMaker also calculates metrics for a hypothetical "best constant classifier" - a model that always predicts the most common class. These serve as a baseline for comparison:</p> <ul> <li>Accuracy, Precision, Recall for the constant classifier</li> <li>F-scores (F0.5, F1, F2) for the constant classifier</li> </ul> <p style="font-size: 15px; color: #0066cc;">Monitoring with Amazon CloudWatch</p> <ol> <li>Enable CloudWatch metrics when creating the monitoring schedule (<code>enable_cloudwatch_metrics=True</code>).</li> <li>Metrics are sent to CloudWatch under specific namespaces: <ul> <li>For real-time endpoints: <code>aws/sagemaker/Endpoints/model-metrics</code></li> <li>For batch transform jobs: <code>aws/sagemaker/ModelMonitoring/model-metrics</code></li> </ul> </li> <li>Use CloudWatch to create alarms based on these metrics, allowing you to set thresholds and receive notifications for model quality issues.</li> </ol> <p style="font-size: 14px;">By leveraging these tools and metrics, you can continuously monitor your model's performance, detect drift or degradation over time, and take proactive measures to maintain high-quality predictions in your machine learning applications.</p>

            <hr />
            <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">Bias Drift Monitoring in Amazon SageMaker</strong></p> <p style="font-size: 14px;">Amazon SageMaker provides tools to monitor and detect bias drift in machine learning models over time. This helps ensure that models remain fair and unbiased in production environments.</p> <p style="font-size: 15px; color: #0066cc;">Creating a Bias Drift Baseline</p> <ol> <li>Use the <code>ModelBiasMonitor</code> class from the SageMaker Python SDK.</li> <li>Configure the following components: <ul> <li><code>DataConfig</code>: Specifies the dataset, format, headers, and label.</li> <li><code>BiasConfig</code>: Defines sensitive groups (facets) and positive labels.</li> <li><code>ModelPredictedLabelConfig</code>: Specifies how to extract predicted labels from model output.</li> <li><code>ModelConfig</code>: Provides model details for inference.</li> </ul> </li> <li>Start the baselining job using <code>suggest_baseline()</code> method.</li> </ol> <p style="font-size: 15px; color: #0066cc;">Bias Drift Violations</p> <p style="font-size: 14px;">Bias drift jobs evaluate baseline constraints against current analysis results. Violations are recorded in the <code>constraint_violations.json</code> file with the following schema:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Field</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">facet</td> <td style="border: 1px solid #ddd; padding: 8px;">Name of the sensitive attribute</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">facet_value</td> <td style="border: 1px solid #ddd; padding: 8px;">Value of the facet</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">metric_name</td> <td style="border: 1px solid #ddd; padding: 8px;">Short name of the bias metric (e.g., "CI" for class imbalance)</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">constraint_check_type</td> <td style="border: 1px solid #ddd; padding: 8px;">Type of violation (currently only "bias_drift_check")</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">description</td> <td style="border: 1px solid #ddd; padding: 8px;">Explanation of the violation</td> </tr> </table> <p style="font-size: 14px;">A violation is logged when the bias metric value in the current analysis is worse (farther from zero) than the baseline constraint.</p> <p style="font-size: 15px; color: #0066cc;">CloudWatch Metrics for Bias Drift Analysis</p> <p style="font-size: 14px;">SageMaker publishes bias drift metrics to CloudWatch for real-time monitoring:</p> <ul> <li>Namespace: <ul> <li>For real-time endpoints: <code>aws/sagemaker/Endpoints/bias-metrics</code></li> <li>For batch transform jobs: <code>aws/sagemaker/ModelMonitoring/bias-metrics</code></li> </ul> </li> <li>Metric names: <code>bias_metric_[ShortName]</code> (e.g., <code>bias_metric_CI</code> for class imbalance)</li> </ul> <p style="font-size: 14px;">Each metric includes the following properties:</p> <ul> <li>Endpoint: Name of the monitored endpoint</li> <li>MonitoringSchedule: Name of the monitoring job schedule</li> <li>BiasStage: Stage of bias monitoring (Pre-training or Post-Training)</li> <li>Label: Name of the target feature</li> <li>LabelValue: Value of the target feature</li> <li>Facet: Name of the sensitive attribute</li> <li>FacetValue: Value of the sensitive attribute</li> </ul> <p style="font-size: 15px; color: #0066cc;">Best Practices for Bias Drift Monitoring</p> <ol> <li>Identify sensitive attributes and potential sources of bias in your data.</li> <li>Create a comprehensive baseline that includes various bias metrics.</li> <li>Set appropriate thresholds for bias drift alerts based on your use case and regulatory requirements.</li> <li>Regularly review and analyze bias drift reports to identify trends or patterns.</li> <li>Take corrective actions when significant bias drift is detected, such as retraining the model or adjusting the training data.</li> <li>Document all bias monitoring efforts and results for compliance and auditing purposes.</li> </ol> <p style="font-size: 14px;">By implementing robust bias drift monitoring, you can ensure that your machine learning models remain fair and unbiased throughout their lifecycle, helping to maintain trust in your AI systems and comply with ethical AI guidelines.</p>

            <hr />
            <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">Feature Attribution Monitoring in Amazon SageMaker</strong></p> <p style="font-size: 14px;">Amazon SageMaker provides tools to monitor and detect drift in feature attribution for machine learning models over time. This helps ensure that the importance of features in model predictions remains consistent in production environments.</p> <p style="font-size: 15px; color: #0066cc;">Creating a SHAP Baseline for Models in Production</p> <ol> <li>Use the <code>ModelExplainabilityMonitor</code> class from the SageMaker Python SDK.</li> <li>Configure the following components: <ul> <li><code>DataConfig</code>: Specifies the dataset, format, headers, and label.</li> <li><code>SHAPConfig</code>: Defines SHAP (SHapley Additive exPlanations) configuration.</li> <li><code>ModelConfig</code>: Provides model details for inference.</li> </ul> </li> <li>Start the baselining job using <code>suggest_baseline()</code> method.</li> </ol> <p style="font-size: 14px;">SageMaker Clarify supports different aggregation methods for global explanations:</p> <ul> <li><code>mean_abs</code>: Mean of absolute SHAP values for all instances.</li> <li><code>median</code>: Median of SHAP values for all instances.</li> <li><code>mean_sq</code>: Mean of squared SHAP values for all instances.</li> </ul> <p style="font-size: 15px; color: #0066cc;">Feature Attribution Drift Violations</p> <p style="font-size: 14px;">Feature attribution drift jobs evaluate baseline constraints against current analysis results. Violations are recorded in the <code>constraint_violations.json</code> file with the following schema:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Field</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">label</td> <td style="border: 1px solid #ddd; padding: 8px;">Name of the label or a placeholder</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">metric_name</td> <td style="border: 1px solid #ddd; padding: 8px;">Name of the explainability analysis method (currently only "shap")</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">constraint_check_type</td> <td style="border: 1px solid #ddd; padding: 8px;">Type of violation (currently only "feature_attribution_drift_check")</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">description</td> <td style="border: 1px solid #ddd; padding: 8px;">Explanation of the violation</td> </tr> </table> <p style="font-size: 14px;">A violation is logged when the nDCG score of global SHAP values is less than 0.9 when comparing baseline to current analysis.</p> <p style="font-size: 15px; color: #0066cc;">Configuring Parameters for Attribution Drift Monitoring</p> <p style="font-size: 14px;">Key configuration parameters include:</p> <ul> <li><code>headers</code>: List of feature names in the dataset.</li> <li><code>methods</code>: Specifies SHAP computation parameters (baseline, num_samples, agg_method, etc.).</li> <li><code>predictor</code>: Model parameters for creating a shadow endpoint.</li> <li><code>label_headers</code>: List of possible label values.</li> <li><code>content_type</code> and <code>accept_type</code>: Specifies input and output formats for the model.</li> </ul> <p style="font-size: 14px;">Separate configurations are needed for CSV and JSON Lines datasets.</p> <p style="font-size: 15px; color: #0066cc;">CloudWatch Metrics for Feature Drift Analysis</p> <p style="font-size: 14px;">SageMaker publishes feature attribution drift metrics to CloudWatch:</p> <ul> <li>Metrics: <ul> <li>Global SHAP value for each feature (<code>feature_[FeatureName]</code>)</li> <li>Expected value of the metric (<code>ExpectedValue</code>)</li> </ul> </li> <li>Namespace: <ul> <li>For real-time endpoints: <code>aws/sagemaker/Endpoints/explainability-metrics</code></li> <li>For batch transform jobs: <code>aws/sagemaker/ModelMonitoring/explainability-metrics</code></li> </ul> </li> </ul> <p style="font-size: 14px;">Each metric includes properties such as Endpoint, MonitoringSchedule, ExplainabilityMethod, Label, and ValueType.</p> <p style="font-size: 15px; color: #0066cc;">Best Practices for Feature Attribution Monitoring</p> <ol> <li>Choose an appropriate aggregation method for global SHAP values based on your use case.</li> <li>Set up a comprehensive baseline that includes all relevant features.</li> <li>Configure monitoring schedules to run at intervals that match your model update frequency.</li> <li>Use CloudWatch alarms to get notified of significant feature attribution drift.</li> <li>Regularly review attribution drift reports to identify changes in feature importance.</li> <li>When drift is detected, investigate the root cause and consider retraining or updating your model.</li> <li>Document all monitoring configurations and results for auditing and compliance purposes.</li> </ol> <p style="font-size: 14px;">By implementing robust feature attribution monitoring, you can ensure that your machine learning models maintain consistent and explainable behavior in production, enhancing trust and facilitating debugging of model decisions.</p>

            <hr />
            <p style="font-size: 16px; color: #333;"><strong style="color: #0066cc;">Amazon SageMaker Model Monitor FAQ Summary</strong></p> <p style="font-size: 14px;">Key points from the FAQ:</p> <ul> <li>Model Monitor and SageMaker Clarify help monitor model behavior across four dimensions: data quality, model quality, bias drift, and feature attribution drift.</li> <li>When enabled, Model Monitor automates the process of creating baselines, scheduling monitoring jobs, and alerting on drift detection.</li> <li>Data Capture is required to log inputs and outputs from model endpoints or batch transform jobs to Amazon S3.</li> <li>Ground Truth labels are needed for model quality monitoring and bias monitoring.</li> <li>Customers can customize monitoring schedules using pre-processing and post-processing scripts or by bringing their own containers.</li> <li>Pre-processing scripts can be used for data transformation, record exclusion, custom sampling, and custom logging.</li> <li>Post-processing scripts can be used to perform actions after a successful monitoring run.</li> <li>Bringing your own container is recommended for regulatory compliance, complex dependencies, air-gapped environments, or non-tabular data formats like NLP or CV.</li> <li>Model Monitor supports inference pipelines but not multi-model endpoints.</li> <li>Baselines are used as references for comparison and can be created using the <code>suggest_baseline</code> method or customized manually.</li> <li>On-demand monitoring jobs can be run using SageMaker Processing jobs or Pipelines.</li> <li>Model Monitor can be set up using the SageMaker Python SDK, Pipelines, SageMaker Studio Classic, or the SageMaker Model Dashboard.</li> <li>The Model Dashboard provides unified monitoring across all models with automated alerts and troubleshooting capabilities.</li> </ul> <p style="font-size: 14px;">This FAQ covers a wide range of topics related to Model Monitor, from basic setup to advanced customization options, helping users understand and implement effective model monitoring in SageMaker.</p>
		</div>
	</div>
	
	<br/>
	
</div>


<hr />


<div class="container mt-5">
	<h3 class="text-primary h4">Data Capture</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Amazon SageMaker Data Capture</p>
                <p style="line-height: 1.6;">Data Capture is a feature in Amazon SageMaker that allows you to log inputs to your endpoint and inference outputs from your deployed model to Amazon S3. It's designed to record information that can be used for training, debugging, and monitoring purposes.</p>
                
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Purpose of Data Capture:</p>
                <ul style="list-style-type: disc; padding-left: 20px;">
                    <li>Model Monitoring: Use with Amazon SageMaker Model Monitor to automatically parse and compare metrics with a baseline.</li>
                    <li>Debugging: Identify issues with model performance or input data.</li>
                    <li>Continuous Learning: Provide data for retraining or fine-tuning models.</li>
                    <li>Auditing: Allow for review of past predictions and inputs.</li>
                </ul>
                
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Types of Data Capture:</p>
                <ol style="padding-left: 20px;">
                    <li>Real-time Endpoint Data Capture: For models deployed as real-time endpoints.</li>
                    <li>Batch Transform Data Capture: For batch transform jobs.</li>
                </ol>
                
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">How to Enable Data Capture:</p>
                <table style="border-collapse: collapse; width: 100%;">
                    <tr style="background-color: #f2f2f2;">
                        <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Type</th>
                        <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Method</th>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Real-time Endpoints</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Use DataCaptureConfig when creating an endpoint configuration</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 8px;">Batch Transform Jobs</td>
                        <td style="border: 1px solid #ddd; padding: 8px;">Use DataCaptureConfig when creating a transform job</td>
                    </tr>
                </table>

                
<details>
    <summary>Realtime - Enable Data Capture</summary>
<pre><code>
capture_modes = [ "Input",  "Output" ] 
endpoint_config_response = sagemaker_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name, 
    # List of ProductionVariant objects, one for each model that you want to host at this endpoint.
    ProductionVariants=[
        {
            "VariantName": variant_name, 
            "ModelName": model_name, 
            "InstanceType": instance_type, # Specify the compute instance type.
            "InitialInstanceCount": initial_instance_count # Number of instances to launch initially.
        }
    ],
    DataCaptureConfig= {
        'EnableCapture': True, # Whether data should be captured or not.
        'InitialSamplingPercentage' : initial_sampling_percentage,
        'DestinationS3Uri': s3_capture_upload_path,
        'CaptureOptions': [{"CaptureMode" : capture_mode} for capture_mode in capture_modes] 
        # Example - Use list comprehension to capture both Input and Output
    }
)
</code></pre>
</details>

<details>
    <summary>Realtime - Example Data Capture</summary>    
<pre>
    <code>
{
    "captureData": {
        "endpointInput": {
        "observedContentType": "text/csv", # data MIME type
        "mode": "INPUT",
        "data": "50,0,188.9,94,203.9,104,151.8,124,11.6,8,3,",
        "encoding": "CSV"
        },
        "endpointOutput": {
        "observedContentType": "text/csv; charset=character-encoding",
        "mode": "OUTPUT",
        "data": "0.023190177977085114",
        "encoding": "CSV"
        }
    },
    "eventMetadata": {
        "eventId": "aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
        "inferenceTime": "2022-02-14T17:25:06Z"
    },
    "eventVersion": "0"
    }
    </code>
</pre>
</details>

<details>
<summary>Batch - Enable Data Capture</summary>
<pre>
<code>
sm_client.create_transform_job(
    TransformJobName="transform_job_name",
    MaxConcurrentTransforms=2,
    ModelName=model_name,
    TransformInput={
        "DataSource": {
            "S3DataSource": {
                "S3DataType": "S3Prefix",
                "S3Uri": input_data_s3_uri,
            }
        },
        "ContentType": "text/csv",
        "CompressionType": "None",
        "SplitType": "Line",
    },
    TransformOutput={
        "S3OutputPath": output_data_s3_uri,
        "Accept": "text/csv",
        "AssembleWith": "Line",
    },
    TransformResources={
        "InstanceType": "ml.m4.xlarge",
        "InstanceCount": 1,
    },
    DataCaptureConfig={
        "DestinationS3Uri": data_capture_destination,
        "KmsKeyId": "kms_key",
        "GenerateInferenceId": True,
    }
    )
</code>
</pre>
</details>
                
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Key Components:</p>
                <ul style="list-style-type: disc; padding-left: 20px;">
                    <li>Sampling Rate: Percentage of requests to capture.</li>
                    <li>S3 Storage Location: Where captured data is stored.</li>
                    <li>Capture Modes: Input, Output, or both.</li>
                    <li>Content Types: Specify how data should be encoded.</li>
                </ul>
                
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Gotchas and Best Practices:</p>
                <ul style="list-style-type: disc; padding-left: 20px;">
                    <li>Disk Usage: Keep disk utilization below 75% to ensure continuous capture.</li>
                    <li>Latency: Be aware of potential slight latency introduction.</li>
                    <li>S3 Costs: Consider additional storage costs for captured data.</li>
                    <li>Security: Use KMS encryption for sensitive data.</li>
                    <li>Sampling: Start with a lower sampling rate and adjust as needed.</li>
                </ul>
                
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Limitations:</p>
                <ul style="list-style-type: disc; padding-left: 20px;">
                    <li>Not available for all instance types.</li>
                    <li>May impact performance on very high-throughput endpoints.</li>
                    <li>Limited to certain content types and encodings.</li>
                </ul>
                
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Additional Features:</p>
                <ul style="list-style-type: disc; padding-left: 20px;">
                    <li>Generate inference IDs for batch jobs to match captured data with ground truth data.</li>
                    <li>Captured data is organized by timestamp for easy analysis of specific time periods.</li>
                </ul>
                
                <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Use Cases:</p>
                <ul style="list-style-type: disc; padding-left: 20px;">
                    <li>Model Quality Monitoring</li>
                    <li>Bias Detection</li>
                    <li>Feature Attribution</li>
                    <li>Explainability Analysis</li>
                    <li>Compliance and Auditing</li>
                </ul>
                
                

            
            <p style="font-size: 16px; font-weight: bold; color: #232F3E;">Other Important Information:</p>
            <ul style="list-style-type: disc; padding-left: 20px;">
                <li><span style="font-weight: bold;">Data Capture Formats:</span> Captured data is stored in SageMaker-specific JSON-line formatted files.</li>
            
                <li><span style="font-weight: bold;">Capture File Organization:</span> Files are organized with a yyyy/mm/dd/hh S3 prefix to indicate capture time.</li>
            
                <li><span style="font-weight: bold;">Viewing Captured Data:</span> You can use AWS SDK or SageMaker Python SDK to view and analyze captured data.</li>
            
                <li><span style="font-weight: bold;">Batch Transform Specifics:</span> For batch jobs, captured data is stored in separate /input and /output directories.</li>
            
                <li><span style="font-weight: bold;">Inference ID Generation:</span> For batch transform jobs, you can enable inference ID generation to help match captured data with ground truth data.</li>
            
                <li><span style="font-weight: bold;">Output Formats:</span> When inference ID generation is enabled, the output format differs for CSV and JSON/JSONL outputs.</li>
            
                <li><span style="font-weight: bold;">Reserved Fields:</span> SageMakerInferenceId and SageMakerInferenceTime are reserved fields in the output when inference ID generation is enabled.</li>
            
                <li><span style="font-weight: bold;">Capture Options:</span> You can capture either Input, Output, or both for real-time endpoints.</li>
            
                <li><span style="font-weight: bold;">Content Type Headers:</span> You can specify how SageMaker should encode captured data using CaptureContentTypeHeader.</li>
            
                <li><span style="font-weight: bold;">Delivery Delay:</span> There can be a couple of minutes delay in the delivery of captured data to Amazon S3.</li>
            </ul>
            
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Data Capture</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>

<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
