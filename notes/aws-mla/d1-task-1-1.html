<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning Engineer Associate (MLA)</h1>
  
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Domain 1: Data Preparation for Machine Learning (ML)</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
			<p style="color: blueviolet; font-size: 20px;"><stong>Task Statement 1.1: Ingest and store data. </stong></p>
			
			<p style="color: #0066cc;"><strong>Knowledge 1: Data formats and ingestion mechanisms (for example, validated and non-validated formats, Apache Parquet, JSON, CSV, Apache ORC, Apache Avro, RecordIO)</strong></p>
				<p>Understanding various data formats and ingestion mechanisms is crucial for effective data management and analysis in cloud environments. Let's explore some common formats and their characteristics:</p> <p style="color: blue;">Validated vs. Non-validated formats:</p> <p>Validated formats have a predefined structure and schema, ensuring data consistency. They often include built-in error checking and data type enforcement. Examples include Apache Parquet, Apache ORC, and XML.</p> <p>Non-validated formats are more flexible but may require additional processing to ensure data quality. They allow for easier data input but might need extra validation steps. Examples include JSON, CSV, and YAML.</p> <p style="color: blue;">List of Data Formats:</p> <ul> <li><strong>Apache Parquet:</strong> (Validated) <ul> <li>A columnar storage format optimized for analytics workloads.</li> <li>Offers efficient compression and encoding schemes.</li> <li>Ideal for big data processing frameworks like Apache Spark.</li> </ul> </li> <li><strong>JSON (JavaScript Object Notation):</strong> (Non-validated) <ul> <li>A lightweight, human-readable data interchange format.</li> <li>Widely used for API responses and configuration files.</li> <li>Flexible structure, but can be less efficient for large datasets.</li> </ul> </li> <li><strong>CSV (Comma-Separated Values):</strong> (Non-validated) <ul> <li>Simple, tabular format for storing data.</li> <li>Easy to read and write, but lacks metadata and type information.</li> <li>Commonly used for data exchange between different systems.</li> </ul> </li> <li><strong>Apache ORC (Optimized Row Columnar):</strong> (Validated) <ul> <li>Another columnar storage format, similar to Parquet.</li> <li>Designed for Hadoop ecosystems, particularly with Hive.</li> <li>Offers good compression and fast data retrieval.</li> </ul> </li> <li><strong>Apache Avro:</strong> (Validated) <ul> <li>Row-based data serialization format.</li> <li>Supports schema evolution, making it suitable for changing data structures.</li> <li>Compact binary format, good for high-throughput data processing.</li> </ul> </li> <li><strong>RecordIO:</strong> (Validated) <ul> <li>A binary format used in machine learning, particularly with Amazon SageMaker.</li> <li>Optimized for streaming data and efficient I/O operations.</li> <li>Supports both labeled and unlabeled data for training models.</li> </ul> </li> <li><strong>XML (eXtensible Markup Language):</strong> (Validated) <ul> <li>A markup language designed to store and transport data.</li> <li>Self-descriptive and human-readable.</li> <li>Widely used for configuration files and data exchange between systems.</li> </ul> </li> <li><strong>YAML (YAML Ain't Markup Language):</strong> (Non-validated) <ul> <li>A human-readable data serialization format.</li> <li>Often used for configuration files and data exchange.</li> <li>Supports complex data structures and is more readable than JSON.</li> </ul> </li> </ul> <p>When choosing a data format, consider factors such as data size, query patterns, processing requirements, and compatibility with your analytics tools. Each format has its strengths and is suited for different use cases in the data pipeline. Validated formats provide more structure and consistency, while non-validated formats offer greater flexibility but may require additional validation steps.</p>

			<p style="color: #0066cc;"><strong>Knowledge 2: How to use the core AWS data sources (for example, Amazon S3, Amazon Elastic File System [Amazon EFS], Amazon FSx for NetApp ONTAP)</strong></p> 
				<p>AWS provides various data storage options, each with unique characteristics. Let's explore the core AWS data sources and other prominent options used for machine learning:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Object storage service designed for scalability, data availability, security, and performance.</li> <li>Ideal for storing large amounts of unstructured data, such as logs, backups, and data lakes.</li> <li>Supports versioning, lifecycle policies, and fine-grained access controls.</li> <li>Example use: Storing raw data for big data analytics or serving static website content.</li> </ul> </li> <li><strong>Amazon Elastic File System (Amazon EFS):</strong> <ul> <li>Fully managed NFS file system for use with AWS Cloud services and on-premises resources.</li> <li>Automatically grows and shrinks as you add and remove files.</li> <li>Supports concurrent access from multiple EC2 instances.</li> <li>Example use: Shared file storage for content management systems or development environments.</li> </ul> </li> <li><strong>Amazon FSx:</strong> <ul> <li>Amazon FSx for NetApp ONTAP: <ul> <li>Fully managed file storage built on NetApp's ONTAP file system.</li> <li>Provides NFS, SMB, and iSCSI protocols for broad application compatibility.</li> <li>Offers advanced data management features like snapshots, replication, and data tiering.</li> </ul> </li> <li>Amazon FSx for Windows File Server: <ul> <li>Fully managed Windows file servers for Windows-based applications.</li> <li>Supports SMB protocol and integrates with Microsoft Active Directory.</li> </ul> </li> <li>Amazon FSx for Lustre: <ul> <li>High-performance file system optimized for compute-intensive workloads.</li> <li>Ideal for machine learning, high-performance computing (HPC), and financial modeling.</li> </ul> </li> <li>Amazon FSx for OpenZFS: <ul> <li>Fully managed file system built on the OpenZFS file system.</li> <li>Provides high performance, cost-effective storage for Linux workloads.</li> <li>Supports NFS protocol and offers features like snapshots and clones.</li> </ul> </li> </ul> </li> <li><strong>Amazon EBS (Elastic Block Store):</strong> <ul> <li>Provides block-level storage volumes for use with EC2 instances.</li> <li>Offers different volume types optimized for various workloads (e.g., SSD, HDD).</li> <li>Supports snapshots for backup and disaster recovery.</li> </ul> </li> <li><strong>AWS Storage Gateway:</strong> <ul> <li>File Gateway: <ul> <li>Provides on-premises access to virtually unlimited cloud storage.</li> <li>Supports NFS and SMB protocols for seamless integration with existing applications.</li> <li>Caches frequently accessed data locally for low-latency access.</li> </ul> </li> <li>Volume Gateway: <ul> <li>Provides cloud-backed storage volumes that can be mounted as iSCSI devices.</li> <li>Offers both cached and stored volume configurations.</li> <li>Useful for backup and disaster recovery scenarios.</li> </ul> </li> <li>Tape Gateway: <ul> <li>Provides a virtual tape library interface for backup applications.</li> <li>Stores data in Amazon S3 and Glacier for long-term archival.</li> </ul> </li> </ul> </li> <li><strong>Amazon RDS (Relational Database Service):</strong> <ul> <li>Managed relational database service supporting various database engines.</li> <li>Automates time-consuming administration tasks like hardware provisioning and backups.</li> <li>Useful for structured data storage and complex queries in ML pipelines.</li> </ul> </li> <li><strong>Other prominent data sources for machine learning in AWS:</strong> <ul> <li>Amazon DynamoDB: Fully managed NoSQL database service for high-performance applications.</li> <li>Amazon Redshift: Fully managed data warehouse for large-scale data analytics.</li> <li>Amazon Aurora: MySQL and PostgreSQL-compatible relational database built for the cloud.</li> <li>Amazon Kinesis: Real-time data streaming service for ingesting and processing streaming data.</li> <li>AWS Glue Data Catalog: Fully managed metadata repository for discovering and managing data.</li> <li>Amazon SageMaker Feature Store: Fully managed repository for machine learning features.</li> <li>Amazon Timestream: Fully managed time series database service for IoT and operational applications.</li> </ul> </li> </ul> <p>When using these data sources for machine learning, consider the following best practices:</p> <ul> <li>Use S3 for large-scale, cost-effective storage of unstructured data and as a central data lake.</li> <li>Leverage EFS for shared file systems that require POSIX compliance and concurrent access.</li> <li>Choose FSx for NetApp ONTAP when you need enterprise-grade features and compatibility with existing NetApp workflows.</li> <li>Use FSx for Lustre for high-performance computing and machine learning workloads requiring fast data access.</li> <li>Consider FSx for OpenZFS for Linux workloads requiring high performance and advanced file system features.</li> <li>Utilize EBS for block storage when running ML workloads on EC2 instances.</li> <li>Use AWS Storage Gateway to integrate on-premises environments with cloud storage for hybrid ML workflows.</li> <li>Implement proper access controls and encryption for all data sources to ensure security.</li> <li>Use appropriate data transfer methods (e.g., AWS DataSync, S3 Transfer Acceleration) for efficient data movement between sources.</li> <li>Consider using AWS Glue for ETL processes to prepare data for machine learning tasks.</li> <li>Leverage Amazon SageMaker Feature Store for managing and serving ML features at scale.</li> <li>Use Amazon RDS or Aurora for structured data storage and complex queries in ML pipelines.</li> <li>Implement data versioning and lineage tracking to ensure reproducibility of ML experiments.</li> </ul>
			<p style="color: #0066cc;"><strong>Knowledge 3: How to use AWS streaming data sources to ingest data (for example, Amazon Kinesis, Apache Flink, Apache Kafka)</strong></p> <p>Streaming data ingestion is crucial for real-time analytics and processing. AWS offers several options for handling streaming data:</p> <ul> <li><strong>Amazon Kinesis:</strong> <ul> <li>A suite of services for real-time streaming data processing.</li> <li>Kinesis Data Streams: For ingesting and storing streaming data.</li> <li>Kinesis Data Firehose: For loading streaming data into data stores and analytics tools.</li> <li>Kinesis Data Analytics: For processing streaming data in real-time using SQL or Apache Flink.</li> <li>Example use: Processing clickstream data for real-time website analytics.</li> </ul> </li> <li><strong>Apache Flink on AWS:</strong> <ul> <li>Open-source stream processing framework that can be deployed on AWS.</li> <li>Supports both batch and stream processing with low latency.</li> <li>Can be used with Amazon Kinesis Data Analytics for Java applications.</li> <li>Example use: Complex event processing for IoT sensor data.</li> </ul> </li> <li><strong>Apache Kafka on AWS:</strong> <ul> <li>Can be deployed on EC2 instances or used as a managed service with Amazon MSK (Managed Streaming for Apache Kafka).</li> <li>Provides high-throughput, fault-tolerant, publish-subscribe messaging system.</li> <li>Ideal for building real-time data pipelines and streaming applications.</li> <li>Example use: Building a real-time log aggregation system for distributed applications.</li> </ul> </li> </ul> <p>Best practices for using streaming data sources:</p> <ul> <li>Choose the right service based on your data volume, processing requirements, and latency needs.</li> <li>Implement proper error handling and retry mechanisms to deal with data inconsistencies.</li> <li>Use appropriate scaling strategies to handle varying data ingestion rates.</li> <li>Implement data transformation and enrichment as close to the ingestion point as possible.</li> <li>Monitor your streaming pipelines for performance and data quality issues.</li> </ul> 
			<p style="color: #0066cc;"><strong>Knowledge 4: AWS storage options, including use cases and tradeoffs</strong></p> <p>AWS offers a variety of storage options to cater to different use cases. Understanding their characteristics and tradeoffs is essential for optimal data management:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Use case: Object storage for large-scale, unstructured data.</li> <li>Pros: Highly scalable, durable, and cost-effective.</li> <li>Cons: Not suitable for file system operations or transactional databases.</li> </ul> </li> <li><strong>Amazon EBS (Elastic Block Store):</strong> <ul> <li>Use case: Block-level storage volumes for EC2 instances.</li> <li>Pros: Low-latency, persistent storage for databases and file systems.</li> <li>Cons: Limited to a single Availability Zone, more expensive than S3 for large datasets.</li> </ul> </li> <li><strong>Amazon EFS (Elastic File System):</strong> <ul> <li>Use case: Shared file storage for Linux-based workloads.</li> <li>Pros: Scalable, supports concurrent access from multiple EC2 instances.</li> <li>Cons: Higher latency compared to EBS, not suitable for Windows workloads.</li> </ul> </li> <li><strong>Amazon FSx:</strong> <ul> <li>Use case: Fully managed file systems for Windows (FSx for Windows File Server) and high-performance computing (FSx for Lustre).</li> <li>Pros: Native compatibility with specific workloads, high performance.</li> <li>Cons: More expensive than general-purpose storage options.</li> </ul> </li> <li><strong>Amazon DynamoDB:</strong> <ul> <li>Use case: NoSQL database for applications requiring low-latency data access.</li> <li>Pros: Fully managed, scalable, and highly available.</li> <li>Cons: Limited query flexibility compared to relational databases.</li> </ul> </li> <li><strong>Amazon RDS (Relational Database Service):</strong> <ul> <li>Use case: Managed relational databases for structured data.</li> <li>Pros: Easy to set up, manage, and scale relational databases.</li> <li>Cons: Less control over the underlying infrastructure compared to self-managed databases.</li> </ul> </li> </ul> <p>When choosing a storage option, consider factors such as:</p> <ul> <li>Data structure and access patterns</li> <li>Performance requirements (IOPS, throughput)</li> <li>Scalability needs</li> <li>Cost considerations</li> <li>Data durability and availability requirements</li> <li>Integration with other AWS services and existing applications</li> </ul> <p>By carefully evaluating these factors and understanding the tradeoffs between different storage options, you can design an efficient and cost-effective storage architecture for your AWS-based applications and data processing pipelines.</p>

			<hr style="height: 20px; background-color: blue;"/>

			<p style="color: #0066cc;"><strong>Skill 1: Extracting data from storage (for example, Amazon S3, Amazon Elastic Block Store [Amazon EBS], Amazon EFS, Amazon RDS, Amazon DynamoDB) by using relevant AWS service options (for example, Amazon S3 Transfer Acceleration, Amazon EBS Provisioned IOPS)</strong></p> <p>This skill involves the ability to efficiently retrieve data from various AWS storage services using appropriate methods and optimizations. Here's a breakdown of the key components:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Use AWS SDK or CLI to download objects</li> <li>Implement S3 Transfer Acceleration for faster transfers over long distances</li> <li>Utilize S3 Select for retrieving specific data subsets</li> </ul> </li> <li><strong>Amazon EBS (Elastic Block Store):</strong> <ul> <li>Attach EBS volumes to EC2 instances for direct access</li> <li>Use EBS Provisioned IOPS for high-performance, low-latency workloads</li> <li>Create EBS snapshots for backup and data transfer</li> </ul> </li> <li><strong>Amazon EFS (Elastic File System):</strong> <ul> <li>Mount EFS file systems to multiple EC2 instances</li> <li>Use EFS-to-EFS backup for data protection and migration</li> </ul> </li> <li><strong>Amazon RDS (Relational Database Service):</strong> <ul> <li>Use SQL queries to extract data from RDS databases</li> <li>Implement read replicas for improved performance on read-heavy workloads</li> </ul> </li> <li><strong>Amazon DynamoDB:</strong> <ul> <li>Use Query and Scan operations to retrieve data</li> <li>Implement DynamoDB Accelerator (DAX) for faster read performance</li> </ul> </li> </ul> <p>Example: To extract data from an S3 bucket using Python and the AWS SDK (boto3):</p> <p><code> import boto3<br> s3 = boto3.client('s3')<br> response = s3.get_object(Bucket='my-bucket', Key='my-object')<br> data = response['Body'].read() </code></p> 
			<p style="color: #0066cc;"><strong>Skill 2: Choosing appropriate data formats (for example, Parquet, JSON, CSV, ORC) based on data access patterns</strong>
				</p> <p>This skill requires understanding different data formats and their characteristics to select the most suitable one for specific use cases. Key considerations include:</p> <ul> <li><strong>CSV (Comma-Separated Values):</strong> <ul> <li>Simple, tabular format</li> <li>Easy to read and write</li> <li>Good for flat data structures and compatibility with spreadsheet applications</li> <li>Suitable for small to medium-sized datasets with simple structures</li> </ul> </li> <li><strong>JSON (JavaScript Object Notation):</strong> <ul> <li>Human-readable and easy to parse</li> <li>Flexible schema, good for nested and hierarchical data</li> <li>Suitable for web applications and APIs</li> <li>Efficient for semi-structured data</li> </ul> </li> <li><strong>Parquet:</strong> <ul> <li>Columnar storage format</li> <li>Efficient for analytical queries and large datasets</li> <li>Good for data with many columns but only a subset is typically accessed</li> <li>Supports efficient compression and encoding schemes</li> </ul> </li> <li><strong>Avro:</strong> <ul> <li>Row-based data serialization format</li> <li>Supports schema evolution</li> <li>Efficient for data-intensive applications</li> <li>Good for systems with frequent schema changes</li> </ul> </li> <li><strong>ORC (Optimized Row Columnar):</strong> <ul> <li>Columnar storage format optimized for Hive</li> <li>Efficient compression and encoding schemes</li> <li>Good for large-scale data processing in Hadoop ecosystems</li> </ul> </li> <li><strong>LIBSVM:</strong> <ul> <li>Text-based format for labeled sparse data</li> <li>Commonly used in machine learning, especially for support vector machines</li> <li>Efficient for datasets with many features but sparse non-zero values</li> </ul> </li> <li><strong>XML (eXtensible Markup Language):</strong> <ul> <li>Versatile format for structured data</li> <li>Self-descriptive and human-readable</li> <li>Good for complex, hierarchical data structures</li> <li>Can be verbose, leading to larger file sizes</li> </ul> </li> <li><strong>YAML:</strong> <ul> <li>Human-readable data serialization format</li> <li>Often used for configuration files</li> <li>Supports complex data structures</li> <li>More readable than JSON for humans</li> </ul> </li> <li><strong>Pickle:</strong> <ul> <li>Python-specific serialization format</li> <li>Efficient for serializing Python objects</li> <li>Not human-readable or cross-language compatible</li> <li>Security concerns when deserializing untrusted data</li> </ul> </li> <li><strong>Protocol Buffers:</strong> <ul> <li>Language-neutral, platform-neutral serialization format</li> <li>Efficient in terms of size and parsing speed</li> <li>Requires schema definition</li> <li>Good for systems with multiple programming languages</li> </ul> </li> <li><strong>RecordIO:</strong> <ul> <li>Binary format used in machine learning, particularly with Amazon SageMaker</li> <li>Optimized for streaming data and efficient I/O operations</li> <li>Supports both labeled and unlabeled data for training models</li> </ul> </li> </ul> <p>Example: If you're working with time-series data that requires frequent aggregations and filtering on specific columns, Parquet might be the best choice due to its columnar nature and efficient compression. For a machine learning pipeline using SageMaker, RecordIO could be more appropriate due to its optimization for that platform.</p> <p>When choosing a data format, consider factors such as:</p> <ul> <li>Data structure (flat, nested, sparse)</li> <li>Query patterns (row-wise access vs. column-wise analytics)</li> <li>File size and compression requirements</li> <li>Read/write frequency</li> <li>Compatibility with processing tools and platforms</li> <li>Schema flexibility needs</li> <li>Human readability requirements</li> </ul> <p>The right choice of data format can significantly impact performance, storage costs, and ease of data processing in your machine learning and data science workflows.</p>
			<table style="border-collapse: collapse;padding-bottom: 12px;"> <tr> <th style="color: purple; background-color: white; padding: 5px; border: 1px solid lightgray;">Data Format</th> <th style="color: purple; background-color: white; padding: 5px; border: 1px solid lightgray;">Data Structure</th> <th style="color: purple; background-color: white; padding: 5px; border: 1px solid lightgray;">Query Patterns</th> <th style="color: purple; background-color: white; padding: 5px; border: 1px solid lightgray;">Compression</th> <th style="color: purple; background-color: white; padding: 5px; border: 1px solid lightgray;">Human Readability</th> <th style="color: purple; background-color: white; padding: 5px; border: 1px solid lightgray;">Schema Flexibility</th> <th style="color: purple; background-color: white; padding: 5px; border: 1px solid lightgray;">Cross-Language Support</th> <th style="color: purple; background-color: white; padding: 5px; border: 1px solid lightgray;">ML/AI Optimized</th> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">CSV</td> <td style="padding: 5px; border: 1px solid lightgray;">Flat</td> <td style="padding: 5px; border: 1px solid lightgray;">Row-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">JSON</td> <td style="padding: 5px; border: 1px solid lightgray;">Nested</td> <td style="padding: 5px; border: 1px solid lightgray;">Row-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">Parquet</td> <td style="padding: 5px; border: 1px solid lightgray;">Flat/Nested</td> <td style="padding: 5px; border: 1px solid lightgray;">Column-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">Avro</td> <td style="padding: 5px; border: 1px solid lightgray;">Flat/Nested</td> <td style="padding: 5px; border: 1px solid lightgray;">Row-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">ORC</td> <td style="padding: 5px; border: 1px solid lightgray;">Flat/Nested</td> <td style="padding: 5px; border: 1px solid lightgray;">Column-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">LIBSVM</td> <td style="padding: 5px; border: 1px solid lightgray;">Sparse</td> <td style="padding: 5px; border: 1px solid lightgray;">Row-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">XML</td> <td style="padding: 5px; border: 1px solid lightgray;">Nested</td> <td style="padding: 5px; border: 1px solid lightgray;">Row-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">YAML</td> <td style="padding: 5px; border: 1px solid lightgray;">Nested</td> <td style="padding: 5px; border: 1px solid lightgray;">Row-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">Very High</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">Pickle</td> <td style="padding: 5px; border: 1px solid lightgray;">Any</td> <td style="padding: 5px; border: 1px solid lightgray;">N/A</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">Very Low</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low (Python-specific)</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">Protocol Buffers</td> <td style="padding: 5px; border: 1px solid lightgray;">Flat/Nested</td> <td style="padding: 5px; border: 1px solid lightgray;">Row-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> </tr> <tr> <td style="padding: 5px; border: 1px solid lightgray;">RecordIO</td> <td style="padding: 5px; border: 1px solid lightgray;">Flat/Nested</td> <td style="padding: 5px; border: 1px solid lightgray;">Row-wise</td> <td style="padding: 5px; border: 1px solid lightgray;">High</td> <td style="padding: 5px; border: 1px solid lightgray;">Very Low</td> <td style="padding: 5px; border: 1px solid lightgray;">Medium</td> <td style="padding: 5px; border: 1px solid lightgray;">Low</td> <td style="padding: 5px; border: 1px solid lightgray;">Very High</td> </tr> </table>
				<p><strong>Legend:</strong></p> <ul> <li><strong>Data Structure:</strong> The typical structure of data the format is best suited for.</li> <li><strong>Query Patterns:</strong> Whether the format is optimized for row-wise or column-wise access.</li> <li><strong>Compression:</strong> The level of data compression typically achieved.</li> <li><strong>Human Readability:</strong> How easily humans can read the raw data.</li> <li><strong>Schema Flexibility:</strong> The ease of changing or evolving the data schema.</li> <li><strong>Cross-Language Support:</strong> How well the format is supported across different programming languages.</li> <li><strong>ML/AI Optimized:</strong> How well-suited the format is for machine learning and AI workflows.</li> </ul> <p><em>Note: These ratings are general guidelines and may vary depending on specific implementations or use cases. The choice of data format should always be based on the specific requirements of your project and the systems you're working with.</em></p>
			<p style="color: #0066cc;"><strong>Skill 3: Ingesting data into Amazon SageMaker Data Wrangler and SageMaker Feature Store</strong></p> <p>This skill involves understanding how to import and prepare data for machine learning workflows using Amazon SageMaker tools:</p> <ul> <li><strong>Amazon SageMaker Data Wrangler:</strong> <ul> <li>Import data from various sources (S3, Athena, Redshift)</li> <li>Perform data transformations and feature engineering</li> <li>Visualize and analyze data distributions</li> <li>Export prepared data to S3 or use directly in SageMaker</li> </ul> </li> <li><strong>SageMaker Feature Store:</strong> <ul> <li>Create feature groups to organize related features</li> <li>Ingest features using the Feature Store API or SageMaker Processing jobs</li> <li>Configure online and offline storage for feature access</li> <li>Set up feature versioning and time travel capabilities</li> </ul> </li> </ul> 
				<p style="font-size:16px;color:#1f3864;margin-bottom:8px;">Data Ingestion in Amazon SageMaker Data Wrangler</p> <p>Amazon SageMaker Data Wrangler simplifies the process of data ingestion and preparation for machine learning. It provides an intuitive interface to import, explore, transform, and analyze data from various sources. Key benefits include:</p> <ul> <li style="margin-bottom:4px;">Connect to diverse data sources like Amazon S3, Athena, Redshift, Snowflake, and Databricks</li> <li style="margin-bottom:4px;">Create data flows to define and automate data preparation steps</li> <li style="margin-bottom:4px;">Generate insightful data quality and insights reports to identify potential issues and guide data cleaning</li> <li>Leverage built-in and custom data transforms for cleaning, featurization, and preprocessing</li> </ul> <p style="font-size:14px;color:#1f3864;margin-top:16px;margin-bottom:8px;">Data Import and Integration</p> <p>Data Wrangler allows you to import data from various sources and join datasets directly in your data flow. This enables you to:</p> <ul> <li style="margin-bottom:4px;">Combine data from different sources to create richer feature sets</li> <li style="margin-bottom:4px;">Perform joins (inner, outer, left, right, etc.) to integrate related data</li> <li>Handle data at scale and prepare it for machine learning workflows</li> </ul> <p style="font-size:14px;color:#1f3864;margin-top:16px;margin-bottom:8px;">Data Insights and Transformations</p> <p>Data Wrangler provides powerful tools to gain insights into your data and apply transformations:</p> <ul> <li style="margin-bottom:4px;">Generate data quality and insights reports to understand data characteristics, identify issues, and guide data preparation</li> <li style="margin-bottom:4px;">Use built-in transforms for common tasks like handling missing values, encoding categorical variables, scaling numeric features, and more</li> <li style="margin-bottom:4px;">Create custom transformations using Python, SQL, or user-defined functions for flexibility and control</li> <li>Apply dimensionality reduction techniques like PCA to reduce feature space while retaining important information</li> </ul> <p style="font-size:14px;color:#1f3864;margin-top:16px;margin-bottom:8px;">Streamlined ML Workflows</p> <p>Data Wrangler integrates seamlessly with other Amazon SageMaker features to streamline your machine learning workflows:</p> <ul> <li style="margin-bottom:4px;">Export prepared data to Amazon S3 for easy access and sharing</li> <li style="margin-bottom:4px;">Integrate with SageMaker Pipelines to automate data preparation and model deployment</li> <li style="margin-bottom:4px;">Store curated features in SageMaker Feature Store for reuse across projects</li> <li>Generate Python scripts from your data flow for integration into custom workflows</li> </ul> <table style="border-collapse:collapse;margin-top:16px;"> <tr> <td style="border:1px solid #ccc;padding:8px;font-size:14px;color:#1f3864;font-weight:bold;">Key Concept</td> <td style="border:1px solid #ccc;padding:8px;font-size:14px;color:#1f3864;font-weight:bold;">Value Proposition</td> </tr> <tr> <td style="border:1px solid #ccc;padding:8px;">Data Import</td> <td style="border:1px solid #ccc;padding:8px;">Easily connect to and import data from various sources</td> </tr> <tr> <td style="border:1px solid #ccc;padding:8px;">Data Flow</td> <td style="border:1px solid #ccc;padding:8px;">Define and automate data preparation steps</td> </tr> <tr> <td style="border:1px solid #ccc;padding:8px;">Data Insights</td> <td style="border:1px solid #ccc;padding:8px;">Gain valuable insights into data quality and characteristics</td> </tr> <tr> <td style="border:1px solid #ccc;padding:8px;">Data Transforms</td> <td style="border:1px solid #ccc;padding:8px;">Apply built-in and custom transformations for data preparation</td> </tr> <tr> <td style="border:1px solid #ccc;padding:8px;">ML Integration</td> <td style="border:1px solid #ccc;padding:8px;">Seamlessly integrate with SageMaker features for end-to-end ML workflows</td> </tr> </table>
				<br /><p>Example: To ingest data into SageMaker Feature Store using Python:</p> <p><code> from sagemaker.feature_store.feature_group import FeatureGroup<br> from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum<br> <br> feature_group = FeatureGroup(name="my-feature-group", sagemaker_session=session)<br> feature_group.load_feature_definitions([<br> &nbsp;&nbsp;FeatureDefinition(feature_name="feature1", feature_type=FeatureTypeEnum.STRING),<br> &nbsp;&nbsp;FeatureDefinition(feature_name="feature2", feature_type=FeatureTypeEnum.INTEGRAL)<br> ])<br> feature_group.create(s3_uri="s3://my-bucket/my-prefix", record_identifier_name="id")<br> feature_group.ingest(data_frame=my_dataframe) </code></p> 
			<p style="color: #0066cc;"><strong>Skill 4: Merging data from multiple sources (for example, by using programming techniques, AWS Glue, Apache Spark)</strong></p> <p>This skill focuses on combining data from various sources to create a unified dataset. Key aspects include:</p> <ul> <li><strong>Programming techniques:</strong> <ul> <li>Use pandas or other data manipulation libraries for small to medium datasets</li> <li>Implement efficient join algorithms (e.g., hash join, merge join)</li> <li>Handle data type conversions and schema alignment</li> </ul> </li> <li><strong>AWS Glue:</strong> <ul> <li>Create Glue jobs to extract, transform, and load (ETL) data</li> <li>Use Glue Data Catalog to discover and manage metadata</li> <li>Implement Glue DynamicFrames for schema evolution</li> </ul> </li> <li><strong>Apache Spark:</strong> <ul> <li>Utilize Spark SQL for distributed data processing</li> <li>Implement efficient join strategies (e.g., broadcast join, shuffle hash join)</li> <li>Optimize performance using partitioning and caching</li> </ul> </li> </ul> <p>Example: Merging data using pandas in Python:</p> <p><code> import pandas as pd<br> <br> df1 = pd.read_csv('source1.csv')<br> df2 = pd.read_csv('source2.csv')<br> merged_df = pd.merge(df1, df2, on='common_column', how='inner') </code></p> 
			<p style="color: #0066cc;"><strong>Skill 5: Troubleshooting and debugging data ingestion and storage issues that involve capacity and scalability</strong></p> <p>This skill involves identifying and resolving problems related to data ingestion and storage, particularly those affecting system capacity and scalability. Key aspects include:</p> <ul> <li><strong>Capacity issues:</strong> <ul> <li>Monitor storage utilization and implement auto-scaling where possible</li> <li>Use AWS CloudWatch to set up alerts for capacity thresholds</li> <li>Implement data lifecycle policies to manage storage growth</li> </ul> </li> <li><strong>Scalability challenges:</strong> <ul> <li>Identify bottlenecks in data ingestion pipelines</li> <li>Implement parallel processing and distributed systems where appropriate</li> <li>Use AWS services like Kinesis for real-time data ingestion at scale</li> </ul> </li> <li><strong>Debugging techniques:</strong> <ul> <li>Analyze logs and metrics to identify issues</li> <li>Use AWS X-Ray for distributed tracing of applications</li> <li>Implement proper error handling and logging in data pipelines</li> </ul> </li> </ul> <p>Example: Setting up a CloudWatch alarm for S3 bucket size:</p> <p><code> import boto3<br> <br> cloudwatch = boto3.client('cloudwatch')<br> cloudwatch.put_metric_alarm(<br> &nbsp;&nbsp;AlarmName='S3BucketSizeAlarm',<br> &nbsp;&nbsp;ComparisonOperator='GreaterThanThreshold',<br> &nbsp;&nbsp;EvaluationPeriods=1,<br> &nbsp;&nbsp;MetricName='BucketSizeBytes',<br> &nbsp;&nbsp;Namespace='AWS/S3',<br> &nbsp;&nbsp;Period=86400,<br> &nbsp;&nbsp;Statistic='Average',<br> &nbsp;&nbsp;Threshold=1000000000, # 1 GB<br> &nbsp;&nbsp;ActionsEnabled=True,<br> &nbsp;&nbsp;AlarmActions=['arn:aws:sns:region:account-id:topic-name']<br> ) </code></p> 
			<p style="color: #0066cc;"><strong>Skill 6: Making initial storage decisions based on cost, performance, and data structure</strong></p> <p>This skill involves evaluating and selecting appropriate storage solutions based on various factors. Key considerations include:</p> <ul> <li><strong>Cost optimization:</strong> <ul> <li>Compare pricing models of different storage services (e.g., S3 vs. EBS vs. EFS)</li> <li>Implement storage tiering strategies (e.g., S3 Intelligent-Tiering)</li> <li>Consider data transfer costs between services and regions</li> </ul> </li> <li><strong>Performance requirements:</strong> <ul> <li>Evaluate IOPS and throughput needs of the application</li> <li>Consider latency requirements for data access</li> <li>Implement caching solutions where appropriate (e.g., ElastiCache)</li> </ul> </li> <li><strong>Data structure considerations:</strong> <ul> <li>Analyze the nature of data (structured, semi-structured, unstructured)</li> <li>Consider data access patterns (random vs. sequential, read-heavy vs. write-heavy)</li> <li>Evaluate the need for indexing and query capabilities</li> </ul> </li> </ul> <p>Example decision-making process:</p> <ol> <li>For large-scale, infrequently accessed data with no strict performance requirements, consider Amazon S3 Glacier for cost-effective archival storage.</li> <li>For structured data requiring complex queries and transactions, Amazon RDS or Amazon Aurora might be suitable.</li> <li>For high-performance, low-latency block storage needs, consider Amazon EBS with Provisioned IOPS.</li> <li>For shared file systems accessed by multiple EC2 instances, Amazon EFS could be the best choice.</li> </ol> <p>By mastering these skills, you'll be well-prepared to handle various data engineering tasks on AWS, from efficient data extraction and format selection to troubleshooting and making informed storage decisions.</p>


			<hr style="height: 20px; background-color: blue;"/>
			<!--
			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-1: Data Engineering Lifecycle Overview</p>
			<p style="color: goldenrod; font-size:14px;"><strong>Data Engineering Lifecycle Steps</strong></p> <p>The data engineering lifecycle consists of five main steps:</p> <ul> <li>Generation: Where data originates (e.g., databases, IoT devices)</li> <li>Storage: Choosing appropriate data stores</li> <li>Ingestion: Moving data into AWS</li> <li>Transformation: Processing and preparing data</li> <li>Serving: Making data available for use</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Key Considerations for Storage</strong></p> <ul> <li>Choose appropriate AWS storage service for the use case</li> <li>Consider data format</li> <li>Analyze access patterns</li> <li>Determine if data is streaming</li> <li>Assess if data needs to be merged from multiple sources</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>For unprocessed data from IoT devices, Amazon S3 is often preferred over Amazon EFS due to better integration with other AWS services</li> <li>When designing storage solutions, consider immediate access needs, long-term storage requirements, and cost-effectiveness</li> <li>S3 lifecycle policies can help optimize storage costs by moving data between storage classes</li> <li>For SQL querying capabilities, consider using Amazon Athena with S3 rather than alternatives like DynamoDB or Redshift</li> </ul>
			
			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-2: Data Ingestion and Processing</p>
			<p style="color: goldenrod; font-size:14px;"><strong>Data Types and Formats</strong></p> <ul> <li>Structured data</li> <li>Semi-structured data</li> <li>Unstructured data</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>AWS Services for Data Processing</strong></p> <ul> <li>AWS Lambda: For data ingestion tasks</li> <li>Amazon EMR and AWS Glue: For data processing jobs</li> <li>Amazon Kinesis: For real-time data processing</li> <li>AWS Step Functions: For workflow orchestration</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>For real-time insights, AWS Managed Service for Apache Flink is preferred over Amazon Athena</li> <li>When migrating data to S3 using AWS Database Migration Service, consider using Apache Parquet format for more compact storage and faster queries</li> <li>SageMaker's Pipe mode can accelerate data transfer from S3, reducing training time and costs</li> </ul>

			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-3: Data Extraction and Troubleshooting</p>
			<p style="color: goldenrod; font-size:14px;"><strong>Extracting Data from S3</strong></p> <ul> <li>Use Amazon S3 Select for filtering and retrieving subsets of data</li> <li>Utilize Amazon S3 Transfer Acceleration for faster data extraction</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Troubleshooting and Optimization</strong></p> <ul> <li>Consider EBS-optimized instances to separate storage and network traffic</li> <li>Use AWS Glue for ETL jobs to reduce operational management</li> <li>Leverage Amazon Managed Service for Apache Flink for anomaly detection to reduce operational overhead</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>Kinesis Data Streams is preferred over Kinesis Data Firehose for real-time analytics with Amazon Redshift</li> <li>Firehose has limitations on supported destinations (e.g., not directly compatible with Amazon Redshift streaming ingestion)</li> <li>For converting data formats in real-time, consider using AWS Managed Service for Apache Flink with Lambda</li> <li>Kinesis Client Library (KCL) is designed for Kinesis Data Streams, not Firehose delivery streams</li> </ul>

			<hr/>
			-->

			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-1: Data Engineering Lifecycle Overview</p> <p style="color: goldenrod; font-size:14px;"><strong>Data Engineering Lifecycle Steps</strong></p> <p>The data engineering lifecycle consists of five main steps:</p> <ul> <li>Generation: Where data originates (e.g., databases, IoT devices, web applications, social media)</li> <li>Storage: Choosing appropriate data stores based on data type, volume, and access patterns</li> <li>Ingestion: Moving data into AWS using batch or streaming methods</li> <li>Transformation: Processing and preparing data for analysis or machine learning</li> <li>Serving: Making data available for use through APIs, dashboards, or machine learning models</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Key Considerations for Storage</strong></p> <ul> <li>Choose appropriate AWS storage service for the use case (e.g., S3 for object storage, RDS for relational databases)</li> <li>Consider data format (structured, semi-structured, unstructured)</li> <li>Analyze access patterns (frequent vs. infrequent, random vs. sequential)</li> <li>Determine if data is streaming or batch</li> <li>Assess if data needs to be merged from multiple sources</li> <li>Evaluate data retention requirements and compliance needs</li> <li>Consider data encryption and access control mechanisms</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>For unprocessed data from IoT devices, Amazon S3 is often preferred over Amazon EFS due to better integration with other AWS services</li> <li>When designing storage solutions, consider immediate access needs, long-term storage requirements, and cost-effectiveness</li> <li>S3 lifecycle policies can help optimize storage costs by moving data between storage classes (e.g., Standard to Glacier)</li> <li>For SQL querying capabilities, consider using Amazon Athena with S3 rather than alternatives like DynamoDB or Redshift</li> <li>Use S3 Select or Glacier Select to retrieve specific data subsets, reducing data transfer costs and processing time</li> <li>Consider using S3 Intelligent-Tiering for data with unknown or changing access patterns</li> <li>Implement proper IAM policies and bucket policies to ensure secure access to S3 data</li> </ul> 
			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-2: Data Ingestion and Processing</p> <p style="color: goldenrod; font-size:14px;"><strong>Data Types and Formats</strong></p> <ul> <li>Structured data: Relational databases, CSV files</li> <li>Semi-structured data: JSON, XML, YAML</li> <li>Unstructured data: Text documents, images, audio files</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>AWS Services for Data Processing</strong></p> <ul> <li>AWS Lambda: For serverless data ingestion tasks and light processing</li> <li>Amazon EMR: For big data processing using Hadoop ecosystem tools</li> <li>AWS Glue: For serverless ETL jobs and data cataloging</li> <li>Amazon Kinesis: For real-time data processing and analytics</li> <li>AWS Step Functions: For workflow orchestration and complex data pipelines</li> <li>Amazon SageMaker: For machine learning data preparation and model training</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>For real-time insights, AWS Managed Service for Apache Flink is preferred over Amazon Athena</li> <li>When migrating data to S3 using AWS Database Migration Service, consider using Apache Parquet format for more compact storage and faster queries</li> <li>SageMaker's Pipe mode can accelerate data transfer from S3, reducing training time and costs</li> <li>Use Kinesis Data Analytics for SQL to process streaming data with standard SQL queries</li> <li>Consider using AWS Glue DataBrew for visual data preparation without coding</li> <li>Implement error handling and dead-letter queues in Lambda functions for reliable data processing</li> <li>Use AWS Batch for long-running, resource-intensive data processing jobs</li> </ul> 
			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-3: Data Extraction and Troubleshooting</p> <p style="color: goldenrod; font-size:14px;"><strong>Extracting Data from S3</strong></p> <ul> <li>Use Amazon S3 Select for filtering and retrieving subsets of data</li> <li>Utilize Amazon S3 Transfer Acceleration for faster data extraction over long distances</li> <li>Implement S3 Batch Operations for large-scale data retrieval and processing</li> <li>Use AWS DataSync for automated and scheduled data transfer between S3 and on-premises storage</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Troubleshooting and Optimization</strong></p> <ul> <li>Consider EBS-optimized instances to separate storage and network traffic</li> <li>Use AWS Glue for ETL jobs to reduce operational management</li> <li>Leverage Amazon Managed Service for Apache Flink for anomaly detection to reduce operational overhead</li> <li>Implement proper monitoring and logging using Amazon CloudWatch and AWS X-Ray</li> <li>Use AWS Cost Explorer and AWS Trusted Advisor to optimize costs and performance</li> <li>Implement data partitioning in S3 to improve query performance and reduce costs</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>Kinesis Data Streams is preferred over Kinesis Data Firehose for real-time analytics with Amazon Redshift</li> <li>Firehose has limitations on supported destinations (e.g., not directly compatible with Amazon Redshift streaming ingestion)</li> <li>For converting data formats in real-time, consider using AWS Managed Service for Apache Flink with Lambda</li> <li>Kinesis Client Library (KCL) is designed for Kinesis Data Streams, not Firehose delivery streams</li> <li>Use VPC endpoints to improve security and reduce data transfer costs when accessing S3 from within a VPC</li> <li>Implement retry mechanisms and idempotency in data processing pipelines to handle transient failures</li> <li>Consider using AWS Lake Formation for centralized data lake governance and access control</li> </ul>

			<hr/>

			<table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">AWS Service</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Key Points</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Best Solution Characteristics</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Potential Disqualifying Factors</th> </tr> 
				<tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon S3</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Object storage</li> <li>Supports various storage classes</li> <li>Lifecycle policies</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Highly scalable and durable</li> <li>Integrates well with other AWS services</li> <li>Cost-effective for large datasets</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Not suitable for transactional databases</li> <li>Higher latency compared to block storage</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon EFS</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed file storage</li> <li>Supports concurrent access</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Ideal for shared file systems</li> <li>Scalable and elastic</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Less integration with ML services compared to S3</li> <li>Higher cost for large-scale data storage</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon Athena</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Serverless query service</li> <li>Works with S3 data</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Easy SQL querying of S3 data</li> <li>No infrastructure management</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Not suitable for real-time querying</li> <li>Limited to S3 data sources</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">AWS Lambda</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Serverless compute service</li> <li>Event-driven execution</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Ideal for short-running tasks</li> <li>Automatic scaling</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Limited execution time (15 minutes max)</li> <li>Not suitable for long-running processes</li> </ul> </td> </tr> 
				<tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon EMR</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed big data platform</li> <li>Supports various frameworks (e.g., Spark)</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Powerful for large-scale data processing</li> <li>Flexible and customizable</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Complex setup and management</li> <li>Higher cost for small-scale tasks</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">AWS Glue</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed ETL service</li> <li>Serverless data integration</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Easy to use for ETL jobs</li> <li>Integrates well with other AWS services</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Limited control over underlying infrastructure</li> <li>May not be cost-effective for very large-scale jobs</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon Kinesis</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Real-time data streaming</li> <li>Includes Data Streams and Firehose</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Ideal for real-time data ingestion</li> <li>Scalable and durable</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Firehose has limited destinations</li> <li>Can be complex for simple use cases</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">AWS Managed Service for Apache Flink</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed stream processing</li> <li>Real-time analytics</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Powerful for complex stream processing</li> <li>Reduced operational overhead</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Learning curve for Flink programming</li> <li>May be overkill for simple streaming tasks</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon SageMaker</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed machine learning platform</li> <li>Includes Feature Store and Canvas</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Comprehensive ML workflow support</li> <li>Integrates well with other AWS services</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Can be complex for simple ML tasks</li> <li>Higher operational overhead compared to some alternatives</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon Redshift</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Data warehousing service</li> <li>Supports SQL queries</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Excellent for large-scale analytics</li> <li>High performance for complex queries</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Not ideal for real-time data ingestion</li> <li>Can be expensive for small datasets</li> </ul> </td> </tr> </table>
				This table provides a quick reference for the key AWS services mentioned in the transcript, highlighting their strengths and potential limitations in different scenarios.

		</div>
	</div>
	
	<br/>
	
</div>


<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
