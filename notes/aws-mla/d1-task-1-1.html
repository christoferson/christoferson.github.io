<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning Engineer Associate (MLA)</h1>
  
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Domain 1: Data Preparation for Machine Learning (ML)</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
			<p style="color: blueviolet; font-size: 20px;"><stong>Task Statement 1.1: Ingest and store data. </stong></p>
			
			<p style="color: #0066cc;"><strong>Knowledge 1: Data formats and ingestion mechanisms (for example, validated and non-validated formats, Apache Parquet, JSON, CSV, Apache ORC, Apache Avro, RecordIO)</strong></p> <p>Understanding various data formats and ingestion mechanisms is crucial for effective data management and analysis in cloud environments. Let's explore some common formats and their characteristics:</p> <ul> <li><strong>Validated vs. Non-validated formats:</strong> <ul> <li>Validated formats have a predefined structure and schema, ensuring data consistency.</li> <li>Non-validated formats are more flexible but may require additional processing to ensure data quality.</li> </ul> </li> <li><strong>Apache Parquet:</strong> <ul> <li>A columnar storage format optimized for analytics workloads.</li> <li>Offers efficient compression and encoding schemes.</li> <li>Ideal for big data processing frameworks like Apache Spark.</li> </ul> </li> <li><strong>JSON (JavaScript Object Notation):</strong> <ul> <li>A lightweight, human-readable data interchange format.</li> <li>Widely used for API responses and configuration files.</li> <li>Flexible structure, but can be less efficient for large datasets.</li> </ul> </li> <li><strong>CSV (Comma-Separated Values):</strong> <ul> <li>Simple, tabular format for storing data.</li> <li>Easy to read and write, but lacks metadata and type information.</li> <li>Commonly used for data exchange between different systems.</li> </ul> </li> <li><strong>Apache ORC (Optimized Row Columnar):</strong> <ul> <li>Another columnar storage format, similar to Parquet.</li> <li>Designed for Hadoop ecosystems, particularly with Hive.</li> <li>Offers good compression and fast data retrieval.</li> </ul> </li> <li><strong>Apache Avro:</strong> <ul> <li>Row-based data serialization format.</li> <li>Supports schema evolution, making it suitable for changing data structures.</li> <li>Compact binary format, good for high-throughput data processing.</li> </ul> </li> <li><strong>RecordIO:</strong> <ul> <li>A binary format used in machine learning, particularly with Amazon SageMaker.</li> <li>Optimized for streaming data and efficient I/O operations.</li> <li>Supports both labeled and unlabeled data for training models.</li> </ul> </li> </ul> <p>When choosing a data format, consider factors such as data size, query patterns, processing requirements, and compatibility with your analytics tools. Each format has its strengths and is suited for different use cases in the data pipeline.</p> <p style="color: #0066cc;"><strong>Knowledge 2: How to use the core AWS data sources (for example, Amazon S3, Amazon Elastic File System [Amazon EFS], Amazon FSx for NetApp ONTAP)</strong></p> <p>AWS provides various data storage options, each with unique characteristics. Let's explore the core AWS data sources:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Object storage service designed for scalability, data availability, security, and performance.</li> <li>Ideal for storing large amounts of unstructured data, such as logs, backups, and data lakes.</li> <li>Supports versioning, lifecycle policies, and fine-grained access controls.</li> <li>Example use: Storing raw data for big data analytics or serving static website content.</li> </ul> </li> <li><strong>Amazon Elastic File System (Amazon EFS):</strong> <ul> <li>Fully managed NFS file system for use with AWS Cloud services and on-premises resources.</li> <li>Automatically grows and shrinks as you add and remove files.</li> <li>Supports concurrent access from multiple EC2 instances.</li> <li>Example use: Shared file storage for content management systems or development environments.</li> </ul> </li> <li><strong>Amazon FSx for NetApp ONTAP:</strong> <ul> <li>Fully managed file storage built on NetApp's ONTAP file system.</li> <li>Provides NFS, SMB, and iSCSI protocols for broad application compatibility.</li> <li>Offers advanced data management features like snapshots, replication, and data tiering.</li> <li>Example use: Enterprise applications requiring high-performance shared storage with advanced features.</li> </ul> </li> </ul> <p>When using these data sources, consider the following best practices:</p> <ul> <li>Use S3 for large-scale, cost-effective storage of unstructured data.</li> <li>Leverage EFS for shared file systems that require POSIX compliance and concurrent access.</li> <li>Choose FSx for NetApp ONTAP when you need enterprise-grade features and compatibility with existing NetApp workflows.</li> <li>Implement proper access controls and encryption for all data sources to ensure security.</li> <li>Use appropriate data transfer methods (e.g., AWS DataSync, S3 Transfer Acceleration) for efficient data movement between sources.</li> </ul> <p style="color: #0066cc;"><strong>Knowledge 3: How to use AWS streaming data sources to ingest data (for example, Amazon Kinesis, Apache Flink, Apache Kafka)</strong></p> <p>Streaming data ingestion is crucial for real-time analytics and processing. AWS offers several options for handling streaming data:</p> <ul> <li><strong>Amazon Kinesis:</strong> <ul> <li>A suite of services for real-time streaming data processing.</li> <li>Kinesis Data Streams: For ingesting and storing streaming data.</li> <li>Kinesis Data Firehose: For loading streaming data into data stores and analytics tools.</li> <li>Kinesis Data Analytics: For processing streaming data in real-time using SQL or Apache Flink.</li> <li>Example use: Processing clickstream data for real-time website analytics.</li> </ul> </li> <li><strong>Apache Flink on AWS:</strong> <ul> <li>Open-source stream processing framework that can be deployed on AWS.</li> <li>Supports both batch and stream processing with low latency.</li> <li>Can be used with Amazon Kinesis Data Analytics for Java applications.</li> <li>Example use: Complex event processing for IoT sensor data.</li> </ul> </li> <li><strong>Apache Kafka on AWS:</strong> <ul> <li>Can be deployed on EC2 instances or used as a managed service with Amazon MSK (Managed Streaming for Apache Kafka).</li> <li>Provides high-throughput, fault-tolerant, publish-subscribe messaging system.</li> <li>Ideal for building real-time data pipelines and streaming applications.</li> <li>Example use: Building a real-time log aggregation system for distributed applications.</li> </ul> </li> </ul> <p>Best practices for using streaming data sources:</p> <ul> <li>Choose the right service based on your data volume, processing requirements, and latency needs.</li> <li>Implement proper error handling and retry mechanisms to deal with data inconsistencies.</li> <li>Use appropriate scaling strategies to handle varying data ingestion rates.</li> <li>Implement data transformation and enrichment as close to the ingestion point as possible.</li> <li>Monitor your streaming pipelines for performance and data quality issues.</li> </ul> <p style="color: #0066cc;"><strong>Knowledge 4: AWS storage options, including use cases and tradeoffs</strong></p> <p>AWS offers a variety of storage options to cater to different use cases. Understanding their characteristics and tradeoffs is essential for optimal data management:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Use case: Object storage for large-scale, unstructured data.</li> <li>Pros: Highly scalable, durable, and cost-effective.</li> <li>Cons: Not suitable for file system operations or transactional databases.</li> </ul> </li> <li><strong>Amazon EBS (Elastic Block Store):</strong> <ul> <li>Use case: Block-level storage volumes for EC2 instances.</li> <li>Pros: Low-latency, persistent storage for databases and file systems.</li> <li>Cons: Limited to a single Availability Zone, more expensive than S3 for large datasets.</li> </ul> </li> <li><strong>Amazon EFS (Elastic File System):</strong> <ul> <li>Use case: Shared file storage for Linux-based workloads.</li> <li>Pros: Scalable, supports concurrent access from multiple EC2 instances.</li> <li>Cons: Higher latency compared to EBS, not suitable for Windows workloads.</li> </ul> </li> <li><strong>Amazon FSx:</strong> <ul> <li>Use case: Fully managed file systems for Windows (FSx for Windows File Server) and high-performance computing (FSx for Lustre).</li> <li>Pros: Native compatibility with specific workloads, high performance.</li> <li>Cons: More expensive than general-purpose storage options.</li> </ul> </li> <li><strong>Amazon DynamoDB:</strong> <ul> <li>Use case: NoSQL database for applications requiring low-latency data access.</li> <li>Pros: Fully managed, scalable, and highly available.</li> <li>Cons: Limited query flexibility compared to relational databases.</li> </ul> </li> <li><strong>Amazon RDS (Relational Database Service):</strong> <ul> <li>Use case: Managed relational databases for structured data.</li> <li>Pros: Easy to set up, manage, and scale relational databases.</li> <li>Cons: Less control over the underlying infrastructure compared to self-managed databases.</li> </ul> </li> </ul> <p>When choosing a storage option, consider factors such as:</p> <ul> <li>Data structure and access patterns</li> <li>Performance requirements (IOPS, throughput)</li> <li>Scalability needs</li> <li>Cost considerations</li> <li>Data durability and availability requirements</li> <li>Integration with other AWS services and existing applications</li> </ul> <p>By carefully evaluating these factors and understanding the tradeoffs between different storage options, you can design an efficient and cost-effective storage architecture for your AWS-based applications and data processing pipelines.</p>

			<p style="color: #0066cc;"><strong>Skill 1: Extracting data from storage (for example, Amazon S3, Amazon Elastic Block Store [Amazon EBS], Amazon EFS, Amazon RDS, Amazon DynamoDB) by using relevant AWS service options (for example, Amazon S3 Transfer Acceleration, Amazon EBS Provisioned IOPS)</strong></p> <p>This skill involves the ability to efficiently retrieve data from various AWS storage services using appropriate methods and optimizations. Here's a breakdown of the key components:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Use AWS SDK or CLI to download objects</li> <li>Implement S3 Transfer Acceleration for faster transfers over long distances</li> <li>Utilize S3 Select for retrieving specific data subsets</li> </ul> </li> <li><strong>Amazon EBS (Elastic Block Store):</strong> <ul> <li>Attach EBS volumes to EC2 instances for direct access</li> <li>Use EBS Provisioned IOPS for high-performance, low-latency workloads</li> <li>Create EBS snapshots for backup and data transfer</li> </ul> </li> <li><strong>Amazon EFS (Elastic File System):</strong> <ul> <li>Mount EFS file systems to multiple EC2 instances</li> <li>Use EFS-to-EFS backup for data protection and migration</li> </ul> </li> <li><strong>Amazon RDS (Relational Database Service):</strong> <ul> <li>Use SQL queries to extract data from RDS databases</li> <li>Implement read replicas for improved performance on read-heavy workloads</li> </ul> </li> <li><strong>Amazon DynamoDB:</strong> <ul> <li>Use Query and Scan operations to retrieve data</li> <li>Implement DynamoDB Accelerator (DAX) for faster read performance</li> </ul> </li> </ul> <p>Example: To extract data from an S3 bucket using Python and the AWS SDK (boto3):</p> <p><code> import boto3<br> s3 = boto3.client('s3')<br> response = s3.get_object(Bucket='my-bucket', Key='my-object')<br> data = response['Body'].read() </code></p> <p style="color: #0066cc;"><strong>Skill 2: Choosing appropriate data formats (for example, Parquet, JSON, CSV, ORC) based on data access patterns</strong></p> <p>This skill requires understanding different data formats and their characteristics to select the most suitable one for specific use cases. Key considerations include:</p> <ul> <li><strong>Parquet:</strong> <ul> <li>Columnar storage format</li> <li>Efficient for analytical queries and large datasets</li> <li>Good for data with many columns but only a subset is typically accessed</li> </ul> </li> <li><strong>JSON (JavaScript Object Notation):</strong> <ul> <li>Human-readable and easy to parse</li> <li>Flexible schema, good for nested and hierarchical data</li> <li>Suitable for web applications and APIs</li> </ul> </li> <li><strong>CSV (Comma-Separated Values):</strong> <ul> <li>Simple, tabular format</li> <li>Easy to read and write</li> <li>Good for flat data structures and compatibility with spreadsheet applications</li> </ul> </li> <li><strong>ORC (Optimized Row Columnar):</strong> <ul> <li>Columnar storage format optimized for Hive</li> <li>Efficient compression and encoding schemes</li> <li>Good for large-scale data processing in Hadoop ecosystems</li> </ul> </li> </ul> <p>Example: If you're working with time-series data that requires frequent aggregations and filtering on specific columns, Parquet might be the best choice due to its columnar nature and efficient compression.</p> <p style="color: #0066cc;"><strong>Skill 3: Ingesting data into Amazon SageMaker Data Wrangler and SageMaker Feature Store</strong></p> <p>This skill involves understanding how to import and prepare data for machine learning workflows using Amazon SageMaker tools:</p> <ul> <li><strong>Amazon SageMaker Data Wrangler:</strong> <ul> <li>Import data from various sources (S3, Athena, Redshift)</li> <li>Perform data transformations and feature engineering</li> <li>Visualize and analyze data distributions</li> <li>Export prepared data to S3 or use directly in SageMaker</li> </ul> </li> <li><strong>SageMaker Feature Store:</strong> <ul> <li>Create feature groups to organize related features</li> <li>Ingest features using the Feature Store API or SageMaker Processing jobs</li> <li>Configure online and offline storage for feature access</li> <li>Set up feature versioning and time travel capabilities</li> </ul> </li> </ul> <p>Example: To ingest data into SageMaker Feature Store using Python:</p> <p><code> from sagemaker.feature_store.feature_group import FeatureGroup<br> from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum<br> <br> feature_group = FeatureGroup(name="my-feature-group", sagemaker_session=session)<br> feature_group.load_feature_definitions([<br> FeatureDefinition(feature_name="feature1", feature_type=FeatureTypeEnum.STRING),<br> FeatureDefinition(feature_name="feature2", feature_type=FeatureTypeEnum.INTEGRAL)<br> ])<br> feature_group.create(s3_uri="s3://my-bucket/my-prefix", record_identifier_name="id")<br> feature_group.ingest(data_frame=my_dataframe) </code></p> <p style="color: #0066cc;"><strong>Skill 4: Merging data from multiple sources (for example, by using programming techniques, AWS Glue, Apache Spark)</strong></p> <p>This skill focuses on combining data from various sources to create a unified dataset. Key aspects include:</p> <ul> <li><strong>Programming techniques:</strong> <ul> <li>Use pandas or other data manipulation libraries for small to medium datasets</li> <li>Implement efficient join algorithms (e.g., hash join, merge join)</li> <li>Handle data type conversions and schema alignment</li> </ul> </li> <li><strong>AWS Glue:</strong> <ul> <li>Create Glue jobs to extract, transform, and load (ETL) data</li> <li>Use Glue Data Catalog to discover and manage metadata</li> <li>Implement Glue DynamicFrames for schema evolution</li> </ul> </li> <li><strong>Apache Spark:</strong> <ul> <li>Utilize Spark SQL for distributed data processing</li> <li>Implement efficient join strategies (e.g., broadcast join, shuffle hash join)</li> <li>Optimize performance using partitioning and caching</li> </ul> </li> </ul> <p>Example: Merging data using pandas in Python:</p> <p><code> import pandas as pd<br> <br> df1 = pd.read_csv('source1.csv')<br> df2 = pd.read_csv('source2.csv')<br> merged_df = pd.merge(df1, df2, on='common_column', how='inner') </code></p> <p style="color: #0066cc;"><strong>Skill 5: Troubleshooting and debugging data ingestion and storage issues that involve capacity and scalability</strong></p> <p>This skill involves identifying and resolving problems related to data ingestion and storage, particularly those affecting system capacity and scalability. Key aspects include:</p> <ul> <li><strong>Capacity issues:</strong> <ul> <li>Monitor storage utilization and implement auto-scaling where possible</li> <li>Use AWS CloudWatch to set up alerts for capacity thresholds</li> <li>Implement data lifecycle policies to manage storage growth</li> </ul> </li> <li><strong>Scalability challenges:</strong> <ul> <li>Identify bottlenecks in data ingestion pipelines</li> <li>Implement parallel processing and distributed systems where appropriate</li> <li>Use AWS services like Kinesis for real-time data ingestion at scale</li> </ul> </li> <li><strong>Debugging techniques:</strong> <ul> <li>Analyze logs and metrics to identify issues</li> <li>Use AWS X-Ray for distributed tracing of applications</li> <li>Implement proper error handling and logging in data pipelines</li> </ul> </li> </ul> <p>Example: Setting up a CloudWatch alarm for S3 bucket size:</p> <p><code> import boto3<br> <br> cloudwatch = boto3.client('cloudwatch')<br> cloudwatch.put_metric_alarm(<br> AlarmName='S3BucketSizeAlarm',<br> ComparisonOperator='GreaterThanThreshold',<br> EvaluationPeriods=1,<br> MetricName='BucketSizeBytes',<br> Namespace='AWS/S3',<br> Period=86400,<br> Statistic='Average',<br> Threshold=1000000000, # 1 GB<br> ActionsEnabled=True,<br> AlarmActions=['arn:aws:sns:region:account-id:topic-name']<br> ) </code></p> <p style="color: #0066cc;"><strong>Skill 6: Making initial storage decisions based on cost, performance, and data structure</strong></p> <p>This skill involves evaluating and selecting appropriate storage solutions based on various factors. Key considerations include:</p> <ul> <li><strong>Cost optimization:</strong> <ul> <li>Compare pricing models of different storage services (e.g., S3 vs. EBS vs. EFS)</li> <li>Implement storage tiering strategies (e.g., S3 Intelligent-Tiering)</li> <li>Consider data transfer costs between services and regions</li> </ul> </li> <li><strong>Performance requirements:</strong> <ul> <li>Evaluate IOPS and throughput needs of the application</li> <li>Consider latency requirements for data access</li> <li>Implement caching solutions where appropriate (e.g., ElastiCache)</li> </ul> </li> <li><strong>Data structure considerations:</strong> <ul> <li>Analyze the nature of data (structured, semi-structured, unstructured)</li> <li>Consider data access patterns (random vs. sequential, read-heavy vs. write-heavy)</li> <li>Evaluate the need for indexing and query capabilities</li> </ul> </li> </ul> <p>Example decision-making process:</p> <ol> <li>For large-scale, infrequently accessed data with no strict performance requirements, consider Amazon S3 Glacier for cost-effective archival storage.</li> <li>For structured data requiring complex queries and transactions, Amazon RDS or Amazon Aurora might be suitable.</li> <li>For high-performance, low-latency block storage needs, consider Amazon EBS with Provisioned IOPS.</li> <li>For shared file systems accessed by multiple EC2 instances, Amazon EFS could be the best choice.</li> </ol> <p>By mastering these skills, you'll be well-prepared to handle various data engineering tasks on AWS, from efficient data extraction and format selection to troubleshooting and making informed storage decisions.</p>


			<hr/>

			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-1: Data Engineering Lifecycle Overview</p>
			<p style="color: goldenrod; font-size:14px;"><strong>Data Engineering Lifecycle Steps</strong></p> <p>The data engineering lifecycle consists of five main steps:</p> <ul> <li>Generation: Where data originates (e.g., databases, IoT devices)</li> <li>Storage: Choosing appropriate data stores</li> <li>Ingestion: Moving data into AWS</li> <li>Transformation: Processing and preparing data</li> <li>Serving: Making data available for use</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Key Considerations for Storage</strong></p> <ul> <li>Choose appropriate AWS storage service for the use case</li> <li>Consider data format</li> <li>Analyze access patterns</li> <li>Determine if data is streaming</li> <li>Assess if data needs to be merged from multiple sources</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>For unprocessed data from IoT devices, Amazon S3 is often preferred over Amazon EFS due to better integration with other AWS services</li> <li>When designing storage solutions, consider immediate access needs, long-term storage requirements, and cost-effectiveness</li> <li>S3 lifecycle policies can help optimize storage costs by moving data between storage classes</li> <li>For SQL querying capabilities, consider using Amazon Athena with S3 rather than alternatives like DynamoDB or Redshift</li> </ul>
			
			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-2: Data Ingestion and Processing</p>
			<p style="color: goldenrod; font-size:14px;"><strong>Data Types and Formats</strong></p> <ul> <li>Structured data</li> <li>Semi-structured data</li> <li>Unstructured data</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>AWS Services for Data Processing</strong></p> <ul> <li>AWS Lambda: For data ingestion tasks</li> <li>Amazon EMR and AWS Glue: For data processing jobs</li> <li>Amazon Kinesis: For real-time data processing</li> <li>AWS Step Functions: For workflow orchestration</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>For real-time insights, AWS Managed Service for Apache Flink is preferred over Amazon Athena</li> <li>When migrating data to S3 using AWS Database Migration Service, consider using Apache Parquet format for more compact storage and faster queries</li> <li>SageMaker's Pipe mode can accelerate data transfer from S3, reducing training time and costs</li> </ul>

			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-3: Data Extraction and Troubleshooting</p>
			<p style="color: goldenrod; font-size:14px;"><strong>Extracting Data from S3</strong></p> <ul> <li>Use Amazon S3 Select for filtering and retrieving subsets of data</li> <li>Utilize Amazon S3 Transfer Acceleration for faster data extraction</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Troubleshooting and Optimization</strong></p> <ul> <li>Consider EBS-optimized instances to separate storage and network traffic</li> <li>Use AWS Glue for ETL jobs to reduce operational management</li> <li>Leverage Amazon Managed Service for Apache Flink for anomaly detection to reduce operational overhead</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>Kinesis Data Streams is preferred over Kinesis Data Firehose for real-time analytics with Amazon Redshift</li> <li>Firehose has limitations on supported destinations (e.g., not directly compatible with Amazon Redshift streaming ingestion)</li> <li>For converting data formats in real-time, consider using AWS Managed Service for Apache Flink with Lambda</li> <li>Kinesis Client Library (KCL) is designed for Kinesis Data Streams, not Firehose delivery streams</li> </ul>

			<hr/>

			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-1: Data Engineering Lifecycle Overview</p> <p style="color: goldenrod; font-size:14px;"><strong>Data Engineering Lifecycle Steps</strong></p> <p>The data engineering lifecycle consists of five main steps:</p> <ul> <li>Generation: Where data originates (e.g., databases, IoT devices, web applications, social media)</li> <li>Storage: Choosing appropriate data stores based on data type, volume, and access patterns</li> <li>Ingestion: Moving data into AWS using batch or streaming methods</li> <li>Transformation: Processing and preparing data for analysis or machine learning</li> <li>Serving: Making data available for use through APIs, dashboards, or machine learning models</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Key Considerations for Storage</strong></p> <ul> <li>Choose appropriate AWS storage service for the use case (e.g., S3 for object storage, RDS for relational databases)</li> <li>Consider data format (structured, semi-structured, unstructured)</li> <li>Analyze access patterns (frequent vs. infrequent, random vs. sequential)</li> <li>Determine if data is streaming or batch</li> <li>Assess if data needs to be merged from multiple sources</li> <li>Evaluate data retention requirements and compliance needs</li> <li>Consider data encryption and access control mechanisms</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>For unprocessed data from IoT devices, Amazon S3 is often preferred over Amazon EFS due to better integration with other AWS services</li> <li>When designing storage solutions, consider immediate access needs, long-term storage requirements, and cost-effectiveness</li> <li>S3 lifecycle policies can help optimize storage costs by moving data between storage classes (e.g., Standard to Glacier)</li> <li>For SQL querying capabilities, consider using Amazon Athena with S3 rather than alternatives like DynamoDB or Redshift</li> <li>Use S3 Select or Glacier Select to retrieve specific data subsets, reducing data transfer costs and processing time</li> <li>Consider using S3 Intelligent-Tiering for data with unknown or changing access patterns</li> <li>Implement proper IAM policies and bucket policies to ensure secure access to S3 data</li> </ul> 
			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-2: Data Ingestion and Processing</p> <p style="color: goldenrod; font-size:14px;"><strong>Data Types and Formats</strong></p> <ul> <li>Structured data: Relational databases, CSV files</li> <li>Semi-structured data: JSON, XML, YAML</li> <li>Unstructured data: Text documents, images, audio files</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>AWS Services for Data Processing</strong></p> <ul> <li>AWS Lambda: For serverless data ingestion tasks and light processing</li> <li>Amazon EMR: For big data processing using Hadoop ecosystem tools</li> <li>AWS Glue: For serverless ETL jobs and data cataloging</li> <li>Amazon Kinesis: For real-time data processing and analytics</li> <li>AWS Step Functions: For workflow orchestration and complex data pipelines</li> <li>Amazon SageMaker: For machine learning data preparation and model training</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>For real-time insights, AWS Managed Service for Apache Flink is preferred over Amazon Athena</li> <li>When migrating data to S3 using AWS Database Migration Service, consider using Apache Parquet format for more compact storage and faster queries</li> <li>SageMaker's Pipe mode can accelerate data transfer from S3, reducing training time and costs</li> <li>Use Kinesis Data Analytics for SQL to process streaming data with standard SQL queries</li> <li>Consider using AWS Glue DataBrew for visual data preparation without coding</li> <li>Implement error handling and dead-letter queues in Lambda functions for reliable data processing</li> <li>Use AWS Batch for long-running, resource-intensive data processing jobs</li> </ul> 
			<p style="color: orangered; font-size:16px;font-weight: bold;">Topic-3: Data Extraction and Troubleshooting</p> <p style="color: goldenrod; font-size:14px;"><strong>Extracting Data from S3</strong></p> <ul> <li>Use Amazon S3 Select for filtering and retrieving subsets of data</li> <li>Utilize Amazon S3 Transfer Acceleration for faster data extraction over long distances</li> <li>Implement S3 Batch Operations for large-scale data retrieval and processing</li> <li>Use AWS DataSync for automated and scheduled data transfer between S3 and on-premises storage</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Troubleshooting and Optimization</strong></p> <ul> <li>Consider EBS-optimized instances to separate storage and network traffic</li> <li>Use AWS Glue for ETL jobs to reduce operational management</li> <li>Leverage Amazon Managed Service for Apache Flink for anomaly detection to reduce operational overhead</li> <li>Implement proper monitoring and logging using Amazon CloudWatch and AWS X-Ray</li> <li>Use AWS Cost Explorer and AWS Trusted Advisor to optimize costs and performance</li> <li>Implement data partitioning in S3 to improve query performance and reduce costs</li> </ul> <p style="color: goldenrod; font-size:14px;"><strong>Gotchas and Insights</strong></p> <ul> <li>Kinesis Data Streams is preferred over Kinesis Data Firehose for real-time analytics with Amazon Redshift</li> <li>Firehose has limitations on supported destinations (e.g., not directly compatible with Amazon Redshift streaming ingestion)</li> <li>For converting data formats in real-time, consider using AWS Managed Service for Apache Flink with Lambda</li> <li>Kinesis Client Library (KCL) is designed for Kinesis Data Streams, not Firehose delivery streams</li> <li>Use VPC endpoints to improve security and reduce data transfer costs when accessing S3 from within a VPC</li> <li>Implement retry mechanisms and idempotency in data processing pipelines to handle transient failures</li> <li>Consider using AWS Lake Formation for centralized data lake governance and access control</li> </ul>

			<hr/>

			<table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">AWS Service</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Key Points</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Best Solution Characteristics</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Potential Disqualifying Factors</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon S3</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Object storage</li> <li>Supports various storage classes</li> <li>Lifecycle policies</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Highly scalable and durable</li> <li>Integrates well with other AWS services</li> <li>Cost-effective for large datasets</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Not suitable for transactional databases</li> <li>Higher latency compared to block storage</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon EFS</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed file storage</li> <li>Supports concurrent access</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Ideal for shared file systems</li> <li>Scalable and elastic</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Less integration with ML services compared to S3</li> <li>Higher cost for large-scale data storage</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon Athena</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Serverless query service</li> <li>Works with S3 data</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Easy SQL querying of S3 data</li> <li>No infrastructure management</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Not suitable for real-time querying</li> <li>Limited to S3 data sources</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">AWS Lambda</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Serverless compute service</li> <li>Event-driven execution</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Ideal for short-running tasks</li> <li>Automatic scaling</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Limited execution time (15 minutes max)</li> <li>Not suitable for long-running processes</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon EMR</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed big data platform</li> <li>Supports various frameworks (e.g., Spark)</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Powerful for large-scale data processing</li> <li>Flexible and customizable</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Complex setup and management</li> <li>Higher cost for small-scale tasks</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">AWS Glue</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed ETL service</li> <li>Serverless data integration</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Easy to use for ETL jobs</li> <li>Integrates well with other AWS services</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Limited control over underlying infrastructure</li> <li>May not be cost-effective for very large-scale jobs</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon Kinesis</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Real-time data streaming</li> <li>Includes Data Streams and Firehose</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Ideal for real-time data ingestion</li> <li>Scalable and durable</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Firehose has limited destinations</li> <li>Can be complex for simple use cases</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">AWS Managed Service for Apache Flink</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed stream processing</li> <li>Real-time analytics</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Powerful for complex stream processing</li> <li>Reduced operational overhead</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Learning curve for Flink programming</li> <li>May be overkill for simple streaming tasks</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon SageMaker</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Managed machine learning platform</li> <li>Includes Feature Store and Canvas</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Comprehensive ML workflow support</li> <li>Integrates well with other AWS services</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Can be complex for simple ML tasks</li> <li>Higher operational overhead compared to some alternatives</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Amazon Redshift</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Data warehousing service</li> <li>Supports SQL queries</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Excellent for large-scale analytics</li> <li>High performance for complex queries</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Not ideal for real-time data ingestion</li> <li>Can be expensive for small datasets</li> </ul> </td> </tr> </table>
This table provides a quick reference for the key AWS services mentioned in the transcript, highlighting their strengths and potential limitations in different scenarios.

		</div>
	</div>
	
	<br/>
	
</div>


<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
