<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning Engineer Associate (MLA)</h1>
  
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Domain 1: Data Preparation for Machine Learning (ML)</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
			<p style="color: blueviolet; font-size: 20px;"><stong>Task Statement 1.1: Ingest and store data. </stong></p>
			
			<p style="color: #0066cc;"><strong>Knowledge 1: Data formats and ingestion mechanisms (for example, validated and non-validated formats, Apache Parquet, JSON, CSV, Apache ORC, Apache Avro, RecordIO)</strong></p> <p>Understanding various data formats and ingestion mechanisms is crucial for effective data management and analysis in cloud environments. Let's explore some common formats and their characteristics:</p> <ul> <li><strong>Validated vs. Non-validated formats:</strong> <ul> <li>Validated formats have a predefined structure and schema, ensuring data consistency.</li> <li>Non-validated formats are more flexible but may require additional processing to ensure data quality.</li> </ul> </li> <li><strong>Apache Parquet:</strong> <ul> <li>A columnar storage format optimized for analytics workloads.</li> <li>Offers efficient compression and encoding schemes.</li> <li>Ideal for big data processing frameworks like Apache Spark.</li> </ul> </li> <li><strong>JSON (JavaScript Object Notation):</strong> <ul> <li>A lightweight, human-readable data interchange format.</li> <li>Widely used for API responses and configuration files.</li> <li>Flexible structure, but can be less efficient for large datasets.</li> </ul> </li> <li><strong>CSV (Comma-Separated Values):</strong> <ul> <li>Simple, tabular format for storing data.</li> <li>Easy to read and write, but lacks metadata and type information.</li> <li>Commonly used for data exchange between different systems.</li> </ul> </li> <li><strong>Apache ORC (Optimized Row Columnar):</strong> <ul> <li>Another columnar storage format, similar to Parquet.</li> <li>Designed for Hadoop ecosystems, particularly with Hive.</li> <li>Offers good compression and fast data retrieval.</li> </ul> </li> <li><strong>Apache Avro:</strong> <ul> <li>Row-based data serialization format.</li> <li>Supports schema evolution, making it suitable for changing data structures.</li> <li>Compact binary format, good for high-throughput data processing.</li> </ul> </li> <li><strong>RecordIO:</strong> <ul> <li>A binary format used in machine learning, particularly with Amazon SageMaker.</li> <li>Optimized for streaming data and efficient I/O operations.</li> <li>Supports both labeled and unlabeled data for training models.</li> </ul> </li> </ul> <p>When choosing a data format, consider factors such as data size, query patterns, processing requirements, and compatibility with your analytics tools. Each format has its strengths and is suited for different use cases in the data pipeline.</p> <p style="color: #0066cc;"><strong>Knowledge 2: How to use the core AWS data sources (for example, Amazon S3, Amazon Elastic File System [Amazon EFS], Amazon FSx for NetApp ONTAP)</strong></p> <p>AWS provides various data storage options, each with unique characteristics. Let's explore the core AWS data sources:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Object storage service designed for scalability, data availability, security, and performance.</li> <li>Ideal for storing large amounts of unstructured data, such as logs, backups, and data lakes.</li> <li>Supports versioning, lifecycle policies, and fine-grained access controls.</li> <li>Example use: Storing raw data for big data analytics or serving static website content.</li> </ul> </li> <li><strong>Amazon Elastic File System (Amazon EFS):</strong> <ul> <li>Fully managed NFS file system for use with AWS Cloud services and on-premises resources.</li> <li>Automatically grows and shrinks as you add and remove files.</li> <li>Supports concurrent access from multiple EC2 instances.</li> <li>Example use: Shared file storage for content management systems or development environments.</li> </ul> </li> <li><strong>Amazon FSx for NetApp ONTAP:</strong> <ul> <li>Fully managed file storage built on NetApp's ONTAP file system.</li> <li>Provides NFS, SMB, and iSCSI protocols for broad application compatibility.</li> <li>Offers advanced data management features like snapshots, replication, and data tiering.</li> <li>Example use: Enterprise applications requiring high-performance shared storage with advanced features.</li> </ul> </li> </ul> <p>When using these data sources, consider the following best practices:</p> <ul> <li>Use S3 for large-scale, cost-effective storage of unstructured data.</li> <li>Leverage EFS for shared file systems that require POSIX compliance and concurrent access.</li> <li>Choose FSx for NetApp ONTAP when you need enterprise-grade features and compatibility with existing NetApp workflows.</li> <li>Implement proper access controls and encryption for all data sources to ensure security.</li> <li>Use appropriate data transfer methods (e.g., AWS DataSync, S3 Transfer Acceleration) for efficient data movement between sources.</li> </ul> <p style="color: #0066cc;"><strong>Knowledge 3: How to use AWS streaming data sources to ingest data (for example, Amazon Kinesis, Apache Flink, Apache Kafka)</strong></p> <p>Streaming data ingestion is crucial for real-time analytics and processing. AWS offers several options for handling streaming data:</p> <ul> <li><strong>Amazon Kinesis:</strong> <ul> <li>A suite of services for real-time streaming data processing.</li> <li>Kinesis Data Streams: For ingesting and storing streaming data.</li> <li>Kinesis Data Firehose: For loading streaming data into data stores and analytics tools.</li> <li>Kinesis Data Analytics: For processing streaming data in real-time using SQL or Apache Flink.</li> <li>Example use: Processing clickstream data for real-time website analytics.</li> </ul> </li> <li><strong>Apache Flink on AWS:</strong> <ul> <li>Open-source stream processing framework that can be deployed on AWS.</li> <li>Supports both batch and stream processing with low latency.</li> <li>Can be used with Amazon Kinesis Data Analytics for Java applications.</li> <li>Example use: Complex event processing for IoT sensor data.</li> </ul> </li> <li><strong>Apache Kafka on AWS:</strong> <ul> <li>Can be deployed on EC2 instances or used as a managed service with Amazon MSK (Managed Streaming for Apache Kafka).</li> <li>Provides high-throughput, fault-tolerant, publish-subscribe messaging system.</li> <li>Ideal for building real-time data pipelines and streaming applications.</li> <li>Example use: Building a real-time log aggregation system for distributed applications.</li> </ul> </li> </ul> <p>Best practices for using streaming data sources:</p> <ul> <li>Choose the right service based on your data volume, processing requirements, and latency needs.</li> <li>Implement proper error handling and retry mechanisms to deal with data inconsistencies.</li> <li>Use appropriate scaling strategies to handle varying data ingestion rates.</li> <li>Implement data transformation and enrichment as close to the ingestion point as possible.</li> <li>Monitor your streaming pipelines for performance and data quality issues.</li> </ul> <p style="color: #0066cc;"><strong>Knowledge 4: AWS storage options, including use cases and tradeoffs</strong></p> <p>AWS offers a variety of storage options to cater to different use cases. Understanding their characteristics and tradeoffs is essential for optimal data management:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Use case: Object storage for large-scale, unstructured data.</li> <li>Pros: Highly scalable, durable, and cost-effective.</li> <li>Cons: Not suitable for file system operations or transactional databases.</li> </ul> </li> <li><strong>Amazon EBS (Elastic Block Store):</strong> <ul> <li>Use case: Block-level storage volumes for EC2 instances.</li> <li>Pros: Low-latency, persistent storage for databases and file systems.</li> <li>Cons: Limited to a single Availability Zone, more expensive than S3 for large datasets.</li> </ul> </li> <li><strong>Amazon EFS (Elastic File System):</strong> <ul> <li>Use case: Shared file storage for Linux-based workloads.</li> <li>Pros: Scalable, supports concurrent access from multiple EC2 instances.</li> <li>Cons: Higher latency compared to EBS, not suitable for Windows workloads.</li> </ul> </li> <li><strong>Amazon FSx:</strong> <ul> <li>Use case: Fully managed file systems for Windows (FSx for Windows File Server) and high-performance computing (FSx for Lustre).</li> <li>Pros: Native compatibility with specific workloads, high performance.</li> <li>Cons: More expensive than general-purpose storage options.</li> </ul> </li> <li><strong>Amazon DynamoDB:</strong> <ul> <li>Use case: NoSQL database for applications requiring low-latency data access.</li> <li>Pros: Fully managed, scalable, and highly available.</li> <li>Cons: Limited query flexibility compared to relational databases.</li> </ul> </li> <li><strong>Amazon RDS (Relational Database Service):</strong> <ul> <li>Use case: Managed relational databases for structured data.</li> <li>Pros: Easy to set up, manage, and scale relational databases.</li> <li>Cons: Less control over the underlying infrastructure compared to self-managed databases.</li> </ul> </li> </ul> <p>When choosing a storage option, consider factors such as:</p> <ul> <li>Data structure and access patterns</li> <li>Performance requirements (IOPS, throughput)</li> <li>Scalability needs</li> <li>Cost considerations</li> <li>Data durability and availability requirements</li> <li>Integration with other AWS services and existing applications</li> </ul> <p>By carefully evaluating these factors and understanding the tradeoffs between different storage options, you can design an efficient and cost-effective storage architecture for your AWS-based applications and data processing pipelines.</p>

			<p style="color: #0066cc;"><strong>Skill 1: Extracting data from storage (for example, Amazon S3, Amazon Elastic Block Store [Amazon EBS], Amazon EFS, Amazon RDS, Amazon DynamoDB) by using relevant AWS service options (for example, Amazon S3 Transfer Acceleration, Amazon EBS Provisioned IOPS)</strong></p> <p>This skill involves the ability to efficiently retrieve data from various AWS storage services using appropriate methods and optimizations. Here's a breakdown of the key components:</p> <ul> <li><strong>Amazon S3 (Simple Storage Service):</strong> <ul> <li>Use AWS SDK or CLI to download objects</li> <li>Implement S3 Transfer Acceleration for faster transfers over long distances</li> <li>Utilize S3 Select for retrieving specific data subsets</li> </ul> </li> <li><strong>Amazon EBS (Elastic Block Store):</strong> <ul> <li>Attach EBS volumes to EC2 instances for direct access</li> <li>Use EBS Provisioned IOPS for high-performance, low-latency workloads</li> <li>Create EBS snapshots for backup and data transfer</li> </ul> </li> <li><strong>Amazon EFS (Elastic File System):</strong> <ul> <li>Mount EFS file systems to multiple EC2 instances</li> <li>Use EFS-to-EFS backup for data protection and migration</li> </ul> </li> <li><strong>Amazon RDS (Relational Database Service):</strong> <ul> <li>Use SQL queries to extract data from RDS databases</li> <li>Implement read replicas for improved performance on read-heavy workloads</li> </ul> </li> <li><strong>Amazon DynamoDB:</strong> <ul> <li>Use Query and Scan operations to retrieve data</li> <li>Implement DynamoDB Accelerator (DAX) for faster read performance</li> </ul> </li> </ul> <p>Example: To extract data from an S3 bucket using Python and the AWS SDK (boto3):</p> <p><code> import boto3<br> s3 = boto3.client('s3')<br> response = s3.get_object(Bucket='my-bucket', Key='my-object')<br> data = response['Body'].read() </code></p> <p style="color: #0066cc;"><strong>Skill 2: Choosing appropriate data formats (for example, Parquet, JSON, CSV, ORC) based on data access patterns</strong></p> <p>This skill requires understanding different data formats and their characteristics to select the most suitable one for specific use cases. Key considerations include:</p> <ul> <li><strong>Parquet:</strong> <ul> <li>Columnar storage format</li> <li>Efficient for analytical queries and large datasets</li> <li>Good for data with many columns but only a subset is typically accessed</li> </ul> </li> <li><strong>JSON (JavaScript Object Notation):</strong> <ul> <li>Human-readable and easy to parse</li> <li>Flexible schema, good for nested and hierarchical data</li> <li>Suitable for web applications and APIs</li> </ul> </li> <li><strong>CSV (Comma-Separated Values):</strong> <ul> <li>Simple, tabular format</li> <li>Easy to read and write</li> <li>Good for flat data structures and compatibility with spreadsheet applications</li> </ul> </li> <li><strong>ORC (Optimized Row Columnar):</strong> <ul> <li>Columnar storage format optimized for Hive</li> <li>Efficient compression and encoding schemes</li> <li>Good for large-scale data processing in Hadoop ecosystems</li> </ul> </li> </ul> <p>Example: If you're working with time-series data that requires frequent aggregations and filtering on specific columns, Parquet might be the best choice due to its columnar nature and efficient compression.</p> <p style="color: #0066cc;"><strong>Skill 3: Ingesting data into Amazon SageMaker Data Wrangler and SageMaker Feature Store</strong></p> <p>This skill involves understanding how to import and prepare data for machine learning workflows using Amazon SageMaker tools:</p> <ul> <li><strong>Amazon SageMaker Data Wrangler:</strong> <ul> <li>Import data from various sources (S3, Athena, Redshift)</li> <li>Perform data transformations and feature engineering</li> <li>Visualize and analyze data distributions</li> <li>Export prepared data to S3 or use directly in SageMaker</li> </ul> </li> <li><strong>SageMaker Feature Store:</strong> <ul> <li>Create feature groups to organize related features</li> <li>Ingest features using the Feature Store API or SageMaker Processing jobs</li> <li>Configure online and offline storage for feature access</li> <li>Set up feature versioning and time travel capabilities</li> </ul> </li> </ul> <p>Example: To ingest data into SageMaker Feature Store using Python:</p> <p><code> from sagemaker.feature_store.feature_group import FeatureGroup<br> from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum<br> <br> feature_group = FeatureGroup(name="my-feature-group", sagemaker_session=session)<br> feature_group.load_feature_definitions([<br> FeatureDefinition(feature_name="feature1", feature_type=FeatureTypeEnum.STRING),<br> FeatureDefinition(feature_name="feature2", feature_type=FeatureTypeEnum.INTEGRAL)<br> ])<br> feature_group.create(s3_uri="s3://my-bucket/my-prefix", record_identifier_name="id")<br> feature_group.ingest(data_frame=my_dataframe) </code></p> <p style="color: #0066cc;"><strong>Skill 4: Merging data from multiple sources (for example, by using programming techniques, AWS Glue, Apache Spark)</strong></p> <p>This skill focuses on combining data from various sources to create a unified dataset. Key aspects include:</p> <ul> <li><strong>Programming techniques:</strong> <ul> <li>Use pandas or other data manipulation libraries for small to medium datasets</li> <li>Implement efficient join algorithms (e.g., hash join, merge join)</li> <li>Handle data type conversions and schema alignment</li> </ul> </li> <li><strong>AWS Glue:</strong> <ul> <li>Create Glue jobs to extract, transform, and load (ETL) data</li> <li>Use Glue Data Catalog to discover and manage metadata</li> <li>Implement Glue DynamicFrames for schema evolution</li> </ul> </li> <li><strong>Apache Spark:</strong> <ul> <li>Utilize Spark SQL for distributed data processing</li> <li>Implement efficient join strategies (e.g., broadcast join, shuffle hash join)</li> <li>Optimize performance using partitioning and caching</li> </ul> </li> </ul> <p>Example: Merging data using pandas in Python:</p> <p><code> import pandas as pd<br> <br> df1 = pd.read_csv('source1.csv')<br> df2 = pd.read_csv('source2.csv')<br> merged_df = pd.merge(df1, df2, on='common_column', how='inner') </code></p> <p style="color: #0066cc;"><strong>Skill 5: Troubleshooting and debugging data ingestion and storage issues that involve capacity and scalability</strong></p> <p>This skill involves identifying and resolving problems related to data ingestion and storage, particularly those affecting system capacity and scalability. Key aspects include:</p> <ul> <li><strong>Capacity issues:</strong> <ul> <li>Monitor storage utilization and implement auto-scaling where possible</li> <li>Use AWS CloudWatch to set up alerts for capacity thresholds</li> <li>Implement data lifecycle policies to manage storage growth</li> </ul> </li> <li><strong>Scalability challenges:</strong> <ul> <li>Identify bottlenecks in data ingestion pipelines</li> <li>Implement parallel processing and distributed systems where appropriate</li> <li>Use AWS services like Kinesis for real-time data ingestion at scale</li> </ul> </li> <li><strong>Debugging techniques:</strong> <ul> <li>Analyze logs and metrics to identify issues</li> <li>Use AWS X-Ray for distributed tracing of applications</li> <li>Implement proper error handling and logging in data pipelines</li> </ul> </li> </ul> <p>Example: Setting up a CloudWatch alarm for S3 bucket size:</p> <p><code> import boto3<br> <br> cloudwatch = boto3.client('cloudwatch')<br> cloudwatch.put_metric_alarm(<br> AlarmName='S3BucketSizeAlarm',<br> ComparisonOperator='GreaterThanThreshold',<br> EvaluationPeriods=1,<br> MetricName='BucketSizeBytes',<br> Namespace='AWS/S3',<br> Period=86400,<br> Statistic='Average',<br> Threshold=1000000000, # 1 GB<br> ActionsEnabled=True,<br> AlarmActions=['arn:aws:sns:region:account-id:topic-name']<br> ) </code></p> <p style="color: #0066cc;"><strong>Skill 6: Making initial storage decisions based on cost, performance, and data structure</strong></p> <p>This skill involves evaluating and selecting appropriate storage solutions based on various factors. Key considerations include:</p> <ul> <li><strong>Cost optimization:</strong> <ul> <li>Compare pricing models of different storage services (e.g., S3 vs. EBS vs. EFS)</li> <li>Implement storage tiering strategies (e.g., S3 Intelligent-Tiering)</li> <li>Consider data transfer costs between services and regions</li> </ul> </li> <li><strong>Performance requirements:</strong> <ul> <li>Evaluate IOPS and throughput needs of the application</li> <li>Consider latency requirements for data access</li> <li>Implement caching solutions where appropriate (e.g., ElastiCache)</li> </ul> </li> <li><strong>Data structure considerations:</strong> <ul> <li>Analyze the nature of data (structured, semi-structured, unstructured)</li> <li>Consider data access patterns (random vs. sequential, read-heavy vs. write-heavy)</li> <li>Evaluate the need for indexing and query capabilities</li> </ul> </li> </ul> <p>Example decision-making process:</p> <ol> <li>For large-scale, infrequently accessed data with no strict performance requirements, consider Amazon S3 Glacier for cost-effective archival storage.</li> <li>For structured data requiring complex queries and transactions, Amazon RDS or Amazon Aurora might be suitable.</li> <li>For high-performance, low-latency block storage needs, consider Amazon EBS with Provisioned IOPS.</li> <li>For shared file systems accessed by multiple EC2 instances, Amazon EFS could be the best choice.</li> </ol> <p>By mastering these skills, you'll be well-prepared to handle various data engineering tasks on AWS, from efficient data extraction and format selection to troubleshooting and making informed storage decisions.</p>

		</div>
	</div>
	
	<br/>
	
</div>


<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
