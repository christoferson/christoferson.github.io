<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>

    
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
    <style>
        details {
            border: 1px solid #aaa;
            border-radius: 2px;
            padding: .5em .5em 0;
            color: indigo;
            font-size: 12px;
        }
    
        summary {
            font-weight: bold;
            margin: -.5em -.5em 0;
            padding: .5em;
            cursor: pointer;
        }
    
        details[open] {
            padding: .5em;
        }
    
        details[open] summary {
            border-bottom: 1px solid #aaa;
            margin-bottom: .5em;
        }
    </style>

</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning Engineer Associate (MLA) - Services SageMaker - Deploy</h1>  
</div>



<div style="color: darkmagenta;font-size: 20px;padding:5px;">Model Deploy</div>
<hr style="height: 12px;background-color:#0066cc"/>





<div class="container mt-5">
	<h3 class="text-primary h4">Deploy models for inference</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">

		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Deploy models for inference with Amazon SageMaker</p> <p style="color: #555; font-size: 14px;">Amazon SageMaker offers a wide range of ML infrastructure and model deployment options to meet various inference needs. SageMaker Inference helps you:</p> <ul> <li>Scale your model deployment</li> <li>Manage models effectively in production</li> <li>Reduce operational burden</li> </ul> <p style="color: #555; font-size: 14px;">SageMaker provides several inference options:</p> <ul> <li>Real-time endpoints for low latency inference</li> <li>Serverless endpoints for fully managed infrastructure and auto-scaling</li> <li>Asynchronous endpoints for batches of requests</li> </ul> <p style="color: #333; font-size: 16px;">Choosing a feature</p> <p style="color: #555; font-size: 14px;">SageMaker offers different features for various use cases:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Use Case</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">1. Low-code or no-code environment</td> <td style="border: 1px solid #ddd; padding: 8px;">Amazon SageMaker JumpStart</td> <td style="border: 1px solid #ddd; padding: 8px;">Deploy pre-trained models through Amazon SageMaker Studio interface</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">2. Code-based deployment with flexibility</td> <td style="border: 1px solid #ddd; padding: 8px;">ModelBuilder class in SageMaker Python SDK</td> <td style="border: 1px solid #ddd; padding: 8px;">Deploy custom models with fine-grained control over settings</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">3. Large-scale deployment</td> <td style="border: 1px solid #ddd; padding: 8px;">AWS SDK for Python (Boto3) and AWS CloudFormation</td> <td style="border: 1px solid #ddd; padding: 8px;">Manage models at scale in production with IaC and CI/CD tools</td> </tr> </table> <p style="color: #333; font-size: 16px;">Feature Comparison</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Low-code</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Code-based deployment</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Large-scale deployment</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Optimized for</strong></td> <td style="border: 1px solid #ddd; padding: 8px;">Fast deployments of popular open source models</td> <td style="border: 1px solid #ddd; padding: 8px;">Deploying your own models</td> <td style="border: 1px solid #ddd; padding: 8px;">Ongoing management of models in production</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Considerations</strong></td> <td style="border: 1px solid #ddd; padding: 8px;">Limited customization for container settings</td> <td style="border: 1px solid #ddd; padding: 8px;">Requires Python coding skills</td> <td style="border: 1px solid #ddd; padding: 8px;">Requires infrastructure management and AWS SDK knowledge</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Recommended environment</strong></td> <td style="border: 1px solid #ddd; padding: 8px;">SageMaker domain</td> <td style="border: 1px solid #ddd; padding: 8px;">Python development environment or SageMaker IDE</td> <td style="border: 1px solid #ddd; padding: 8px;">AWS CLI, local development environment, IaC and CI/CD tools</td> </tr> </table> <p style="color: #333; font-size: 16px;">Additional Deployment Options</p> <p style="color: #555; font-size: 14px;">SageMaker offers various inference options:</p> <ul> <li>Real-time inference: For interactive, low latency requirements</li> <li>Serverless Inference: Deploy models without managing infrastructure</li> <li>Asynchronous inference: For large payloads and long processing times</li> </ul> <p style="color: #333; font-size: 16px;">Cost Optimization</p> <p style="color: #555; font-size: 14px;">To optimize inference costs, consider:</p> <ul> <li>SageMaker Neo: Optimize model performance and efficiency</li> <li>Autoscaling: Dynamically adjust compute resources based on traffic patterns</li> </ul>

            <hr />

            <p style="color: #333; font-size: 16px;">Detailed Comparison of Amazon SageMaker Endpoint Types</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Real-time Inference</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Serverless Inference</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Asynchronous Inference</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Use Case</td> <td style="border: 1px solid #ddd; padding: 8px;">Low-latency, interactive applications</td> <td style="border: 1px solid #ddd; padding: 8px;">Intermittent or unpredictable workloads</td> <td style="border: 1px solid #ddd; padding: 8px;">Large payloads, long processing times</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Latency</td> <td style="border: 1px solid #ddd; padding: 8px;">Lowest (milliseconds)</td> <td style="border: 1px solid #ddd; padding: 8px;">Low (seconds, with potential cold starts)</td> <td style="border: 1px solid #ddd; padding: 8px;">Highest (minutes to hours)</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Scalability</td> <td style="border: 1px solid #ddd; padding: 8px;">Manual or auto-scaling</td> <td style="border: 1px solid #ddd; padding: 8px;">Automatic scaling</td> <td style="border: 1px solid #ddd; padding: 8px;">Automatic scaling</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Cost Model</td> <td style="border: 1px solid #ddd; padding: 8px;">Pay for provisioned instances</td> <td style="border: 1px solid #ddd; padding: 8px;">Pay per request and compute time</td> <td style="border: 1px solid #ddd; padding: 8px;">Pay for processing time</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Max. Payload Size</td> <td style="border: 1px solid #ddd; padding: 8px;">6 MB</td> <td style="border: 1px solid #ddd; padding: 8px;">6 MB</td> <td style="border: 1px solid #ddd; padding: 8px;">1 GB</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Max. Processing Time</td> <td style="border: 1px solid #ddd; padding: 8px;">60 seconds</td> <td style="border: 1px solid #ddd; padding: 8px;">60 seconds</td> <td style="border: 1px solid #ddd; padding: 8px;">1 hour</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Infrastructure Management</td> <td style="border: 1px solid #ddd; padding: 8px;">User managed</td> <td style="border: 1px solid #ddd; padding: 8px;">Fully managed</td> <td style="border: 1px solid #ddd; padding: 8px;">Fully managed</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Cold Starts</td> <td style="border: 1px solid #ddd; padding: 8px;">No</td> <td style="border: 1px solid #ddd; padding: 8px;">Yes (mitigated by provisioned concurrency)</td> <td style="border: 1px solid #ddd; padding: 8px;">No</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Advantages</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Lowest latency</li> <li>Predictable performance</li> <li>Full control over instance types</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>No infrastructure management</li> <li>Automatic scaling</li> <li>Cost-effective for variable workloads</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Handles large payloads</li> <li>Long processing times</li> <li>Cost-effective for batch processing</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Limitations</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Requires capacity planning</li> <li>Potentially higher costs for idle resources</li> <li>Limited payload size and processing time</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Potential cold start latency</li> <li>Limited customization options</li> <li>Max. 6 MB payload and 60 seconds processing</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Not suitable for real-time applications</li> <li>Requires additional setup for result handling</li> <li>Max. 1 hour processing time</li> </ul> </td> </tr> </table> <p style="color: #555; font-size: 14px;">Key takeaways:</p> <ul> <li><strong>Real-time Inference</strong> is best for applications requiring immediate responses and consistent low latency, such as interactive web applications or real-time decision making systems.</li> <li><strong>Serverless Inference</strong> is ideal for applications with variable or unpredictable traffic patterns, offering automatic scaling and cost optimization for sporadic workloads.</li> <li><strong>Asynchronous Inference</strong> excels in scenarios involving large payloads or long processing times, such as batch processing of images or videos, or complex ML models that require extensive computation.</li> </ul> <p style="color: #555; font-size: 14px;">When choosing an endpoint type, consider factors such as latency requirements, payload size, processing time, traffic patterns, and cost optimization needs.
            
            <p style="color: #555; font-size: 14px;">Each type has its strengths and is designed to address specific use cases. Here are some additional considerations when choosing an endpoint type:</p> <ul> <li><strong>Traffic patterns:</strong> If your workload has consistent, predictable traffic, real-time endpoints might be more suitable. For sporadic or unpredictable traffic, serverless inference can be more cost-effective.</li> <li><strong>Model complexity:</strong> Complex models with long inference times might benefit from asynchronous inference, while simpler models with quick inference times are well-suited for real-time or serverless endpoints.</li> <li><strong>Integration requirements:</strong> Consider how the endpoint will integrate with your existing infrastructure and applications. Real-time endpoints offer the most flexibility in terms of integration options.</li> <li><strong>Budget constraints:</strong> Serverless and asynchronous inference can offer cost savings for certain workloads, but it's important to analyze your specific use case to determine the most cost-effective option.</li> </ul> <p style="color: #333; font-size: 16px;">Advanced Features and Optimizations</p> <p style="color: #555; font-size: 14px;">Amazon SageMaker offers several advanced features and optimizations that can be applied across different endpoint types:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Applicable Endpoint Types</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Multi-Model Endpoints</td> <td style="border: 1px solid #ddd; padding: 8px;">Host multiple models on a single endpoint, reducing infrastructure costs and improving resource utilization.</td> <td style="border: 1px solid #ddd; padding: 8px;">Real-time, Serverless</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Auto Scaling</td> <td style="border: 1px solid #ddd; padding: 8px;">Automatically adjust the number of instances to match the workload demand.</td> <td style="border: 1px solid #ddd; padding: 8px;">Real-time</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Elastic Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Attach GPU-powered inference acceleration to a CPU-based instance for cost-effective inference.</td> <td style="border: 1px solid #ddd; padding: 8px;">Real-time</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">SageMaker Neo</td> <td style="border: 1px solid #ddd; padding: 8px;">Optimize machine learning models to run more efficiently on specific hardware platforms.</td> <td style="border: 1px solid #ddd; padding: 8px;">All</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Inference Pipeline</td> <td style="border: 1px solid #ddd; padding: 8px;">Create a pipeline of multiple models or preprocessing logic that are invoked sequentially on a single endpoint.</td> <td style="border: 1px solid #ddd; padding: 8px;">Real-time, Serverless</td> </tr> </table> <p style="color: #333; font-size: 16px;">Monitoring and Management</p> <p style="color: #555; font-size: 14px;">Regardless of the endpoint type chosen, it's crucial to implement proper monitoring and management practices:</p> <ul> <li><strong>Amazon CloudWatch:</strong> Use CloudWatch to monitor endpoint metrics such as invocations, latency, and errors.</li> <li><strong>Model Monitor:</strong> Implement SageMaker Model Monitor to detect concept drift and data quality issues in your deployed models.</li> <li><strong>A/B Testing:</strong> Utilize SageMaker's built-in A/B testing capabilities to compare different model versions or configurations.</li> <li><strong>Endpoint Deployment Guardrails:</strong> Implement deployment strategies like blue/green deployments to minimize risk when updating models in production.</li> </ul> <p style="color: #333; font-size: 16px;">Best Practices</p> <p style="color: #555; font-size: 14px;">To ensure optimal performance and cost-effectiveness of your SageMaker endpoints, consider the following best practices:</p> <ul> <li>Regularly review and optimize your endpoint configurations based on actual usage patterns.</li> <li>Use appropriate instance types that match your model's resource requirements.</li> <li>Implement proper error handling and retry mechanisms in your client applications.</li> <li>Leverage SageMaker's built-in security features, such as encryption at rest and in transit.</li> <li>Implement a robust CI/CD pipeline for model deployment and updates.</li> <li>Regularly update your SageMaker SDK and dependencies to benefit from the latest features and optimizations.</li> </ul> <p style="color: #555; font-size: 14px;">By carefully considering your use case requirements and leveraging SageMaker's various endpoint types and advanced features, you can create an efficient, scalable, and cost-effective inference solution for your machine learning models.</p>

            <hr />

            <p style="color: #333; font-size: 16px;">Deploy a Model in Amazon SageMaker</p> <p style="color: #555; font-size: 14px;">After training your machine learning model, Amazon SageMaker offers various deployment options for getting predictions. The choice depends on your specific use case:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Deployment Option</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Use Case</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">SageMaker real-time hosting services</td> <td style="border: 1px solid #ddd; padding: 8px;">Persistent, real-time endpoints for single predictions</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Serverless Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Workloads with idle periods between traffic spikes, tolerant of cold starts</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Asynchronous Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Large payloads (up to 1GB), long processing times, near real-time latency requirements</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Batch transform</td> <td style="border: 1px solid #ddd; padding: 8px;">Predictions for entire datasets</td> </tr> </table> <p style="color: #555; font-size: 14px;">SageMaker also provides additional features for managing resources and optimizing inference performance:</p> <ul> <li><strong>SageMaker Edge Manager:</strong> Manage models on edge devices (e.g., smart cameras, robots, personal computers, mobile devices)</li> <li><strong>Neo:</strong> Optimize various types of models (Gluon, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX) for inference on different platforms and processors</li> </ul> <p style="color: #555; font-size: 14px;">Key points to remember:</p> <ul> <li>Choose the deployment option that best fits your use case and requirements.</li> <li>Consider factors such as latency, payload size, processing time, and traffic patterns when selecting a deployment method.</li> <li>Leverage SageMaker's additional features for managing and optimizing your deployed models.</li> <li>Explore edge deployment options for scenarios involving edge devices.</li> <li>Use model optimization tools like Neo to improve inference performance across various platforms.</li> </ul> <p style="color: #555; font-size: 14px;">For a comprehensive overview of all deployment options, refer to the "Deploy models for inference" documentation in Amazon SageMaker.</p>

            <hr />

            <p style="color: #333; font-size: 16px;">Steps for Model Deployment</p> <p style="color: #555; font-size: 14px;">The general workflow for inference endpoints consists of the following steps:</p> <ol> <li>Create a model in SageMaker Inference by pointing to model artifacts in Amazon S3 and a container image.</li> <li>Select an inference option.</li> <li>Create a SageMaker Inference endpoint configuration, choosing instance type and number of instances.</li> <li>Create a SageMaker Inference endpoint.</li> <li>Invoke your endpoint to receive an inference as a response.</li> </ol> <p style="color: #555; font-size: 14px;">You can perform these actions using the AWS console, AWS SDKs, SageMaker Python SDK, AWS CloudFormation, or AWS CLI.</p> <p style="color: #555; font-size: 14px;">For batch inference with batch transform, point to your model artifacts and input data, create a batch inference job, and SageMaker will output your inferences to an Amazon S3 location of your choice.</p> <p style="color: #333; font-size: 16px;">Inference Options</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Option</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Use Case</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Key Features</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Real-Time Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Online inferences with low latency or high throughput requirements</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Persistent, fully managed endpoint</li> <li>Payload up to 6 MB</li> <li>Processing time up to 60 seconds</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Serverless Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Intermittent or unpredictable traffic patterns</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>No instance management</li> <li>Pay only for what you use</li> <li>Payload up to 4 MB</li> <li>Processing time up to 60 seconds</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Batch Transform</td> <td style="border: 1px solid #ddd; padding: 8px;">Offline processing of large datasets</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Support for large datasets (GBs)</li> <li>Processing times of days</li> <li>No persistent endpoint</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Asynchronous Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Queue requests with large payloads and long processing times</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Payload up to 1 GB</li> <li>Processing time up to one hour</li> <li>Scale down to 0 when idle</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;">Advanced Endpoint Options for Real-Time Inference</p> <ul> <li><strong>Multi-Model Endpoints (same container):</strong> Host multiple models using the same framework in one container behind one endpoint.</li> <li><strong>Multi-Model Endpoints (different containers):</strong> Host multiple models using different frameworks in separate containers behind one endpoint.</li> <li><strong>Serial Inference Pipelines:</strong> Host models with pre-processing and post-processing logic behind an endpoint for lower latency.</li> </ul> <p style="color: #333; font-size: 16px;">Bring Your Own Model</p> <p style="color: #555; font-size: 14px;">To use an existing Docker container or create a new one for inference in SageMaker:</p> <ul> <li>Adapt your Docker container to work with SageMaker</li> <li>Use your own inference code with hosting services</li> <li>Use your own inference code with batch transform</li> </ul> <p style="color: #555; font-size: 14px;">Refer to the SageMaker documentation for detailed guidance on these options.</p>

            <hr />

            <p style="color: #333; font-size: 16px;">Monitoring</p> <ul> <li>Use <strong>Model Monitor</strong> to track model metrics and set alerts for deviations in model quality.</li> <li>Monitor endpoint health using <strong>Amazon CloudWatch</strong> metrics (e.g., invocation errors, model latency).</li> <li>Refer to "Monitor Amazon SageMaker" documentation for more tools and techniques.</li> </ul> <p style="color: #333; font-size: 16px;">CI/CD for Model Deployment</p> <ul> <li>Utilize <strong>SageMaker MLOps</strong> to automate machine learning workflows and practice CI/CD.</li> <li>Use <strong>MLOps Project Templates</strong> for setup and implementation of SageMaker MLOps projects.</li> <li>SageMaker supports using third-party Git repos for creating CI/CD systems.</li> <li>Use <strong>Model Registry</strong> to manage model versions and automate deployments.</li> </ul> <p style="color: #333; font-size: 16px;">Deployment Guardrails</p> <ul> <li>Control the switch from current to new models in production.</li> <li>Use traffic shifting modes for granular control over the deployment process.</li> <li>Leverage built-in safeguards like auto-rollbacks to catch issues early.</li> </ul> <p style="color: #333; font-size: 16px;">Inferentia</p> <ul> <li>Use <strong>Inf1 instances</strong> for large-scale ML and deep learning applications.</li> <li>Suitable for image/speech recognition, NLP, personalization, forecasting, fraud detection.</li> <li>Provides higher throughput and lower cost per inference than GPU-based instances.</li> <li>Compile models with SageMaker Neo for deployment on Inf1 instances.</li> </ul> <p style="color: #333; font-size: 16px;">Optimize Model Performance</p> <ul> <li>Use SageMaker's built-in algorithms, pre-built models, and prebuilt Docker images.</li> <li>Leverage <strong>SageMaker Neo</strong> to optimize models for deployment on various processors.</li> <li>Supports TensorFlow, Apache MXNet, PyTorch, ONNX, and XGBoost models.</li> </ul> <p style="color: #333; font-size: 16px;">Autoscaling</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Option</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Manual Autoscaling</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Dynamically adjust the number of instances based on workload changes.</li> <li>Set up scaling policies to manage resources during peak and low traffic periods.</li> <li>Refer to "Automatically Scale Amazon SageMaker Models" documentation.</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Serverless Inference</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Suitable for unpredictable traffic patterns.</li> <li>SageMaker manages autoscaling automatically.</li> <li>Scales down during low traffic and up during high traffic.</li> <li>Refer to "Deploy models with Amazon SageMaker Serverless Inference" documentation.</li> </ul> </td> </tr> </table> <p style="color: #555; font-size: 14px;">By leveraging these advanced features and best practices, you can optimize your model deployment, ensure robust monitoring, and implement efficient scaling strategies in Amazon SageMaker.</p>

		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Create a model in Amazon SageMaker with ModelBuilder</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Create a Model in Amazon SageMaker with ModelBuilder</p> <p style="color: #555; font-size: 14px;">ModelBuilder simplifies the process of preparing and deploying models on SageMaker endpoints by performing the following tasks:</p> <ul> <li>Converts ML models from various frameworks into deployable models in one step</li> <li>Performs automatic container selection based on the model framework</li> <li>Handles serialization and deserialization of data</li> <li>Captures dependencies and packages the model automatically</li> <li>Optionally performs local parameter tuning for LLM use cases</li> <li>Supports popular model servers and containers</li> </ul> <p style="color: #333; font-size: 16px;">Building Your Model with ModelBuilder</p> <p style="color: #555; font-size: 14px;">Basic usage of ModelBuilder:</p>
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
from sagemaker.serve.builder.model_builder import ModelBuilder 
from sagemaker.serve.builder.schema_builder import SchemaBuilder 
model_builder = ModelBuilder( model=model, schema_builder=SchemaBuilder(input, output), role_arn="execution-role", ) </pre> 
            <p style="color: #333; font-size: 16px;">Defining Serialization and Deserialization Methods</p> <p style="color: #555; font-size: 14px;">You can customize serialization and deserialization using CustomPayloadTranslator:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
from sagemaker.serve import CustomPayloadTranslator 
class MyRequestTranslator(CustomPayloadTranslator): 
    def serialize_payload_to_bytes(self, payload: object) -> bytes: 
    def deserialize_payload_from_stream(self, stream) -> object: 
class MyResponseTranslator(CustomPayloadTranslator):
    def serialize_payload_to_bytes(self, payload: object) -> bytes: 
    def deserialize_payload_from_stream(self, stream) -> object:</pre> 
                <p style="color: #333; font-size: 16px;">Customizing Model Loading and Request Handling</p> <p style="color: #555; font-size: 14px;">Use InferenceSpec to customize model loading and request handling:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
from sagemaker.serve.spec.inference_spec import InferenceSpec 
class MyInferenceSpec(InferenceSpec): 
    def load(self, model_dir: str): # Implementation here 
    def invoke(self, input, model): # Implementation here 
inf_spec = MyInferenceSpec() 
model_builder = ModelBuilder( inference_spec=inf_spec, schema_builder=SchemaBuilder(X_test, y_pred) ) </pre> 
            <p style="color: #333; font-size: 16px;">Building and Deploying Your Model</p> <p style="color: #555; font-size: 14px;">To build and deploy your model:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
model = model_builder.build() 
predictor = model.deploy( initial_instance_count=1, instance_type="ml.c6i.xlarge" ) </pre>
            <p style="color: #333; font-size: 16px;">Bring Your Own Container (BYOC)</p> <p style="color: #555; font-size: 14px;">To use your own container:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
model_builder = ModelBuilder( model=model, model_server=ModelServer.TORCHSERVE, 
    schema_builder=SchemaBuilder(X_test, y_pred), image_uri="your-image-uri" ) </pre> 
            <p style="color: #333; font-size: 16px;">Using ModelBuilder in Local Mode</p> <p style="color: #555; font-size: 14px;">To deploy your model locally:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
model_builder_local = ModelBuilder( model=model, schema_builder=SchemaBuilder(X_test, y_pred), 
    role_arn=execution-role, mode=Mode.LOCAL_CONTAINER ) 
xgb_local_builder = model_builder_local.build() 
predictor_local = xgb_local_builder.deploy() </pre> 
            <p style="color: #333; font-size: 16px;">Troubleshooting Local Mode</p> <p style="color: #555; font-size: 14px;">Common issues and solutions:</p> <ul> <li><strong>Address already in use:</strong> Identify and clean up the process using the port</li> <li><strong>IAM Permission Issue:</strong> Verify the execution role has necessary permissions</li> <li><strong>EBS volume capacity issue:</strong> Move Docker volume to a filesystem with enough space</li> </ul> <p style="color: #555; font-size: 14px;">For detailed steps on moving Docker directory and cleaning up space, refer to the provided instructions in the original documentation.</p>
            
		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Optimize model inference with Amazon SageMaker</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Optimize Model Inference with Amazon SageMaker</p> <p style="color: #555; font-size: 14px;">Amazon SageMaker offers various techniques to improve the performance of generative AI models, helping you achieve better cost-performance for your use case. These optimization techniques include:</p> <ul> <li>Quantization</li> <li>Speculative decoding</li> <li>Compilation</li> </ul> <p style="color: #555; font-size: 14px;">After optimization, you can evaluate the model's performance metrics for latency, throughput, and price. SageMaker also provides pre-optimized versions of many models, each catering to different application needs for latency and throughput.</p> <p style="color: #333; font-size: 16px;">Optimization Techniques</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Technique</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Speculative Decoding</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Speeds up decoding process of large LLMs</li> <li>Optimizes for latency without compromising text quality</li> <li>Uses a smaller, faster "draft model" to generate candidate tokens</li> <li>Larger "target model" verifies and occasionally regenerates tokens</li> <li>SageMaker offers pre-built draft models or supports custom ones</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Quantization</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Reduces hardware requirements by using less precise data types</li> <li>Allows hosting on less expensive and more available GPUs</li> <li>May result in slightly reduced model accuracy</li> <li>SageMaker supports Activation-aware Weight Quantization (AWQ) for GPUs</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Compilation</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Optimizes model for best performance on chosen hardware without accuracy loss</li> <li>Applicable for accelerated hardware like AWS Trainium or AWS Inferentia</li> <li>Provides ahead-of-time compilation benefits</li> <li>Reduces deployment time and auto-scaling latency</li> </ul> </td> </tr> </table> <p style="color: #555; font-size: 14px;">Key benefits of these optimization techniques:</p> <ul> <li>Improved model performance</li> <li>Better cost-efficiency</li> <li>Reduced hardware requirements</li> <li>Faster deployment and scaling</li> </ul> <p style="color: #555; font-size: 14px;">When optimizing your models with SageMaker:</p> <ol> <li>Choose the appropriate optimization technique(s) based on your use case and requirements</li> <li>Apply the selected optimization(s) to your model</li> <li>Evaluate the optimized model's performance metrics</li> <li>Compare with pre-optimized versions provided by SageMaker, if available</li> <li>Deploy the optimized model or choose a pre-optimized version that best suits your needs</li> </ol> <p style="color: #555; font-size: 14px;">By leveraging these optimization techniques, you can significantly improve the performance and cost-effectiveness of your generative AI models in Amazon SageMaker.</p>
            <hr />
            <p style="color: #333; font-size: 16px;">Evaluation Metrics in Amazon SageMaker</p> <p style="color: #555; font-size: 14px;">When evaluating optimized models in Amazon SageMaker, the following metrics are provided:</p> <p style="color: #333; font-size: 16px;">Latency Metrics</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Metric</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Concurrency</td> <td style="border: 1px solid #ddd; padding: 8px;">Number of simulated concurrent users invoking the endpoint</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Time to first token (ms)</td> <td style="border: 1px solid #ddd; padding: 8px;">Time between request sent and first token of streaming response received</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Inter-token latency (ms)</td> <td style="border: 1px solid #ddd; padding: 8px;">Time to generate an output token for each request</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Client latency (ms)</td> <td style="border: 1px solid #ddd; padding: 8px;">Request latency from send to full response received</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Input tokens/sec (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total input tokens generated divided by total duration for the concurrency</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Output tokens/sec (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total output tokens generated divided by total duration for the concurrency</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Client invocations (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total inference requests sent to the endpoint across all users</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Client invocation errors (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total inference requests resulting in an invocation error</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Tokenizer failed (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total requests where tokenizer failed to parse request or response</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Empty inference response (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total requests resulting in zero output tokens or tokenizer parsing failure</td> </tr> </table> <p style="color: #333; font-size: 16px;">Throughput Metrics</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Metric</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Concurrency</td> <td style="border: 1px solid #ddd; padding: 8px;">Number of simulated concurrent users invoking the endpoint</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Input tokens/sec/req (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total input tokens generated per second per request</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Output tokens/sec/req (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total output tokens generated per second per request</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Input tokens (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total input tokens generated per request</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Output tokens (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total output tokens generated per request</td> </tr> </table> <p style="color: #333; font-size: 16px;">Price Metrics</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Metric</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Concurrency</td> <td style="border: 1px solid #ddd; padding: 8px;">Number of simulated concurrent users invoking the endpoint</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Price per million input tokens</td> <td style="border: 1px solid #ddd; padding: 8px;">Cost of processing 1M input tokens</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Price per million output tokens</td> <td style="border: 1px solid #ddd; padding: 8px;">Cost of generating 1M output tokens</td> </tr> </table> <p style="color: #555; font-size: 14px;">These metrics provide a comprehensive view of your model's performance, allowing you to assess latency, throughput, and cost-effectiveness. Use these metrics to compare different optimization techniques and choose the best configuration for your specific use case.</p>

		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Validate a Machine Learning Model</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Validating a Machine Learning Model is a crucial step after training to determine its performance and accuracy. This process helps you assess whether the model can achieve your business goals.</p> <p style="color: #333; font-size: 16px;">There are two main approaches to evaluate your model:</p> <ul> <li style="color: #333; font-size: 16px;">Offline testing: Using historical data to send inference requests to the model</li> <li style="color: #333; font-size: 16px;">Online testing: Using live data to test the model in production</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Offline Testing:</span></p> <ul> <li style="color: #333; font-size: 16px;">Deploy the trained model to an alpha endpoint</li> <li style="color: #333; font-size: 16px;">Use historical data for inference requests</li> <li style="color: #333; font-size: 16px;">Utilize a Jupyter notebook in Amazon SageMaker notebook instance</li> <li style="color: #333; font-size: 16px;">Use AWS SDK for Python (Boto) or SageMaker's high-level Python library</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Online Testing:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use production variants for A/B testing</li> <li style="color: #333; font-size: 16px;">Deploy models with the same inference code on the same SageMaker endpoint</li> <li style="color: #333; font-size: 16px;">Configure variants to receive a portion of live traffic (e.g., 10%)</li> <li style="color: #333; font-size: 16px;">Evaluate performance and adjust traffic distribution accordingly</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Offline Model Evaluation Options:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Method</strong></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Validating using a holdout set</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Set aside 20-30% of training data as a holdout set</li> <li>Evaluate model performance on unseen data</li> <li>Assess model's ability to generalize</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">k-fold validation</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Split dataset into k parts (typically 5-10)</li> <li>Treat each part as a holdout set for k training runs</li> <li>Use remaining k-1 parts as training set for each run</li> <li>Produce k models and aggregate results</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;">When evaluating your model, consider the following factors:</p> <ul> <li style="color: #333; font-size: 16px;">Apply different business rules to each model</li> <li style="color: #333; font-size: 16px;">Use various measures to determine model suitability</li> <li style="color: #333; font-size: 16px;">Consider whether the model needs to be more sensitive or specific</li> <li style="color: #333; font-size: 16px;">Assess the model's ability to generalize from initial training</li> </ul> <p style="color: #333; font-size: 16px;">For further information on model evaluation, consult additional resources such as articles and books on the topic, like "Evaluating Machine Learning Models."</p>
            
            <hr />

            <p style="color: #333; font-size: 16px;">Amazon SageMaker Inference Recommender is a capability of Amazon SageMaker that streamlines the process of deploying machine learning (ML) models to production. It automates load testing and model tuning across SageMaker ML instances, helping you find the optimal deployment configuration for your models.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">Reduces time to get ML models into production</li> <li style="color: #333; font-size: 16px;">Helps deploy models to real-time or serverless inference endpoints</li> <li style="color: #333; font-size: 16px;">Optimizes for best performance at the lowest cost</li> <li style="color: #333; font-size: 16px;">Considers multiple factors for optimal configuration</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Factors Considered:</span></p> <ul> <li style="color: #333; font-size: 16px;">Instance type</li> <li style="color: #333; font-size: 16px;">Instance count</li> <li style="color: #333; font-size: 16px;">Container parameters</li> <li style="color: #333; font-size: 16px;">Model optimizations</li> <li style="color: #333; font-size: 16px;">Max concurrency</li> <li style="color: #333; font-size: 16px;">Memory size</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Pricing:</span></p> <p style="color: #333; font-size: 16px;">Amazon SageMaker Inference Recommender only charges for the instances used during job execution, making it a cost-effective solution for optimizing your ML deployments.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How It Works:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Step</strong></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">1. Model Preparation</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Create a SageMaker model, or</li> <li>Register a model to the SageMaker model registry with your model artifacts</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">2. Run Benchmarking Jobs</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Use AWS SDK for Python (Boto3) or SageMaker console</li> <li>Test different SageMaker endpoint configurations</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">3. Analyze Results</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Collect and visualize metrics</li> <li>Evaluate performance and resource utilization</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">4. Make Decisions</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Choose the optimal endpoint type and configuration</li> <li>Balance performance and cost considerations</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Benefits:</span></p> <ul> <li style="color: #333; font-size: 16px;">Automates the process of finding the best deployment configuration</li> <li style="color: #333; font-size: 16px;">Saves time and resources in optimizing ML model deployments</li> <li style="color: #333; font-size: 16px;">Provides data-driven insights for decision-making</li> <li style="color: #333; font-size: 16px;">Helps achieve optimal performance while minimizing costs</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging Amazon SageMaker Inference Recommender, you can streamline your ML deployment process, ensure optimal performance, and manage costs effectively. This tool is particularly valuable for teams looking to scale their ML operations and maximize the value of their models in production environments.</p>

            <hr />

            <p style="color: #333; font-size: 16px;">Real-time inference in Amazon SageMaker is ideal for workloads that require interactive, low-latency responses. It allows you to deploy your model to SageMaker hosting services and obtain an endpoint for inference.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">Fully managed endpoints</li> <li style="color: #333; font-size: 16px;">Support for autoscaling</li> <li style="color: #333; font-size: 16px;">Interactive or programmatic deployment options</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deployment Options:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Method</strong></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">SageMaker Studio</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Interactive deployment through the SageMaker Studio interface</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">AWS SDKs</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Programmatic deployment using SageMaker Python SDK or SDK for Python (Boto3)</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">AWS CLI</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Deployment using the AWS Command Line Interface</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Prerequisites:</span></p> <ul> <li style="color: #333; font-size: 16px;">AWS Region of your Amazon S3 bucket</li> <li style="color: #333; font-size: 16px;">Amazon S3 URI path for model artifacts</li> <li style="color: #333; font-size: 16px;">IAM role for SageMaker</li> <li style="color: #333; font-size: 16px;">Docker Amazon ECR URI registry path for custom inference code or built-in Docker image details</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Shared Resource Utilization:</span></p> <p style="color: #333; font-size: 16px;">SageMaker allows deploying multiple models to a single endpoint, enabling shared resource utilization. This is best achieved using inference components.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Inference Components:</span></p> <p style="color: #333; font-size: 16px;">An inference component is a SageMaker hosting object used to deploy a model to an endpoint. It specifies the model, endpoint, and resource utilization settings.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Aspects of Inference Components:</span></p> <ul> <li style="color: #333; font-size: 16px;">Flexible specification of model (SageMaker Model object or direct artifact and image specification)</li> <li style="color: #333; font-size: 16px;">Customizable resource allocation (CPU cores, accelerators, memory)</li> <li style="color: #333; font-size: 16px;">Multiple components can be deployed to a single endpoint</li> <li style="color: #333; font-size: 16px;">Direct model invocation using InvokeEndpoint API action</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Benefits of Inference Components:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Benefit</strong></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Flexibility</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Decouples model hosting details from the endpoint</li> <li>Allows hosting multiple models on the same infrastructure</li> <li>Enables adding or removing models from an endpoint as needed</li> <li>Supports independent model updates</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Scalability</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Specify number of copies for each model</li> <li>Set minimum number of copies to ensure availability</li> <li>Scale any inference component copy down to zero</li> <li>Allows for efficient resource allocation</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deployment Methods for Inference Components:</span></p> <ul> <li style="color: #333; font-size: 16px;">SageMaker Studio Classic</li> <li style="color: #333; font-size: 16px;">SageMaker Python SDK (using EndpointType.INFERENCE_COMPONENT_BASED)</li> <li style="color: #333; font-size: 16px;">AWS SDK for Python (Boto3) to define and deploy InferenceComponent objects</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging real-time inference and inference components in Amazon SageMaker, you can create flexible, scalable, and efficient deployment solutions for your machine learning models, optimizing resource utilization and improving overall performance.</p>

            <hr />
            <p style="color: #333; font-size: 16px;">Deploying models with Python SDKs in Amazon SageMaker offers flexibility and programmatic control over the deployment process. This guide covers two main approaches: using the SageMaker Python SDK and using the AWS SDK for Python (Boto3).</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">SageMaker Python SDK Approach:</span></p> <p style="color: #333; font-size: 16px;">There are two ways to build your model using the SageMaker Python SDK:</p> <ol> <li style="color: #333; font-size: 16px;">Using the Model class</li> <li style="color: #333; font-size: 16px;">Using the ModelBuilder class</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Setup:</span></p> <p style="color: #333; font-size: 16px;">First, import the necessary libraries and define the S3 URL for your model artifacts:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;"> 
import boto3 
from datetime import datetime 
from sagemaker.compute_resource_requirements.resource_requirements 
import ResourceRequirements 
from sagemaker.predictor import Predictor 
from sagemaker.enums import EndpointType 
from sagemaker.model import Model 
from sagemaker.session import Session

#Define S3 URL for model artifacts
s3_bucket = "amzn-s3-demo-bucket" 
bucket_prefix = "sagemaker/model/path" 
model_filename = "my-model-artifact.tar.gz" 
model_s3_key = f"{bucket_prefix}/{model_filename}" 
model_url = f"s3://{s3_bucket}/{model_s3_key}" </pre> 
            </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Configuration:</span></p> <p style="color: #333; font-size: 16px;">Configure the resources required for your model deployment:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                    <pre style="margin: 0; white-space: pre-wrap;"> 
resources = ResourceRequirements( requests = { "num_cpus": 2, 
    "num_accelerators": 1, "memory": 8192, "copies": 1, }, limits = {}, )
now = datetime.now() 
dt_string = now.strftime("%d-%m-%Y-%H-%M-%S") 
model_name = "my-sm-model"+dt_string

Using Model class
model = Model( name = "model-name", image_uri = "image-uri", 
    model_data = model_url, role = "arn:aws:iam::111122223333:role/service-role/role-name", 
    resources = resources, predictor_cls = Predictor, )

Alternatively, using ModelBuilder (commented out)
model_builder = ModelBuilder(
model="<HuggingFace-ID>",
schema_builder=SchemaBuilder(sample_input,sample_output),
env_vars={ "HUGGING_FACE_HUB_TOKEN": "<HuggingFace_token>}" }
)
model = model_builder.build()
model.model_name = unique_name_from_base("mb-inference-component")
model.resources = resources
                </pre>
                </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deployment:</span></p> <p style="color: #333; font-size: 16px;">Deploy the model to an endpoint:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                    <pre style="margin: 0; white-space: pre-wrap;">
predictor = model.deploy( initial_instance_count = 1, instance_type = "ml.p4d.24xlarge", 
    endpoint_type = EndpointType.INFERENCE_COMPONENT_BASED, resources = resources, ) </pre> 
                    </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deploying Multiple Models to the Same Endpoint:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                        <pre style="margin: 0; white-space: pre-wrap;"> 
falcon_predictor = falcon_model.deploy( initial_instance_count = 1, 
    instance_type = "ml.p4d.24xlarge", 
    endpoint_type = EndpointType.INFERENCE_COMPONENT_BASED, 
    endpoint_name = "<endpoint_name>" resources = resources, )
llama2_predictor = llama2_model.deploy( endpoint_type = EndpointType.INFERENCE_COMPONENT_BASED, 
    endpoint_name = "<endpoint_name>" # same endpoint name as for falcon model ) </pre> 
                    </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">The Model class requires specifying the model package or inference code, serialization/deserialization scripts, and dependencies.</li> <li style="color: #333; font-size: 16px;">ModelBuilder automatically captures dependencies, infers serialization/deserialization functions, and packages dependencies.</li> <li style="color: #333; font-size: 16px;">ResourceRequirements object defines CPU cores, accelerators, and memory allocation for the model.</li> <li style="color: #333; font-size: 16px;">The deploy method is used to create a real-time, HTTPS endpoint for the model.</li> <li style="color: #333; font-size: 16px;">Multiple models can be deployed to the same endpoint by specifying the same endpoint_name.</li> </ul> <p style="color: #333; font-size: 16px;">This approach provides a streamlined way to deploy models in SageMaker, offering flexibility in resource allocation and the ability to deploy multiple models to a single endpoint for efficient resource utilization.</p>


                <hr />

                <p style="color: #333; font-size: 16px;">After deploying your model to an Amazon SageMaker endpoint, you can invoke it using the AWS SDK for Python (Boto3). This guide covers the process of setting up the client and using the invoke_endpoint method for invoking the endpoint.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Setting Up the Client and Invoking the Endpoint:</span></p> <p style="color: #333; font-size: 16px;">The following code demonstrates how to set up the SageMaker Runtime client, specify your endpoint name, and invoke the endpoint:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                    <pre style="margin: 0; white-space: pre-wrap;"> 
import boto3

#Create a low-level client representing Amazon SageMaker Runtime
sagemaker_runtime = boto3.client( "sagemaker-runtime", region_name='aws_region')

#The endpoint name must be unique within an AWS Region in your AWS account.
endpoint_name='endpoint-name'

#Gets inference from the model hosted at the specified endpoint:
response = sagemaker_runtime.invoke_endpoint( EndpointName=endpoint_name, 
    Body=bytes('{"features": ["This is great!"]}', 'utf-8') )

#Decodes and prints the response body:
print(response['Body'].read().decode('utf-8'))

Print the HTTP status code:
print(response["HTTPStatusCode"]) </pre>
                </td>
                    
                    </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">The boto3 client is created for the "sagemaker-runtime" service.</li> <li style="color: #333; font-size: 16px;">The endpoint_name variable should be set to your unique endpoint name.</li> <li style="color: #333; font-size: 16px;">The invoke_endpoint method is used to send an inference request to the model.</li> <li style="color: #333; font-size: 16px;">Input data is provided in the Body field and must match the format used during training.</li> <li style="color: #333; font-size: 16px;">The response includes the inference result, HTTP status, and other metadata.</li> <li style="color: #333; font-size: 16px;">The response body is decoded and printed.</li> <li style="color: #333; font-size: 16px;">The HTTP status code is printed separately to check for successful invocation.</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Ensure that your input data format matches what the model expects.</li> <li style="color: #333; font-size: 16px;">Handle potential errors and exceptions in your invocation code.</li> <li style="color: #333; font-size: 16px;">Monitor the HTTP status codes to ensure successful invocations.</li> <li style="color: #333; font-size: 16px;">The response variable provides access to various fields, including the HTTP status and the name of the deployed model.</li> </ul> <p style="color: #333; font-size: 16px;">By using the invoke_endpoint method with the AWS SDK for Python (Boto3), you can effectively interact with your deployed SageMaker models. This approach allows you to send inference requests and receive responses from your model in a straightforward manner.</p>                

		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Hosting Options</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Multi-model endpoints in Amazon SageMaker provide a scalable and cost-effective solution for deploying large numbers of models. This approach uses shared resources and a single serving container to host multiple models, reducing hosting costs and deployment overhead.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Benefits:</span></p> <ul> <li style="color: #333; font-size: 16px;">Improved endpoint utilization compared to single-model endpoints</li> <li style="color: #333; font-size: 16px;">Reduced deployment overhead</li> <li style="color: #333; font-size: 16px;">SageMaker manages model loading and scaling based on traffic patterns</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Ideal Use Cases:</span></p> <ul> <li style="color: #333; font-size: 16px;">Hosting many models using the same ML framework</li> <li style="color: #333; font-size: 16px;">Mix of frequently and infrequently accessed models</li> <li style="color: #333; font-size: 16px;">Applications tolerant of occasional cold start latency</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Support for CPU and GPU Models:</span></p> <ul> <li style="color: #333; font-size: 16px;">Both CPU and GPU backed models are supported</li> <li style="color: #333; font-size: 16px;">GPU models can lower deployment costs through increased endpoint usage</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Compatible Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">AWS PrivateLink and VPCs</li> <li style="color: #333; font-size: 16px;">Auto scaling</li> <li style="color: #333; font-size: 16px;">Serial inference pipelines (with limitations)</li> <li style="color: #333; font-size: 16px;">A/B testing</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Invoking a Multi-Model Endpoint:</span></p> <p style="color: #333; font-size: 16px;">To invoke a multi-model endpoint, use the <code>invoke_endpoint</code> method from the SageMaker Runtime, adding a <code>TargetModel</code> parameter to specify which model to invoke:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
response = runtime_sagemaker_client.invoke_endpoint( EndpointName = <ENDPOINT_NAME>, 
    ContentType = "text/csv", TargetModel = <MODEL_FILENAME>.tar.gz, Body = body) </pre> 
                            </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Handling Cold Starts:</span></p> <ul> <li style="color: #333; font-size: 16px;">First request to a model may take longer due to model loading (cold start)</li> <li style="color: #333; font-size: 16px;">Subsequent calls are faster once the model is loaded</li> <li style="color: #333; font-size: 16px;">For GPU instances, a 507 HTTP response indicates resource constraints</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Retrying Requests on ModelNotReadyException:</span></p> <p style="color: #333; font-size: 16px;">For large models that take longer than 60 seconds to load, implement a retry strategy:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
import boto3 from botocore.config import Config
config = Config( read_timeout=70, retries={ 'max_attempts': 2 # Can be adjusted up to 5 for 360s max timeout } ) 
runtime_sagemaker_client = boto3.client('sagemaker-runtime', config=config) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use multi-model endpoints for models with similar size and latency requirements</li> <li style="color: #333; font-size: 16px;">Consider dedicated endpoints for models with high TPS or strict latency requirements</li> <li style="color: #333; font-size: 16px;">Implement appropriate retry strategies for large models</li> <li style="color: #333; font-size: 16px;">Monitor and optimize resource utilization across models</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging multi-model endpoints in Amazon SageMaker, you can efficiently manage and serve multiple models, optimizing resource usage and reducing costs while maintaining flexibility in your machine learning deployments.</p>

        <hr />

        <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Multi-Model Endpoint Security</span></p> <p style="color: #333; font-size: 16px;">Security for multi-model endpoints in Amazon SageMaker involves several key aspects:</p> <ul> <li style="color: #333; font-size: 16px;">Models and data are co-located on instance storage volume and container memory</li> <li style="color: #333; font-size: 16px;">Instances run on single-tenant containers owned by you</li> <li style="color: #333; font-size: 16px;">Only your models can run on your multi-model endpoint</li> <li style="color: #333; font-size: 16px;">You are responsible for managing request-to-model mapping and user access to target models</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">IAM-based Access Control:</span></p> <p style="color: #333; font-size: 16px;">SageMaker uses IAM roles to provide identity-based policies. To restrict <code>InvokeEndpoint</code> access to specific models, you can:</p> <ol> <li style="color: #333; font-size: 16px;">Use the <code>sagemaker:TargetModel</code> IAM condition key</li> <li style="color: #333; font-size: 16px;">Create multi-model endpoints with more restrictive S3 prefixes</li> </ol> <p style="color: #333; font-size: 16px;">Example IAM policy using <code>sagemaker:TargetModel</code>:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
            <pre style="margin: 0; white-space: pre-wrap;">
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "sagemaker:InvokeEndpoint"
            ],
            "Effect": "Allow",
            "Resource":
            "arn:aws:sagemaker:region:account-id:endpoint/endpoint_name",
            "Condition": {
                // TargetModel provided must be from this set of values
                "StringLike": {
                    "sagemaker:TargetModel": ["company_a/*", "common/*"]
                }
            }
        }
    ]
}
            </pre> 
        </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">CloudWatch Metrics for Multi-Model Endpoint Deployments</span></p> <p style="color: #333; font-size: 16px;">Amazon SageMaker provides CloudWatch metrics for monitoring multi-model endpoints. These metrics are available for both CPU and GPU backed endpoints.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Metrics for CPU and GPU Backed Endpoints:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Metric</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelLoadingWaitTime</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time waited for model to be downloaded or loaded</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelUnloadingTime</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time taken to unload a model</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelDownloadingTime</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time taken to download a model from S3</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelLoadingTime</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time taken to load a model</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelCacheHit</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Number of requests for which the model was already loaded</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">LoadedModelCount</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Number of models loaded in the containers</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">CPUUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Sum of individual CPU core utilization</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">MemoryUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Percentage of memory used by containers</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">DiskUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Percentage of disk space used by containers</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Additional Metrics for GPU Backed Endpoints:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Metric</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">GPUUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Percentage of GPU units used by containers</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">GPUMemoryUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Percentage of GPU memory used by containers</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Important Notes:</span></p> <ul> <li style="color: #333; font-size: 16px;">Metrics are available at a 1-minute frequency</li> <li style="color: #333; font-size: 16px;">Per-model metrics are not supported</li> <li style="color: #333; font-size: 16px;">For information on metric retention, refer to the Amazon CloudWatch API Reference</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging these security measures and monitoring capabilities, you can ensure the safe and efficient operation of your multi-model endpoints in Amazon SageMaker, while maintaining visibility into their performance and resource utilization.</p>

        <hr />

        <p style="color: #333; font-size: 16px;">Amazon SageMaker multi-model endpoints support automatic scaling, which manages model replicas based on traffic patterns. This guide covers how to set up auto-scaling policies for both CPU and GPU backed multi-model endpoints.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">Configure endpoint and instance size based on SageMaker recommendations</li> <li style="color: #333; font-size: 16px;">Set up instance-based auto-scaling for your endpoint</li> <li style="color: #333; font-size: 16px;">Invocation rates for auto-scale events are based on aggregate predictions across all models</li> <li style="color: #333; font-size: 16px;">Metrics are available at one-minute granularity</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Defining a Scaling Policy:</span></p> <p style="color: #333; font-size: 16px;">Use a target-tracking scaling policy, which can be configured using either predefined or custom metrics. The policy is defined as a JSON block and saved in a text file for use with AWS CLI or Application Auto Scaling API.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">1. Using a Predefined Metric:</span></p> <p style="color: #333; font-size: 16px;">The recommended predefined metric is <code>SageMakerVariantInvocationsPerInstance</code>.</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "TargetValue": 70.0, "PredefinedMetricSpecification": { "PredefinedMetricType": "InvocationsPerInstance" } } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">2. Using a Custom Metric:</span></p> <p style="color: #333; font-size: 16px;">Custom metrics can be defined based on production variant metrics that change proportionally to scaling.</p> <p style="color: #333; font-size: 16px;">Example for CPU backed multi-model endpoint:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "TargetValue": 50, "CustomizedMetricSpecification": { "MetricName": "CPUUtilization", "Namespace": "/aws/sagemaker/Endpoints", "Dimensions": [ {"Name": "EndpointName", "Value": "my-endpoint" }, {"Name": "ModelName","Value": "my-model"} ], "Statistic": "Average", "Unit": "Percent" } } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;">Example for GPU backed multi-model endpoint:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "TargetValue": 50, "CustomizedMetricSpecification": { "MetricName": "GPUUtilization", "Namespace": "/aws/sagemaker/Endpoints", "Dimensions": [ {"Name": "EndpointName", "Value": "my-endpoint" }, {"Name": "ModelName","Value": "my-model"} ], "Statistic": "Average", "Unit": "Percent" } } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Adding Cooldown Periods:</span></p> <p style="color: #333; font-size: 16px;">Cooldown periods can be added for scaling out and scaling in operations:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "TargetValue": 70.0, "PredefinedMetricSpecification": { "PredefinedMetricType": "SageMakerVariantInvocationsPerInstance" }, "ScaleInCooldown": 600, "ScaleOutCooldown": 300 } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices and Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use <code>InvocationsPerInstance</code> for multi-model endpoints</li> <li style="color: #333; font-size: 16px;">The <code>TargetValue</code> should be based on your application's latency requirements</li> <li style="color: #333; font-size: 16px;">Perform load testing to determine suitable scaling parameter values</li> <li style="color: #333; font-size: 16px;">Custom metrics must be valid utilization metrics that describe instance busyness</li> <li style="color: #333; font-size: 16px;">The metric value should decrease when the number of instances increases</li> <li style="color: #333; font-size: 16px;">Always test auto-scaling with custom metrics before deploying to production</li> </ul> <p style="color: #333; font-size: 16px;">By implementing these auto-scaling policies, you can ensure that your multi-model endpoints in Amazon SageMaker efficiently handle varying loads, optimizing resource utilization and maintaining performance under different traffic conditions.</p>


		</div>
	</div>
	
	<br/>
	
</div>








<div class="container mt-5">
	<h3 class="text-primary h4">Host multiple models which use different containers behind one endpoint</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Amazon SageMaker multi-container endpoints allow you to deploy multiple containers, each using different models or frameworks, on a single SageMaker endpoint. This approach offers flexibility in how you invoke these containers, either sequentially as an inference pipeline or individually through direct invocation.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;">Multiple containers can be deployed on a single endpoint</li> <li style="color: #333; font-size: 16px;">Containers can be run sequentially or invoked individually</li> <li style="color: #333; font-size: 16px;">Improves endpoint utilization and optimizes costs</li> <li style="color: #333; font-size: 16px;">Supports up to 15 containers on a multi-container endpoint</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating a Multi-Container Endpoint (Boto 3):</span></p> <p style="color: #333; font-size: 16px;">To create a multi-container endpoint, you need to call the <code>CreateModel</code>, <code>CreateEndpointConfig</code>, and <code>CreateEndpoint</code> APIs. Here's an example of creating a multi-container model for direct invocation:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
# Create container elements and InferenceExecutionConfig with direct invocation 
container1 = { 'Image': '123456789012.dkr.ecr.us-east-1.amazonaws.com/myimage1:mytag', 
    'ContainerHostname': 'firstContainer' }
container2 = { 'Image': '123456789012.dkr.ecr.us-east-1.amazonaws.com/myimage2:mytag', 
    'ContainerHostname': 'secondContainer' }

inferenceExecutionConfig = {'Mode': 'Direct'}

# Create the model
import boto3 sm_client = boto3.Session().client('sagemaker')

response = sm_client.create_model( ModelName = 'my-direct-mode-model-name', 
    InferenceExecutionConfig = inferenceExecutionConfig, 
    ExecutionRoleArn = role, 
    Containers = [container1, container2] ) 
            </pre> 
        </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Requirements:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use the <code>Containers</code> parameter instead of <code>PrimaryContainer</code></li> <li style="color: #333; font-size: 16px;">Include more than one container in the <code>Containers</code> parameter</li> <li style="color: #333; font-size: 16px;">Set the <code>ContainerHostname</code> parameter for each container</li> <li style="color: #333; font-size: 16px;">Set the <code>Mode</code> parameter in <code>InferenceExecutionConfig</code> to either <code>Direct</code> or <code>Serial</code></li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Updating a Multi-Container Endpoint:</span></p> <p style="color: #333; font-size: 16px;">To update a multi-container endpoint, you would typically create a new endpoint configuration and update the existing endpoint to use the new configuration. This process is similar to updating any other SageMaker endpoint.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deleting a Multi-Container Endpoint:</span></p> <p style="color: #333; font-size: 16px;">Deleting a multi-container endpoint follows the same process as deleting any other SageMaker endpoint. You would use the <code>DeleteEndpoint</code> API call.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Using a Multi-Container Endpoint with Direct Invocation:</span></p> <p style="color: #333; font-size: 16px;">When using direct invocation, you can specify which container to invoke by including the <code>TargetContainerHostname</code> parameter in your <code>InvokeEndpoint</code> request. This allows you to target a specific container within your multi-container endpoint.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices and Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Carefully plan your container structure and invocation strategy</li> <li style="color: #333; font-size: 16px;">Ensure that your containers are compatible if using them in a serial inference pipeline</li> <li style="color: #333; font-size: 16px;">Monitor the performance and resource utilization of your multi-container endpoint</li> <li style="color: #333; font-size: 16px;">Consider the trade-offs between using multiple single-container endpoints and a multi-container endpoint</li> <li style="color: #333; font-size: 16px;">Test thoroughly to ensure proper functionality and performance of all containers</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging multi-container endpoints in Amazon SageMaker, you can create more flexible and efficient deployment architectures, potentially reducing costs and improving resource utilization while maintaining the ability to use different models or frameworks within a single endpoint.</p>
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Host models along with pre-processing logic as serial inference pipeline behind one endpoint</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">An inference pipeline in Amazon SageMaker is a model composed of a linear sequence of two to fifteen containers that process requests for inferences on data. This feature allows you to combine preprocessing, predictions, and post-processing tasks in a single, managed pipeline.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;">Combines pretrained SageMaker built-in algorithms and custom algorithms</li> <li style="color: #333; font-size: 16px;">Supports SageMaker Spark ML Serving and scikit-learn containers</li> <li style="color: #333; font-size: 16px;">Handles invocations as a sequence of HTTP requests</li> <li style="color: #333; font-size: 16px;">Containers are co-located on the same EC2 instances for low latency</li> <li style="color: #333; font-size: 16px;">Supports real-time predictions and batch transforms</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating a Pipeline Model:</span></p> <p style="color: #333; font-size: 16px;">To create an inference pipeline, use the <code>CreateModel</code> operation or the SageMaker console. Instead of setting a single <code>PrimaryContainer</code>, use the <code>Containers</code> parameter to define the sequence of containers:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
import boto3
sm_client = boto3.client('sagemaker')

response = sm_client.create_model( ModelName='MyInferencePipeline', 
    ExecutionRoleArn='arn:aws:iam::123456789012:role/SageMakerRole', 
    Containers=[ { 
        'Image': '123456789012.dkr.ecr.us-west-2.amazonaws.com/mypreprocessor:latest', 
        'ModelDataUrl': 's3://mybucket/preprocessor-model.tar.gz' }, { 
        'Image': '123456789012.dkr.ecr.us-west-2.amazonaws.com/myclassifier:latest', 
        'ModelDataUrl': 's3://mybucket/classifier-model.tar.gz' } ] ) 
                </pre> 
        </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Running Real-time Predictions:</span></p> <p style="color: #333; font-size: 16px;">To run real-time predictions with an inference pipeline, deploy the pipeline model to an endpoint and use the <code>InvokeEndpoint</code> API:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> runtime_client = boto3.client('sagemaker-runtime')
                response = runtime_client.invoke_endpoint( EndpointName='MyInferencePipelineEndpoint', ContentType='text/csv', Body='1,2,3,4,5' )
                
                result = response['Body'].read().decode() print(result) </pre> </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Running Batch Transforms:</span></p> <p style="color: #333; font-size: 16px;">To run batch transforms with an inference pipeline, use the <code>CreateTransformJob</code> API:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> response = sm_client.create_transform_job( TransformJobName='MyBatchTransformJob', ModelName='MyInferencePipeline', TransformInput={ 'DataSource': { 'S3DataSource': { 'S3DataType': 'S3Prefix', 'S3Uri': 's3://mybucket/input-data/' } }, 'ContentType': 'text/csv', 'SplitType': 'Line' }, TransformOutput={ 'S3OutputPath': 's3://mybucket/output-data/' }, TransformResources={ 'InstanceType': 'ml.m5.xlarge', 'InstanceCount': 1 } ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Logging and Metrics:</span></p> <p style="color: #333; font-size: 16px;">SageMaker provides logs and metrics for inference pipelines through Amazon CloudWatch. Each container in the pipeline has its own log stream, and you can monitor metrics such as invocations, latency, and errors for the entire pipeline.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Troubleshooting:</span></p> <ul> <li style="color: #333; font-size: 16px;">Ensure all containers in the pipeline are compatible in terms of input/output formats</li> <li style="color: #333; font-size: 16px;">Check individual container logs for specific error messages</li> <li style="color: #333; font-size: 16px;">Verify that the IAM role has necessary permissions for all operations</li> <li style="color: #333; font-size: 16px;">Test each container individually before combining them in a pipeline</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Sample Notebooks:</span></p> <p style="color: #333; font-size: 16px;">SageMaker provides sample notebooks demonstrating inference pipelines:</p> <ul> <li style="color: #333; font-size: 16px;">"Inference Pipeline with Scikit-learn and Linear Learner" in the advanced_functionality folder</li> <li style="color: #333; font-size: 16px;">Additional inference pipeline notebooks in the sagemaker-python-sdk folder</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Design your pipeline to minimize data transfer between containers</li> <li style="color: #333; font-size: 16px;">Consider using SageMaker Spark ML Serving for complex preprocessing tasks</li> <li style="color: #333; font-size: 16px;">Optimize each container for performance and resource utilization</li> <li style="color: #333; font-size: 16px;">Use A/B testing to compare different pipeline configurations</li> <li style="color: #333; font-size: 16px;">Regularly update and test your inference pipeline to ensure optimal performance</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging inference pipelines in Amazon SageMaker, you can create sophisticated, end-to-end machine learning workflows that combine preprocessing, prediction, and postprocessing steps in a single, managed endpoint or batch transform job.</p>
                
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Automatically Scale Amazon SageMaker Models</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Amazon SageMaker supports automatic scaling (auto scaling) for hosted models, dynamically adjusting the number of instances in response to workload changes. This feature helps optimize costs and performance by scaling resources up or down as needed.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;">Auto scaling works with existing SageMaker model endpoints</li> <li style="color: #333; font-size: 16px;">Multiple model versions (production variants) can exist for the same endpoint</li> <li style="color: #333; font-size: 16px;">Scaling can be configured via the SageMaker console, AWS CLI, or AWS SDKs</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Scaling Policy Overview:</span></p> <p style="color: #333; font-size: 16px;">Two types of scaling policies are available:</p> <ol> <li style="color: #333; font-size: 16px;"><strong>Target Tracking Scaling (Recommended):</strong> Automatically adjusts based on a target metric value</li> <li style="color: #333; font-size: 16px;"><strong>Step Scaling:</strong> Provides more advanced configuration options</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating a Target Tracking Scaling Policy:</span></p> <p style="color: #333; font-size: 16px;">Specify the following:</p> <ul> <li style="color: #333; font-size: 16px;">Metric: CloudWatch metric to track (e.g., average invocations per instance)</li> <li style="color: #333; font-size: 16px;">Target value: Desired value for the metric</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Scheduled Scaling:</span></p> <p style="color: #333; font-size: 16px;">Create scheduled actions to perform scaling at specific times, either one-time or recurring.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Scaling Limits:</span></p> <ul> <li style="color: #333; font-size: 16px;">Minimum value: At least 1, and equal to or less than the maximum value</li> <li style="color: #333; font-size: 16px;">Maximum value: Equal to or greater than the minimum value (no upper limit enforced)</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Cooldown Period:</span></p> <p style="color: #333; font-size: 16px;">A cooldown period prevents over-scaling by slowing down subsequent scaling activities. Default is 300 seconds for both scale-in and scale-out activities.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Permissions:</span></p> <p style="color: #333; font-size: 16px;">Auto scaling requires specific IAM permissions. The <code>SagemakerFullAccessPolicy</code> includes all necessary permissions. If managing your own policy, include the following:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "sagemaker:DescribeEndpoint", "sagemaker:DescribeEndpointConfig", "sagemaker:UpdateEndpointWeightsAndCapacities" ], "Resource": "*" }, { "Effect": "Allow", "Action": [ "application-autoscaling:*" ], "Resource": "*" }, { "Effect": "Allow", "Action": "iam:CreateServiceLinkedRole", "Resource": "arn:aws:iam::*:role/aws-service-role/sagemaker.application-autoscaling.amazonaws.com/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint", "Condition": { "StringLike": { "iam:AWSServiceName": "sagemaker.application-autoscaling.amazonaws.com" } } }, { "Effect": "Allow", "Action": [ "cloudwatch:PutMetricAlarm", "cloudwatch:DescribeAlarms", "cloudwatch:DeleteAlarms" ], "Resource": "*" } ] } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Service-Linked Role:</span></p> <p style="color: #333; font-size: 16px;">Auto scaling uses the <code>AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint</code> service-linked role, which is created automatically if you have the <code>iam:CreateServiceLinkedRole</code> permission.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Test your auto scaling configuration with expected traffic patterns</li> <li style="color: #333; font-size: 16px;">Adjust cooldown periods based on your model's traffic patterns</li> <li style="color: #333; font-size: 16px;">Use target tracking scaling policies for most use cases</li> <li style="color: #333; font-size: 16px;">Monitor CloudWatch metrics to ensure optimal scaling behavior</li> <li style="color: #333; font-size: 16px;">Regularly review and update your scaling policies as your workload changes</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">New Inference Capabilities:</span></p> <p style="color: #333; font-size: 16px;">SageMaker has introduced new inference capabilities built on real-time inference endpoints, including inference components. These new features can help reduce deployment costs and latency for foundation models.</p> <p style="color: #333; font-size: 16px;">By leveraging auto scaling in Amazon SageMaker, you can optimize your model deployment for both performance and cost-efficiency, ensuring that your endpoints can handle varying workloads while minimizing unnecessary resource usage.</p>
            
            <pre>
Endpoint:
Type: "AWS::SageMaker::Endpoint"
Properties:
    EndpointName: yourEndpointName
    EndpointConfigName: yourEndpointConfigName

ScalingTarget:
Type: "AWS::ApplicationAutoScaling::ScalableTarget"
Properties:
    MaxCapacity: 10
    MinCapacity: 2
    ResourceId: endpoint/my-endpoint/variant/my-variant
    RoleARN: arn
    ScalableDimension: sagemaker:variant:DesiredInstanceCount
    ServiceNamespace: sagemaker

ScalingPolicy:
Type: "AWS::ApplicationAutoScaling::ScalingPolicy"
Properties:
    PolicyName: my-scaling-policy
    PolicyType: TargetTrackingScaling
    ScalingTargetId:
    Ref: ScalingTarget
    TargetTrackingScalingPolicyConfiguration:
    TargetValue: 70.0
    ScaleInCooldown: 600
    ScaleOutCooldown: 30
    PredefinedMetricSpecification:
        PredefinedMetricType: SageMakerVariantInvocationsPerInstance
            </pre>
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Safely validate models in production</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker provides powerful features for safely validating models in production environments. This is achieved through the use of variants behind endpoints, allowing for comparison and testing of different models or model versions.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;"><strong>Variants:</strong> Consist of an ML instance and serving components specified in a SageMaker model</li> <li style="color: #333; font-size: 16px;"><strong>Production Variants:</strong> Actively serve inference requests</li> <li style="color: #333; font-size: 16px;"><strong>Shadow Variants:</strong> Receive replicated requests but don't return responses to callers</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Production Variants:</span></p> <p style="color: #333; font-size: 16px;">Production variants allow you to compare and test different models, instances, and containers. There are two main methods for testing with production variants:</p> <ol> <li style="color: #333; font-size: 16px;"><strong>Testing by Specifying Traffic Distribution:</strong></li> <ul> <li style="color: #333; font-size: 16px;">Distribute endpoint invocation requests across multiple variants</li> <li style="color: #333; font-size: 16px;">Specify the percentage of traffic for each variant in the endpoint configuration</li> <li style="color: #333; font-size: 16px;">Use the <code>CreateEndpointConfig</code> API to set up traffic distribution</li> </ul> <li style="color: #333; font-size: 16px;"><strong>Testing by Invoking Specific Variants:</strong></li> <ul> <li style="color: #333; font-size: 16px;">Specify the model version to invoke for each request</li> <li style="color: #333; font-size: 16px;">Use the <code>TargetVariant</code> parameter when calling <code>InvokeEndpoint</code></li> <li style="color: #333; font-size: 16px;">Targeted routing overrides random traffic distribution if both are specified</li> </ul> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Model A/B Testing Example:</span></p> <p style="color: #333; font-size: 16px;">A/B testing is an effective final step in validating a new model:</p> <ul> <li style="color: #333; font-size: 16px;">Test different variants of your models with production traffic</li> <li style="color: #333; font-size: 16px;">Compare performance between new and old model versions</li> <li style="color: #333; font-size: 16px;">Replace the old version with the new version if performance improves</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Shadow Variants:</span></p> <p style="color: #333; font-size: 16px;">SageMaker Model Shadow Deployments allow for long-running shadow variants to validate new components of your model serving stack:</p> <ul> <li style="color: #333; font-size: 16px;">Receive replicated requests from production variants</li> <li style="color: #333; font-size: 16px;">Responses are logged for comparison but not returned to callers</li> <li style="color: #333; font-size: 16px;">Allows testing of new models without exposing callers to potentially inferior results</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices for Safe Model Validation:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Gradual Rollout:</strong> Start with a small percentage of traffic to the new variant and gradually increase it</li> <li style="color: #333; font-size: 16px;"><strong>Monitor Key Metrics:</strong> Track performance, latency, and error rates for all variants</li> <li style="color: #333; font-size: 16px;"><strong>Use Shadow Variants:</strong> Test new models without impacting production traffic</li> <li style="color: #333; font-size: 16px;"><strong>Automated Rollback:</strong> Implement automated rollback mechanisms if the new variant underperforms</li> <li style="color: #333; font-size: 16px;"><strong>Comprehensive Logging:</strong> Ensure detailed logging for both production and shadow variants for post-deployment analysis</li> <li style="color: #333; font-size: 16px;"><strong>Statistically Significant Testing:</strong> Ensure your tests run long enough to gather statistically significant results</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Implementation Example:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
import boto3
client = boto3.client('sagemaker')

# Create endpoint configuration with multiple production variants
response = client.create_endpoint_config( 
    EndpointConfigName='MyEndpointConfig', 
    ProductionVariants=[ 
        { 'VariantName': 'VariantA', 'ModelName': 'ModelA', 'InitialInstanceCount': 1, 
            'InstanceType': 'ml.t2.medium', 'InitialVariantWeight': 0.5 }, 
        { 'VariantName': 'VariantB', 'ModelName': 'ModelB', 'InitialInstanceCount': 1, 
            'InstanceType': 'ml.t2.medium', 'InitialVariantWeight': 0.5 } ] )

# Create endpoint
response = client.create_endpoint( EndpointName='MyEndpoint', EndpointConfigName='MyEndpointConfig' )

Invoke endpoint with specific variant
runtime = boto3.client('sagemaker-runtime') 
response = runtime.invoke_endpoint( EndpointName='MyEndpoint', ContentType='text/csv', Body='1,2,3,4,5', TargetVariant='VariantA' ) </pre> 
</td>

</tr> </table> <p style="color: #333; font-size: 16px;">By leveraging these features in Amazon SageMaker, you can safely validate and deploy new models or model versions in production environments, ensuring optimal performance and reliability of your machine learning applications.</p>

            
		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Online Explainability with SageMaker Clarify</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker Clarify provides online explainability for real-time inference endpoints, allowing continuous analysis of model predictions. This feature is part of the "Deploy to production" stage in the SageMaker Machine Learning workflow.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How Clarify Online Explainability Works:</span></p> <ol> <li style="color: #333; font-size: 16px;">Application sends an <code>InvokeEndpoint</code> request to the SageMaker Runtime Service</li> <li style="color: #333; font-size: 16px;">Service routes the request to a SageMaker endpoint</li> <li style="color: #333; font-size: 16px;">Endpoint returns predictions and explanations</li> <li style="color: #333; font-size: 16px;">Service sends the response back to the application</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Steps to Create an Endpoint with Online Explainability:</span></p> <ol> <li style="color: #333; font-size: 16px;">Pre-check model compatibility</li> <li style="color: #333; font-size: 16px;">Create endpoint configuration with Clarify explainer configuration</li> <li style="color: #333; font-size: 16px;">Create endpoint using the configuration</li> <li style="color: #333; font-size: 16px;">Invoke the endpoint for predictions and explanations</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Pre-checking the Model Container:</span></p> <p style="color: #333; font-size: 16px;">Ensure your model container inputs and outputs are compatible with Clarify's requirements:</p> <p style="color: #333; font-size: 16px;"><strong>Model Container Input:</strong></p> <ul> <li style="color: #333; font-size: 16px;">Supports CSV or JSON Lines format</li> <li style="color: #333; font-size: 16px;">CSV inputs should use MIME type: text/csv</li> <li style="color: #333; font-size: 16px;">Can support batch requests for efficiency</li> </ul> <p style="color: #333; font-size: 16px;"><strong>Model Container Output:</strong></p> <ul> <li style="color: #333; font-size: 16px;">Should be in CSV or JSON Lines dense format</li> <li style="color: #333; font-size: 16px;">Must include probabilities of input records</li> <li style="color: #333; font-size: 16px;">For regression and binary classification: single probability value</li> <li style="color: #333; font-size: 16px;">For multi-class problems: list of probabilities</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Model Container Validation:</span></p> <p style="color: #333; font-size: 16px;">Deploy your model to a SageMaker real-time inference endpoint and test it using the AWS CLI:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws sagemaker-runtime invoke-endpoint \ --endpoint-name $ENDPOINT_NAME \ --content-type $CONTENT_TYPE \ --accept $ACCEPT_TYPE \ --body $REQUEST_DATA \ $CLI_BINARY_FORMAT \ /dev/stderr 1>/dev/null </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Example Validations:</span></p> <p style="color: #333; font-size: 16px;"><strong>1. Single record request (CSV):</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws sagemaker-runtime invoke-endpoint \ --endpoint-name test-endpoint-sagemaker-xgboost-model \ --content-type text/csv \ --accept text/csv \ --body '1,2,3,4' \ /dev/stderr 1>/dev/null
Expected output: 0.6
</pre>
</td>

</tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. Multi-record request (CSV):</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws sagemaker-runtime invoke-endpoint \ --endpoint-name test-endpoint-sagemaker-xgboost-model \ --content-type text/csv \ --accept text/csv \ --body $'1,2,3,4\n5,6,7,8' \ /dev/stderr 1>/dev/null
Expected output: 0.6,0.3
</pre>
</td>

</tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. Multi-class model request (CSV):</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws sagemaker-runtime invoke-endpoint \ --endpoint-name test-endpoint-csv-1 \ --content-type text/csv \ --accept text/csv \ --body '1,2,3,4' \ /dev/stderr 1>/dev/null
Expected output: 0.1,0.6,0.3
</pre>
</td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Ensure your model container supports batch requests for efficiency</li> <li style="color: #333; font-size: 16px;">Validate both single record and multi-record requests</li> <li style="color: #333; font-size: 16px;">Test with different input and output formats (CSV and JSON Lines)</li> <li style="color: #333; font-size: 16px;">Verify that probabilities are correctly included in the output</li> <li style="color: #333; font-size: 16px;">Delete testing endpoints after validation is complete</li> </ul> <p style="color: #333; font-size: 16px;">By following these steps and best practices, you can ensure that your model is compatible with SageMaker Clarify's online explainability feature, allowing for real-time insights into your model's predictions in production environments.</p>

<hr />

<p style="color: #333; font-size: 16px;">This guide provides code examples for creating and invoking endpoints that use SageMaker Clarify online explainability, using the AWS SDK for Python. We'll cover examples for both tabular and text data.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Tabular Data Example:</span></p> <p style="color: #333; font-size: 16px;">This example uses a SageMaker model that accepts CSV data with four numerical features.</p> <p style="color: #333; font-size: 16px;"><strong>1. Configure the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_config_name = 'tabular_explainer_endpoint_config' response = sagemaker_client.create_endpoint_config( EndpointConfigName=endpoint_config_name, ProductionVariants=[{ 'VariantName': 'AllTraffic', 'ModelName': model_name, 'InitialInstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', }], ExplainerConfig={ 'ClarifyExplainerConfig': { 'ShapConfig': { 'ShapBaselineConfig': { 'ShapBaseline': '0,0,0,0', }, }, }, }, ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. Create the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_name = 'tabular_explainer_endpoint' response = sagemaker_client.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name, ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. Invoke the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> response = sagemaker_runtime_client.invoke_endpoint( EndpointName=endpoint_name, ContentType='text/csv', Accept='text/csv', Body='1,2,3,4', ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>4. Parse the response:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import codecs import json json.load(codecs.getreader('utf-8')(response['Body'])) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Text Data Example:</span></p> <p style="color: #333; font-size: 16px;">This example uses a SageMaker model that accepts CSV data with a single string feature.</p> <p style="color: #333; font-size: 16px;"><strong>1. Configure the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_config_name = 'text_explainer_endpoint_config' response = sagemaker_client.create_endpoint_config( EndpointConfigName=endpoint_config_name, ProductionVariants=[{ 'VariantName': 'AllTraffic', 'ModelName': model_name, 'InitialInstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', }], ExplainerConfig={ 'ClarifyExplainerConfig': { 'InferenceConfig': { 'FeatureTypes': ['text'], 'MaxRecordCount': 100, }, 'ShapConfig': { 'ShapBaselineConfig': { 'ShapBaseline': '"<MASK>"', }, 'TextConfig': { 'Granularity': 'token', 'Language': 'en', }, 'NumberOfSamples': 100, }, }, }, ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. Create the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_name = 'text_explainer_endpoint' response = sagemaker_client.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name, ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. Invoke the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> response = sagemaker_runtime_client.invoke_endpoint( EndpointName=endpoint_name, ContentType='text/csv', Accept='text/csv', Body='"This is a good product"', ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">For tabular data, set <code>ShapBaseline</code> appropriately (not just zeros as in the example)</li> <li style="color: #333; font-size: 16px;">For text data, use <code>FeatureTypes</code> to identify text features</li> <li style="color: #333; font-size: 16px;">Use <code>TextConfig</code> to specify granularity and language for text analysis</li> <li style="color: #333; font-size: 16px;">Set <code>NumberOfSamples</code> to limit the size of the synthetic dataset</li> <li style="color: #333; font-size: 16px;">Use <code>MaxRecordCount</code> to stabilize performance</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Visualization:</span></p> <p style="color: #333; font-size: 16px;">Use visualization tools to interpret the returned explanations:</p> <ul> <li style="color: #333; font-size: 16px;">For tabular data, SHAP plots can show how each feature contributes to the prediction</li> <li style="color: #333; font-size: 16px;">For text data, tools like the captum visualization utility can highlight word importance</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Always check the endpoint status before invoking it</li> <li style="color: #333; font-size: 16px;">Handle errors and exceptions appropriately</li> <li style="color: #333; font-size: 16px;">For multi-model endpoints, use the <code>TargetModel</code> parameter in the request</li> <li style="color: #333; font-size: 16px;">Use <code>EnableExplanations</code> for on-demand explanations based on prediction thresholds</li> <li style="color: #333; font-size: 16px;">Regularly monitor and analyze the explanations to ensure model fairness and reliability</li> </ul> <p style="color: #333; font-size: 16px;">By following these examples and best practices, you can effectively implement and use SageMaker Clarify's online explainability feature in your machine learning workflows, providing real-time insights into your model's predictions.</p>

    

		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>





<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
