<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>

    
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
    <style>
        details {
            border: 1px solid #aaa;
            border-radius: 2px;
            padding: .5em .5em 0;
            color: indigo;
            font-size: 12px;
        }
    
        summary {
            font-weight: bold;
            margin: -.5em -.5em 0;
            padding: .5em;
            cursor: pointer;
        }
    
        details[open] {
            padding: .5em;
        }
    
        details[open] summary {
            border-bottom: 1px solid #aaa;
            margin-bottom: .5em;
        }
    </style>

</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning Engineer Associate (MLA) - Services SageMaker - Deploy</h1>  
</div>



<div style="color: darkmagenta;font-size: 20px;padding:5px;">Model Deploy</div>
<hr style="height: 12px;background-color:#0066cc"/>





<div class="container mt-5">
	<h3 class="text-primary h4">Deploy models for inference</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">

		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Deploy models for inference with Amazon SageMaker</p> <p style="color: #555; font-size: 14px;">Amazon SageMaker offers a wide range of ML infrastructure and model deployment options to meet various inference needs. SageMaker Inference helps you:</p> <ul> <li>Scale your model deployment</li> <li>Manage models effectively in production</li> <li>Reduce operational burden</li> </ul> <p style="color: #555; font-size: 14px;">SageMaker provides several inference options:</p> <ul> <li>Real-time endpoints for low latency inference</li> <li>Serverless endpoints for fully managed infrastructure and auto-scaling</li> <li>Asynchronous endpoints for batches of requests</li> </ul> <p style="color: #333; font-size: 16px;">Choosing a feature</p> <p style="color: #555; font-size: 14px;">SageMaker offers different features for various use cases:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Use Case</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">1. Low-code or no-code environment</td> <td style="border: 1px solid #ddd; padding: 8px;">Amazon SageMaker JumpStart</td> <td style="border: 1px solid #ddd; padding: 8px;">Deploy pre-trained models through Amazon SageMaker Studio interface</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">2. Code-based deployment with flexibility</td> <td style="border: 1px solid #ddd; padding: 8px;">ModelBuilder class in SageMaker Python SDK</td> <td style="border: 1px solid #ddd; padding: 8px;">Deploy custom models with fine-grained control over settings</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">3. Large-scale deployment</td> <td style="border: 1px solid #ddd; padding: 8px;">AWS SDK for Python (Boto3) and AWS CloudFormation</td> <td style="border: 1px solid #ddd; padding: 8px;">Manage models at scale in production with IaC and CI/CD tools</td> </tr> </table> <p style="color: #333; font-size: 16px;">Feature Comparison</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Low-code</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Code-based deployment</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Large-scale deployment</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Optimized for</strong></td> <td style="border: 1px solid #ddd; padding: 8px;">Fast deployments of popular open source models</td> <td style="border: 1px solid #ddd; padding: 8px;">Deploying your own models</td> <td style="border: 1px solid #ddd; padding: 8px;">Ongoing management of models in production</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Considerations</strong></td> <td style="border: 1px solid #ddd; padding: 8px;">Limited customization for container settings</td> <td style="border: 1px solid #ddd; padding: 8px;">Requires Python coding skills</td> <td style="border: 1px solid #ddd; padding: 8px;">Requires infrastructure management and AWS SDK knowledge</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Recommended environment</strong></td> <td style="border: 1px solid #ddd; padding: 8px;">SageMaker domain</td> <td style="border: 1px solid #ddd; padding: 8px;">Python development environment or SageMaker IDE</td> <td style="border: 1px solid #ddd; padding: 8px;">AWS CLI, local development environment, IaC and CI/CD tools</td> </tr> </table> <p style="color: #333; font-size: 16px;">Additional Deployment Options</p> <p style="color: #555; font-size: 14px;">SageMaker offers various inference options:</p> <ul> <li>Real-time inference: For interactive, low latency requirements</li> <li>Serverless Inference: Deploy models without managing infrastructure</li> <li>Asynchronous inference: For large payloads and long processing times</li> </ul> <p style="color: #333; font-size: 16px;">Cost Optimization</p> <p style="color: #555; font-size: 14px;">To optimize inference costs, consider:</p> <ul> <li>SageMaker Neo: Optimize model performance and efficiency</li> <li>Autoscaling: Dynamically adjust compute resources based on traffic patterns</li> </ul>

            <hr />

            <p style="color: #333; font-size: 16px;">Detailed Comparison of Amazon SageMaker Endpoint Types</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Real-time Inference</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Serverless Inference</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Asynchronous Inference</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Use Case</td> <td style="border: 1px solid #ddd; padding: 8px;">Low-latency, interactive applications</td> <td style="border: 1px solid #ddd; padding: 8px;">Intermittent or unpredictable workloads</td> <td style="border: 1px solid #ddd; padding: 8px;">Large payloads, long processing times</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Latency</td> <td style="border: 1px solid #ddd; padding: 8px;">Lowest (milliseconds)</td> <td style="border: 1px solid #ddd; padding: 8px;">Low (seconds, with potential cold starts)</td> <td style="border: 1px solid #ddd; padding: 8px;">Highest (minutes to hours)</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Scalability</td> <td style="border: 1px solid #ddd; padding: 8px;">Manual or auto-scaling</td> <td style="border: 1px solid #ddd; padding: 8px;">Automatic scaling</td> <td style="border: 1px solid #ddd; padding: 8px;">Automatic scaling</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Cost Model</td> <td style="border: 1px solid #ddd; padding: 8px;">Pay for provisioned instances</td> <td style="border: 1px solid #ddd; padding: 8px;">Pay per request and compute time</td> <td style="border: 1px solid #ddd; padding: 8px;">Pay for processing time</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Max. Payload Size</td> <td style="border: 1px solid #ddd; padding: 8px;">6 MB</td> <td style="border: 1px solid #ddd; padding: 8px;">6 MB</td> <td style="border: 1px solid #ddd; padding: 8px;">1 GB</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Max. Processing Time</td> <td style="border: 1px solid #ddd; padding: 8px;">60 seconds</td> <td style="border: 1px solid #ddd; padding: 8px;">60 seconds</td> <td style="border: 1px solid #ddd; padding: 8px;">1 hour</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Infrastructure Management</td> <td style="border: 1px solid #ddd; padding: 8px;">User managed</td> <td style="border: 1px solid #ddd; padding: 8px;">Fully managed</td> <td style="border: 1px solid #ddd; padding: 8px;">Fully managed</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Cold Starts</td> <td style="border: 1px solid #ddd; padding: 8px;">No</td> <td style="border: 1px solid #ddd; padding: 8px;">Yes (mitigated by provisioned concurrency)</td> <td style="border: 1px solid #ddd; padding: 8px;">No</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Advantages</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Lowest latency</li> <li>Predictable performance</li> <li>Full control over instance types</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>No infrastructure management</li> <li>Automatic scaling</li> <li>Cost-effective for variable workloads</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Handles large payloads</li> <li>Long processing times</li> <li>Cost-effective for batch processing</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Limitations</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Requires capacity planning</li> <li>Potentially higher costs for idle resources</li> <li>Limited payload size and processing time</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Potential cold start latency</li> <li>Limited customization options</li> <li>Max. 6 MB payload and 60 seconds processing</li> </ul> </td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Not suitable for real-time applications</li> <li>Requires additional setup for result handling</li> <li>Max. 1 hour processing time</li> </ul> </td> </tr> </table> <p style="color: #555; font-size: 14px;">Key takeaways:</p> <ul> <li><strong>Real-time Inference</strong> is best for applications requiring immediate responses and consistent low latency, such as interactive web applications or real-time decision making systems.</li> <li><strong>Serverless Inference</strong> is ideal for applications with variable or unpredictable traffic patterns, offering automatic scaling and cost optimization for sporadic workloads.</li> <li><strong>Asynchronous Inference</strong> excels in scenarios involving large payloads or long processing times, such as batch processing of images or videos, or complex ML models that require extensive computation.</li> </ul> <p style="color: #555; font-size: 14px;">When choosing an endpoint type, consider factors such as latency requirements, payload size, processing time, traffic patterns, and cost optimization needs.
            
            <p style="color: #555; font-size: 14px;">Each type has its strengths and is designed to address specific use cases. Here are some additional considerations when choosing an endpoint type:</p> <ul> <li><strong>Traffic patterns:</strong> If your workload has consistent, predictable traffic, real-time endpoints might be more suitable. For sporadic or unpredictable traffic, serverless inference can be more cost-effective.</li> <li><strong>Model complexity:</strong> Complex models with long inference times might benefit from asynchronous inference, while simpler models with quick inference times are well-suited for real-time or serverless endpoints.</li> <li><strong>Integration requirements:</strong> Consider how the endpoint will integrate with your existing infrastructure and applications. Real-time endpoints offer the most flexibility in terms of integration options.</li> <li><strong>Budget constraints:</strong> Serverless and asynchronous inference can offer cost savings for certain workloads, but it's important to analyze your specific use case to determine the most cost-effective option.</li> </ul> 
			<img style="max-width: 720px;" src="https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/inference-workflow-options.png" />
			<p style="color: #333; font-size: 16px;">Advanced Features and Optimizations</p> <p style="color: #555; font-size: 14px;">Amazon SageMaker offers several advanced features and optimizations that can be applied across different endpoint types:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Feature</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Applicable Endpoint Types</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Multi-Model Endpoints</td> <td style="border: 1px solid #ddd; padding: 8px;">Host multiple models on a single endpoint, reducing infrastructure costs and improving resource utilization.</td> <td style="border: 1px solid #ddd; padding: 8px;">Real-time, Serverless</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Auto Scaling</td> <td style="border: 1px solid #ddd; padding: 8px;">Automatically adjust the number of instances to match the workload demand.</td> <td style="border: 1px solid #ddd; padding: 8px;">Real-time</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Elastic Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Attach GPU-powered inference acceleration to a CPU-based instance for cost-effective inference.</td> <td style="border: 1px solid #ddd; padding: 8px;">Real-time</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">SageMaker Neo</td> <td style="border: 1px solid #ddd; padding: 8px;">Optimize machine learning models to run more efficiently on specific hardware platforms.</td> <td style="border: 1px solid #ddd; padding: 8px;">All</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Inference Pipeline</td> <td style="border: 1px solid #ddd; padding: 8px;">Create a pipeline of multiple models or preprocessing logic that are invoked sequentially on a single endpoint.</td> <td style="border: 1px solid #ddd; padding: 8px;">Real-time, Serverless</td> </tr> </table> <p style="color: #333; font-size: 16px;">Monitoring and Management</p> <p style="color: #555; font-size: 14px;">Regardless of the endpoint type chosen, it's crucial to implement proper monitoring and management practices:</p> <ul> <li><strong>Amazon CloudWatch:</strong> Use CloudWatch to monitor endpoint metrics such as invocations, latency, and errors.</li> <li><strong>Model Monitor:</strong> Implement SageMaker Model Monitor to detect concept drift and data quality issues in your deployed models.</li> <li><strong>A/B Testing:</strong> Utilize SageMaker's built-in A/B testing capabilities to compare different model versions or configurations.</li> <li><strong>Endpoint Deployment Guardrails:</strong> Implement deployment strategies like blue/green deployments to minimize risk when updating models in production.</li> </ul> <p style="color: #333; font-size: 16px;">Best Practices</p> <p style="color: #555; font-size: 14px;">To ensure optimal performance and cost-effectiveness of your SageMaker endpoints, consider the following best practices:</p> <ul> <li>Regularly review and optimize your endpoint configurations based on actual usage patterns.</li> <li>Use appropriate instance types that match your model's resource requirements.</li> <li>Implement proper error handling and retry mechanisms in your client applications.</li> <li>Leverage SageMaker's built-in security features, such as encryption at rest and in transit.</li> <li>Implement a robust CI/CD pipeline for model deployment and updates.</li> <li>Regularly update your SageMaker SDK and dependencies to benefit from the latest features and optimizations.</li> </ul> <p style="color: #555; font-size: 14px;">By carefully considering your use case requirements and leveraging SageMaker's various endpoint types and advanced features, you can create an efficient, scalable, and cost-effective inference solution for your machine learning models.</p>

            <hr />

            <p style="color: #333; font-size: 16px;">Deploy a Model in Amazon SageMaker</p> <p style="color: #555; font-size: 14px;">After training your machine learning model, Amazon SageMaker offers various deployment options for getting predictions. The choice depends on your specific use case:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Deployment Option</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Use Case</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">SageMaker real-time hosting services</td> <td style="border: 1px solid #ddd; padding: 8px;">Persistent, real-time endpoints for single predictions</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Serverless Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Workloads with idle periods between traffic spikes, tolerant of cold starts</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Asynchronous Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Large payloads (up to 1GB), long processing times, near real-time latency requirements</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Batch transform</td> <td style="border: 1px solid #ddd; padding: 8px;">Predictions for entire datasets</td> </tr> </table> <p style="color: #555; font-size: 14px;">SageMaker also provides additional features for managing resources and optimizing inference performance:</p> <ul> <li><strong>SageMaker Edge Manager:</strong> Manage models on edge devices (e.g., smart cameras, robots, personal computers, mobile devices)</li> <li><strong>Neo:</strong> Optimize various types of models (Gluon, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX) for inference on different platforms and processors</li> </ul> <p style="color: #555; font-size: 14px;">Key points to remember:</p> <ul> <li>Choose the deployment option that best fits your use case and requirements.</li> <li>Consider factors such as latency, payload size, processing time, and traffic patterns when selecting a deployment method.</li> <li>Leverage SageMaker's additional features for managing and optimizing your deployed models.</li> <li>Explore edge deployment options for scenarios involving edge devices.</li> <li>Use model optimization tools like Neo to improve inference performance across various platforms.</li> </ul> <p style="color: #555; font-size: 14px;">For a comprehensive overview of all deployment options, refer to the "Deploy models for inference" documentation in Amazon SageMaker.</p>

            <hr />

            <p style="color: #333; font-size: 16px;">Steps for Model Deployment</p> <p style="color: #555; font-size: 14px;">The general workflow for inference endpoints consists of the following steps:</p> <ol> <li>Create a model in SageMaker Inference by pointing to model artifacts in Amazon S3 and a container image.</li> <li>Select an inference option.</li> <li>Create a SageMaker Inference endpoint configuration, choosing instance type and number of instances.</li> <li>Create a SageMaker Inference endpoint.</li> <li>Invoke your endpoint to receive an inference as a response.</li> </ol> <p style="color: #555; font-size: 14px;">You can perform these actions using the AWS console, AWS SDKs, SageMaker Python SDK, AWS CloudFormation, or AWS CLI.</p> <p style="color: #555; font-size: 14px;">For batch inference with batch transform, point to your model artifacts and input data, create a batch inference job, and SageMaker will output your inferences to an Amazon S3 location of your choice.</p> <p style="color: #333; font-size: 16px;">Inference Options</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Option</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Use Case</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Key Features</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Real-Time Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Online inferences with low latency or high throughput requirements</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Persistent, fully managed endpoint</li> <li>Payload up to 6 MB</li> <li>Processing time up to 60 seconds</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Serverless Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Intermittent or unpredictable traffic patterns</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>No instance management</li> <li>Pay only for what you use</li> <li>Payload up to 4 MB</li> <li>Processing time up to 60 seconds</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Batch Transform</td> <td style="border: 1px solid #ddd; padding: 8px;">Offline processing of large datasets</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Support for large datasets (GBs)</li> <li>Processing times of days</li> <li>No persistent endpoint</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Asynchronous Inference</td> <td style="border: 1px solid #ddd; padding: 8px;">Queue requests with large payloads and long processing times</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Payload up to 1 GB</li> <li>Processing time up to one hour</li> <li>Scale down to 0 when idle</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;">Advanced Endpoint Options for Real-Time Inference</p> <ul> <li><strong>Multi-Model Endpoints (same container):</strong> Host multiple models using the same framework in one container behind one endpoint.</li> <li><strong>Multi-Model Endpoints (different containers):</strong> Host multiple models using different frameworks in separate containers behind one endpoint.</li> <li><strong>Serial Inference Pipelines:</strong> Host models with pre-processing and post-processing logic behind an endpoint for lower latency.</li> </ul> <p style="color: #333; font-size: 16px;">Bring Your Own Model</p> <p style="color: #555; font-size: 14px;">To use an existing Docker container or create a new one for inference in SageMaker:</p> <ul> <li>Adapt your Docker container to work with SageMaker</li> <li>Use your own inference code with hosting services</li> <li>Use your own inference code with batch transform</li> </ul> <p style="color: #555; font-size: 14px;">Refer to the SageMaker documentation for detailed guidance on these options.</p>

            <hr />

            <p style="color: #333; font-size: 16px;">Monitoring</p> <ul> <li>Use <strong>Model Monitor</strong> to track model metrics and set alerts for deviations in model quality.</li> <li>Monitor endpoint health using <strong>Amazon CloudWatch</strong> metrics (e.g., invocation errors, model latency).</li> <li>Refer to "Monitor Amazon SageMaker" documentation for more tools and techniques.</li> </ul> <p style="color: #333; font-size: 16px;">CI/CD for Model Deployment</p> <ul> <li>Utilize <strong>SageMaker MLOps</strong> to automate machine learning workflows and practice CI/CD.</li> <li>Use <strong>MLOps Project Templates</strong> for setup and implementation of SageMaker MLOps projects.</li> <li>SageMaker supports using third-party Git repos for creating CI/CD systems.</li> <li>Use <strong>Model Registry</strong> to manage model versions and automate deployments.</li> </ul> <p style="color: #333; font-size: 16px;">Deployment Guardrails</p> <ul> <li>Control the switch from current to new models in production.</li> <li>Use traffic shifting modes for granular control over the deployment process.</li> <li>Leverage built-in safeguards like auto-rollbacks to catch issues early.</li> </ul> <p style="color: #333; font-size: 16px;">Inferentia</p> <ul> <li>Use <strong>Inf1 instances</strong> for large-scale ML and deep learning applications.</li> <li>Suitable for image/speech recognition, NLP, personalization, forecasting, fraud detection.</li> <li>Provides higher throughput and lower cost per inference than GPU-based instances.</li> <li>Compile models with SageMaker Neo for deployment on Inf1 instances.</li> </ul> <p style="color: #333; font-size: 16px;">Optimize Model Performance</p> <ul> <li>Use SageMaker's built-in algorithms, pre-built models, and prebuilt Docker images.</li> <li>Leverage <strong>SageMaker Neo</strong> to optimize models for deployment on various processors.</li> <li>Supports TensorFlow, Apache MXNet, PyTorch, ONNX, and XGBoost models.</li> </ul> <p style="color: #333; font-size: 16px;">Autoscaling</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Option</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Manual Autoscaling</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Dynamically adjust the number of instances based on workload changes.</li> <li>Set up scaling policies to manage resources during peak and low traffic periods.</li> <li>Refer to "Automatically Scale Amazon SageMaker Models" documentation.</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Serverless Inference</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Suitable for unpredictable traffic patterns.</li> <li>SageMaker manages autoscaling automatically.</li> <li>Scales down during low traffic and up during high traffic.</li> <li>Refer to "Deploy models with Amazon SageMaker Serverless Inference" documentation.</li> </ul> </td> </tr> </table> <p style="color: #555; font-size: 14px;">By leveraging these advanced features and best practices, you can optimize your model deployment, ensure robust monitoring, and implement efficient scaling strategies in Amazon SageMaker.</p>

		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Create a model in Amazon SageMaker with ModelBuilder</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Create a Model in Amazon SageMaker with ModelBuilder</p> <p style="color: #555; font-size: 14px;">ModelBuilder simplifies the process of preparing and deploying models on SageMaker endpoints by performing the following tasks:</p> <ul> <li>Converts ML models from various frameworks into deployable models in one step</li> <li>Performs automatic container selection based on the model framework</li> <li>Handles serialization and deserialization of data</li> <li>Captures dependencies and packages the model automatically</li> <li>Optionally performs local parameter tuning for LLM use cases</li> <li>Supports popular model servers and containers</li> </ul> <p style="color: #333; font-size: 16px;">Building Your Model with ModelBuilder</p> <p style="color: #555; font-size: 14px;">Basic usage of ModelBuilder:</p>
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
from sagemaker.serve.builder.model_builder import ModelBuilder 
from sagemaker.serve.builder.schema_builder import SchemaBuilder 
model_builder = ModelBuilder( model=model, schema_builder=SchemaBuilder(input, output), role_arn="execution-role", ) </pre> 
            <p style="color: #333; font-size: 16px;">Defining Serialization and Deserialization Methods</p> <p style="color: #555; font-size: 14px;">You can customize serialization and deserialization using CustomPayloadTranslator:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
from sagemaker.serve import CustomPayloadTranslator 
class MyRequestTranslator(CustomPayloadTranslator): 
    def serialize_payload_to_bytes(self, payload: object) -> bytes: 
    def deserialize_payload_from_stream(self, stream) -> object: 
class MyResponseTranslator(CustomPayloadTranslator):
    def serialize_payload_to_bytes(self, payload: object) -> bytes: 
    def deserialize_payload_from_stream(self, stream) -> object:</pre> 
                <p style="color: #333; font-size: 16px;">Customizing Model Loading and Request Handling</p> <p style="color: #555; font-size: 14px;">Use InferenceSpec to customize model loading and request handling:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
from sagemaker.serve.spec.inference_spec import InferenceSpec 
class MyInferenceSpec(InferenceSpec): 
    def load(self, model_dir: str): # Implementation here 
    def invoke(self, input, model): # Implementation here 
inf_spec = MyInferenceSpec() 
model_builder = ModelBuilder( inference_spec=inf_spec, schema_builder=SchemaBuilder(X_test, y_pred) ) </pre> 
            <p style="color: #333; font-size: 16px;">Building and Deploying Your Model</p> <p style="color: #555; font-size: 14px;">To build and deploy your model:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
model = model_builder.build() 
predictor = model.deploy( initial_instance_count=1, instance_type="ml.c6i.xlarge" ) </pre>
            <p style="color: #333; font-size: 16px;">Bring Your Own Container (BYOC)</p> <p style="color: #555; font-size: 14px;">To use your own container:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
model_builder = ModelBuilder( model=model, model_server=ModelServer.TORCHSERVE, 
    schema_builder=SchemaBuilder(X_test, y_pred), image_uri="your-image-uri" ) </pre> 
            <p style="color: #333; font-size: 16px;">Using ModelBuilder in Local Mode</p> <p style="color: #555; font-size: 14px;">To deploy your model locally:</p> 
            <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">
model_builder_local = ModelBuilder( model=model, schema_builder=SchemaBuilder(X_test, y_pred), 
    role_arn=execution-role, mode=Mode.LOCAL_CONTAINER ) 
xgb_local_builder = model_builder_local.build() 
predictor_local = xgb_local_builder.deploy() </pre> 
            <p style="color: #333; font-size: 16px;">Troubleshooting Local Mode</p> <p style="color: #555; font-size: 14px;">Common issues and solutions:</p> <ul> <li><strong>Address already in use:</strong> Identify and clean up the process using the port</li> <li><strong>IAM Permission Issue:</strong> Verify the execution role has necessary permissions</li> <li><strong>EBS volume capacity issue:</strong> Move Docker volume to a filesystem with enough space</li> </ul> <p style="color: #555; font-size: 14px;">For detailed steps on moving Docker directory and cleaning up space, refer to the provided instructions in the original documentation.</p>
            
		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Optimize model inference with Amazon SageMaker</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Optimize Model Inference with Amazon SageMaker</p> <p style="color: #555; font-size: 14px;">Amazon SageMaker offers various techniques to improve the performance of generative AI models, helping you achieve better cost-performance for your use case. These optimization techniques include:</p> <ul> <li>Quantization</li> <li>Speculative decoding</li> <li>Compilation</li> </ul> <p style="color: #555; font-size: 14px;">After optimization, you can evaluate the model's performance metrics for latency, throughput, and price. SageMaker also provides pre-optimized versions of many models, each catering to different application needs for latency and throughput.</p> <p style="color: #333; font-size: 16px;">Optimization Techniques</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Technique</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Speculative Decoding</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Speeds up decoding process of large LLMs</li> <li>Optimizes for latency without compromising text quality</li> <li>Uses a smaller, faster "draft model" to generate candidate tokens</li> <li>Larger "target model" verifies and occasionally regenerates tokens</li> <li>SageMaker offers pre-built draft models or supports custom ones</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Quantization</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Reduces hardware requirements by using less precise data types</li> <li>Allows hosting on less expensive and more available GPUs</li> <li>May result in slightly reduced model accuracy</li> <li>SageMaker supports Activation-aware Weight Quantization (AWQ) for GPUs</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Compilation</td> <td style="border: 1px solid #ddd; padding: 8px;"> <ul> <li>Optimizes model for best performance on chosen hardware without accuracy loss</li> <li>Applicable for accelerated hardware like AWS Trainium or AWS Inferentia</li> <li>Provides ahead-of-time compilation benefits</li> <li>Reduces deployment time and auto-scaling latency</li> </ul> </td> </tr> </table> <p style="color: #555; font-size: 14px;">Key benefits of these optimization techniques:</p> <ul> <li>Improved model performance</li> <li>Better cost-efficiency</li> <li>Reduced hardware requirements</li> <li>Faster deployment and scaling</li> </ul> <p style="color: #555; font-size: 14px;">When optimizing your models with SageMaker:</p> <ol> <li>Choose the appropriate optimization technique(s) based on your use case and requirements</li> <li>Apply the selected optimization(s) to your model</li> <li>Evaluate the optimized model's performance metrics</li> <li>Compare with pre-optimized versions provided by SageMaker, if available</li> <li>Deploy the optimized model or choose a pre-optimized version that best suits your needs</li> </ol> <p style="color: #555; font-size: 14px;">By leveraging these optimization techniques, you can significantly improve the performance and cost-effectiveness of your generative AI models in Amazon SageMaker.</p>
            <hr />
            <p style="color: #333; font-size: 16px;">Evaluation Metrics in Amazon SageMaker</p> <p style="color: #555; font-size: 14px;">When evaluating optimized models in Amazon SageMaker, the following metrics are provided:</p> <p style="color: #333; font-size: 16px;">Latency Metrics</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Metric</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Concurrency</td> <td style="border: 1px solid #ddd; padding: 8px;">Number of simulated concurrent users invoking the endpoint</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Time to first token (ms)</td> <td style="border: 1px solid #ddd; padding: 8px;">Time between request sent and first token of streaming response received</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Inter-token latency (ms)</td> <td style="border: 1px solid #ddd; padding: 8px;">Time to generate an output token for each request</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Client latency (ms)</td> <td style="border: 1px solid #ddd; padding: 8px;">Request latency from send to full response received</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Input tokens/sec (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total input tokens generated divided by total duration for the concurrency</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Output tokens/sec (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total output tokens generated divided by total duration for the concurrency</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Client invocations (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total inference requests sent to the endpoint across all users</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Client invocation errors (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total inference requests resulting in an invocation error</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Tokenizer failed (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total requests where tokenizer failed to parse request or response</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Empty inference response (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total requests resulting in zero output tokens or tokenizer parsing failure</td> </tr> </table> <p style="color: #333; font-size: 16px;">Throughput Metrics</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Metric</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Concurrency</td> <td style="border: 1px solid #ddd; padding: 8px;">Number of simulated concurrent users invoking the endpoint</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Input tokens/sec/req (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total input tokens generated per second per request</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Output tokens/sec/req (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total output tokens generated per second per request</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Input tokens (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total input tokens generated per request</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Output tokens (count)</td> <td style="border: 1px solid #ddd; padding: 8px;">Total output tokens generated per request</td> </tr> </table> <p style="color: #333; font-size: 16px;">Price Metrics</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Metric</strong></td> <td style="border: 1px solid #ddd; padding: 8px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Concurrency</td> <td style="border: 1px solid #ddd; padding: 8px;">Number of simulated concurrent users invoking the endpoint</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Price per million input tokens</td> <td style="border: 1px solid #ddd; padding: 8px;">Cost of processing 1M input tokens</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Price per million output tokens</td> <td style="border: 1px solid #ddd; padding: 8px;">Cost of generating 1M output tokens</td> </tr> </table> <p style="color: #555; font-size: 14px;">These metrics provide a comprehensive view of your model's performance, allowing you to assess latency, throughput, and cost-effectiveness. Use these metrics to compare different optimization techniques and choose the best configuration for your specific use case.</p>

		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Validate a Machine Learning Model</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Validating a Machine Learning Model is a crucial step after training to determine its performance and accuracy. This process helps you assess whether the model can achieve your business goals.</p> <p style="color: #333; font-size: 16px;">There are two main approaches to evaluate your model:</p> <ul> <li style="color: #333; font-size: 16px;">Offline testing: Using historical data to send inference requests to the model</li> <li style="color: #333; font-size: 16px;">Online testing: Using live data to test the model in production</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Offline Testing:</span></p> <ul> <li style="color: #333; font-size: 16px;">Deploy the trained model to an alpha endpoint</li> <li style="color: #333; font-size: 16px;">Use historical data for inference requests</li> <li style="color: #333; font-size: 16px;">Utilize a Jupyter notebook in Amazon SageMaker notebook instance</li> <li style="color: #333; font-size: 16px;">Use AWS SDK for Python (Boto) or SageMaker's high-level Python library</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Online Testing:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use production variants for A/B testing</li> <li style="color: #333; font-size: 16px;">Deploy models with the same inference code on the same SageMaker endpoint</li> <li style="color: #333; font-size: 16px;">Configure variants to receive a portion of live traffic (e.g., 10%)</li> <li style="color: #333; font-size: 16px;">Evaluate performance and adjust traffic distribution accordingly</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Offline Model Evaluation Options:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Method</strong></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Validating using a holdout set</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Set aside 20-30% of training data as a holdout set</li> <li>Evaluate model performance on unseen data</li> <li>Assess model's ability to generalize</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">k-fold validation</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Split dataset into k parts (typically 5-10)</li> <li>Treat each part as a holdout set for k training runs</li> <li>Use remaining k-1 parts as training set for each run</li> <li>Produce k models and aggregate results</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;">When evaluating your model, consider the following factors:</p> <ul> <li style="color: #333; font-size: 16px;">Apply different business rules to each model</li> <li style="color: #333; font-size: 16px;">Use various measures to determine model suitability</li> <li style="color: #333; font-size: 16px;">Consider whether the model needs to be more sensitive or specific</li> <li style="color: #333; font-size: 16px;">Assess the model's ability to generalize from initial training</li> </ul> <p style="color: #333; font-size: 16px;">For further information on model evaluation, consult additional resources such as articles and books on the topic, like "Evaluating Machine Learning Models."</p>
            
            <hr />

            <p style="color: #333; font-size: 16px;">Amazon SageMaker Inference Recommender is a capability of Amazon SageMaker that streamlines the process of deploying machine learning (ML) models to production. It automates load testing and model tuning across SageMaker ML instances, helping you find the optimal deployment configuration for your models.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">Reduces time to get ML models into production</li> <li style="color: #333; font-size: 16px;">Helps deploy models to real-time or serverless inference endpoints</li> <li style="color: #333; font-size: 16px;">Optimizes for best performance at the lowest cost</li> <li style="color: #333; font-size: 16px;">Considers multiple factors for optimal configuration</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Factors Considered:</span></p> <ul> <li style="color: #333; font-size: 16px;">Instance type</li> <li style="color: #333; font-size: 16px;">Instance count</li> <li style="color: #333; font-size: 16px;">Container parameters</li> <li style="color: #333; font-size: 16px;">Model optimizations</li> <li style="color: #333; font-size: 16px;">Max concurrency</li> <li style="color: #333; font-size: 16px;">Memory size</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Pricing:</span></p> <p style="color: #333; font-size: 16px;">Amazon SageMaker Inference Recommender only charges for the instances used during job execution, making it a cost-effective solution for optimizing your ML deployments.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How It Works:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Step</strong></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">1. Model Preparation</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Create a SageMaker model, or</li> <li>Register a model to the SageMaker model registry with your model artifacts</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">2. Run Benchmarking Jobs</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Use AWS SDK for Python (Boto3) or SageMaker console</li> <li>Test different SageMaker endpoint configurations</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">3. Analyze Results</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Collect and visualize metrics</li> <li>Evaluate performance and resource utilization</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">4. Make Decisions</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Choose the optimal endpoint type and configuration</li> <li>Balance performance and cost considerations</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Benefits:</span></p> <ul> <li style="color: #333; font-size: 16px;">Automates the process of finding the best deployment configuration</li> <li style="color: #333; font-size: 16px;">Saves time and resources in optimizing ML model deployments</li> <li style="color: #333; font-size: 16px;">Provides data-driven insights for decision-making</li> <li style="color: #333; font-size: 16px;">Helps achieve optimal performance while minimizing costs</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging Amazon SageMaker Inference Recommender, you can streamline your ML deployment process, ensure optimal performance, and manage costs effectively. This tool is particularly valuable for teams looking to scale their ML operations and maximize the value of their models in production environments.</p>

            <hr />

            <p style="color: #333; font-size: 16px;">Real-time inference in Amazon SageMaker is ideal for workloads that require interactive, low-latency responses. It allows you to deploy your model to SageMaker hosting services and obtain an endpoint for inference.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">Fully managed endpoints</li> <li style="color: #333; font-size: 16px;">Support for autoscaling</li> <li style="color: #333; font-size: 16px;">Interactive or programmatic deployment options</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deployment Options:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Method</strong></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">SageMaker Studio</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Interactive deployment through the SageMaker Studio interface</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">AWS SDKs</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Programmatic deployment using SageMaker Python SDK or SDK for Python (Boto3)</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">AWS CLI</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Deployment using the AWS Command Line Interface</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Prerequisites:</span></p> <ul> <li style="color: #333; font-size: 16px;">AWS Region of your Amazon S3 bucket</li> <li style="color: #333; font-size: 16px;">Amazon S3 URI path for model artifacts</li> <li style="color: #333; font-size: 16px;">IAM role for SageMaker</li> <li style="color: #333; font-size: 16px;">Docker Amazon ECR URI registry path for custom inference code or built-in Docker image details</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Shared Resource Utilization:</span></p> <p style="color: #333; font-size: 16px;">SageMaker allows deploying multiple models to a single endpoint, enabling shared resource utilization. This is best achieved using inference components.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Inference Components:</span></p> <p style="color: #333; font-size: 16px;">An inference component is a SageMaker hosting object used to deploy a model to an endpoint. It specifies the model, endpoint, and resource utilization settings.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Aspects of Inference Components:</span></p> <ul> <li style="color: #333; font-size: 16px;">Flexible specification of model (SageMaker Model object or direct artifact and image specification)</li> <li style="color: #333; font-size: 16px;">Customizable resource allocation (CPU cores, accelerators, memory)</li> <li style="color: #333; font-size: 16px;">Multiple components can be deployed to a single endpoint</li> <li style="color: #333; font-size: 16px;">Direct model invocation using InvokeEndpoint API action</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Benefits of Inference Components:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Benefit</strong></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><strong>Description</strong></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Flexibility</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Decouples model hosting details from the endpoint</li> <li>Allows hosting multiple models on the same infrastructure</li> <li>Enables adding or removing models from an endpoint as needed</li> <li>Supports independent model updates</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Scalability</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Specify number of copies for each model</li> <li>Set minimum number of copies to ensure availability</li> <li>Scale any inference component copy down to zero</li> <li>Allows for efficient resource allocation</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deployment Methods for Inference Components:</span></p> <ul> <li style="color: #333; font-size: 16px;">SageMaker Studio Classic</li> <li style="color: #333; font-size: 16px;">SageMaker Python SDK (using EndpointType.INFERENCE_COMPONENT_BASED)</li> <li style="color: #333; font-size: 16px;">AWS SDK for Python (Boto3) to define and deploy InferenceComponent objects</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging real-time inference and inference components in Amazon SageMaker, you can create flexible, scalable, and efficient deployment solutions for your machine learning models, optimizing resource utilization and improving overall performance.</p>

            <hr />
            <p style="color: #333; font-size: 16px;">Deploying models with Python SDKs in Amazon SageMaker offers flexibility and programmatic control over the deployment process. This guide covers two main approaches: using the SageMaker Python SDK and using the AWS SDK for Python (Boto3).</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">SageMaker Python SDK Approach:</span></p> <p style="color: #333; font-size: 16px;">There are two ways to build your model using the SageMaker Python SDK:</p> <ol> <li style="color: #333; font-size: 16px;">Using the Model class</li> <li style="color: #333; font-size: 16px;">Using the ModelBuilder class</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Setup:</span></p> <p style="color: #333; font-size: 16px;">First, import the necessary libraries and define the S3 URL for your model artifacts:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;"> 
import boto3 
from datetime import datetime 
from sagemaker.compute_resource_requirements.resource_requirements 
import ResourceRequirements 
from sagemaker.predictor import Predictor 
from sagemaker.enums import EndpointType 
from sagemaker.model import Model 
from sagemaker.session import Session

#Define S3 URL for model artifacts
s3_bucket = "amzn-s3-demo-bucket" 
bucket_prefix = "sagemaker/model/path" 
model_filename = "my-model-artifact.tar.gz" 
model_s3_key = f"{bucket_prefix}/{model_filename}" 
model_url = f"s3://{s3_bucket}/{model_s3_key}" </pre> 
            </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Configuration:</span></p> <p style="color: #333; font-size: 16px;">Configure the resources required for your model deployment:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                    <pre style="margin: 0; white-space: pre-wrap;"> 
resources = ResourceRequirements( requests = { "num_cpus": 2, 
    "num_accelerators": 1, "memory": 8192, "copies": 1, }, limits = {}, )
now = datetime.now() 
dt_string = now.strftime("%d-%m-%Y-%H-%M-%S") 
model_name = "my-sm-model"+dt_string

Using Model class
model = Model( name = "model-name", image_uri = "image-uri", 
    model_data = model_url, role = "arn:aws:iam::111122223333:role/service-role/role-name", 
    resources = resources, predictor_cls = Predictor, )

Alternatively, using ModelBuilder (commented out)
model_builder = ModelBuilder(
model="<HuggingFace-ID>",
schema_builder=SchemaBuilder(sample_input,sample_output),
env_vars={ "HUGGING_FACE_HUB_TOKEN": "<HuggingFace_token>}" }
)
model = model_builder.build()
model.model_name = unique_name_from_base("mb-inference-component")
model.resources = resources
                </pre>
                </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deployment:</span></p> <p style="color: #333; font-size: 16px;">Deploy the model to an endpoint:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                    <pre style="margin: 0; white-space: pre-wrap;">
predictor = model.deploy( initial_instance_count = 1, instance_type = "ml.p4d.24xlarge", 
    endpoint_type = EndpointType.INFERENCE_COMPONENT_BASED, resources = resources, ) </pre> 
                    </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deploying Multiple Models to the Same Endpoint:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                        <pre style="margin: 0; white-space: pre-wrap;"> 
falcon_predictor = falcon_model.deploy( initial_instance_count = 1, 
    instance_type = "ml.p4d.24xlarge", 
    endpoint_type = EndpointType.INFERENCE_COMPONENT_BASED, 
    endpoint_name = "<endpoint_name>" resources = resources, )
llama2_predictor = llama2_model.deploy( endpoint_type = EndpointType.INFERENCE_COMPONENT_BASED, 
    endpoint_name = "<endpoint_name>" # same endpoint name as for falcon model ) </pre> 
                    </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">The Model class requires specifying the model package or inference code, serialization/deserialization scripts, and dependencies.</li> <li style="color: #333; font-size: 16px;">ModelBuilder automatically captures dependencies, infers serialization/deserialization functions, and packages dependencies.</li> <li style="color: #333; font-size: 16px;">ResourceRequirements object defines CPU cores, accelerators, and memory allocation for the model.</li> <li style="color: #333; font-size: 16px;">The deploy method is used to create a real-time, HTTPS endpoint for the model.</li> <li style="color: #333; font-size: 16px;">Multiple models can be deployed to the same endpoint by specifying the same endpoint_name.</li> </ul> <p style="color: #333; font-size: 16px;">This approach provides a streamlined way to deploy models in SageMaker, offering flexibility in resource allocation and the ability to deploy multiple models to a single endpoint for efficient resource utilization.</p>


                <hr />

                <p style="color: #333; font-size: 16px;">After deploying your model to an Amazon SageMaker endpoint, you can invoke it using the AWS SDK for Python (Boto3). This guide covers the process of setting up the client and using the invoke_endpoint method for invoking the endpoint.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Setting Up the Client and Invoking the Endpoint:</span></p> <p style="color: #333; font-size: 16px;">The following code demonstrates how to set up the SageMaker Runtime client, specify your endpoint name, and invoke the endpoint:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                    <pre style="margin: 0; white-space: pre-wrap;"> 
import boto3

#Create a low-level client representing Amazon SageMaker Runtime
sagemaker_runtime = boto3.client( "sagemaker-runtime", region_name='aws_region')

#The endpoint name must be unique within an AWS Region in your AWS account.
endpoint_name='endpoint-name'

#Gets inference from the model hosted at the specified endpoint:
response = sagemaker_runtime.invoke_endpoint( EndpointName=endpoint_name, 
    Body=bytes('{"features": ["This is great!"]}', 'utf-8') )

#Decodes and prints the response body:
print(response['Body'].read().decode('utf-8'))

Print the HTTP status code:
print(response["HTTPStatusCode"]) </pre>
                </td>
                    
                    </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">The boto3 client is created for the "sagemaker-runtime" service.</li> <li style="color: #333; font-size: 16px;">The endpoint_name variable should be set to your unique endpoint name.</li> <li style="color: #333; font-size: 16px;">The invoke_endpoint method is used to send an inference request to the model.</li> <li style="color: #333; font-size: 16px;">Input data is provided in the Body field and must match the format used during training.</li> <li style="color: #333; font-size: 16px;">The response includes the inference result, HTTP status, and other metadata.</li> <li style="color: #333; font-size: 16px;">The response body is decoded and printed.</li> <li style="color: #333; font-size: 16px;">The HTTP status code is printed separately to check for successful invocation.</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Ensure that your input data format matches what the model expects.</li> <li style="color: #333; font-size: 16px;">Handle potential errors and exceptions in your invocation code.</li> <li style="color: #333; font-size: 16px;">Monitor the HTTP status codes to ensure successful invocations.</li> <li style="color: #333; font-size: 16px;">The response variable provides access to various fields, including the HTTP status and the name of the deployed model.</li> </ul> <p style="color: #333; font-size: 16px;">By using the invoke_endpoint method with the AWS SDK for Python (Boto3), you can effectively interact with your deployed SageMaker models. This approach allows you to send inference requests and receive responses from your model in a straightforward manner.</p>                

		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Hosting Options</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Multi-model endpoints in Amazon SageMaker provide a scalable and cost-effective solution for deploying large numbers of models. This approach uses shared resources and a single serving container to host multiple models, reducing hosting costs and deployment overhead.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Benefits:</span></p> <ul> <li style="color: #333; font-size: 16px;">Improved endpoint utilization compared to single-model endpoints</li> <li style="color: #333; font-size: 16px;">Reduced deployment overhead</li> <li style="color: #333; font-size: 16px;">SageMaker manages model loading and scaling based on traffic patterns</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Ideal Use Cases:</span></p> <ul> <li style="color: #333; font-size: 16px;">Hosting many models using the same ML framework</li> <li style="color: #333; font-size: 16px;">Mix of frequently and infrequently accessed models</li> <li style="color: #333; font-size: 16px;">Applications tolerant of occasional cold start latency</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Support for CPU and GPU Models:</span></p> <ul> <li style="color: #333; font-size: 16px;">Both CPU and GPU backed models are supported</li> <li style="color: #333; font-size: 16px;">GPU models can lower deployment costs through increased endpoint usage</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Compatible Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">AWS PrivateLink and VPCs</li> <li style="color: #333; font-size: 16px;">Auto scaling</li> <li style="color: #333; font-size: 16px;">Serial inference pipelines (with limitations)</li> <li style="color: #333; font-size: 16px;">A/B testing</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Invoking a Multi-Model Endpoint:</span></p> <p style="color: #333; font-size: 16px;">To invoke a multi-model endpoint, use the <code>invoke_endpoint</code> method from the SageMaker Runtime, adding a <code>TargetModel</code> parameter to specify which model to invoke:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
response = runtime_sagemaker_client.invoke_endpoint( EndpointName = <ENDPOINT_NAME>, 
    ContentType = "text/csv", TargetModel = <MODEL_FILENAME>.tar.gz, Body = body) </pre> 
                            </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Handling Cold Starts:</span></p> <ul> <li style="color: #333; font-size: 16px;">First request to a model may take longer due to model loading (cold start)</li> <li style="color: #333; font-size: 16px;">Subsequent calls are faster once the model is loaded</li> <li style="color: #333; font-size: 16px;">For GPU instances, a 507 HTTP response indicates resource constraints</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Retrying Requests on ModelNotReadyException:</span></p> <p style="color: #333; font-size: 16px;">For large models that take longer than 60 seconds to load, implement a retry strategy:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
import boto3 from botocore.config import Config
config = Config( read_timeout=70, retries={ 'max_attempts': 2 # Can be adjusted up to 5 for 360s max timeout } ) 
runtime_sagemaker_client = boto3.client('sagemaker-runtime', config=config) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use multi-model endpoints for models with similar size and latency requirements</li> <li style="color: #333; font-size: 16px;">Consider dedicated endpoints for models with high TPS or strict latency requirements</li> <li style="color: #333; font-size: 16px;">Implement appropriate retry strategies for large models</li> <li style="color: #333; font-size: 16px;">Monitor and optimize resource utilization across models</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging multi-model endpoints in Amazon SageMaker, you can efficiently manage and serve multiple models, optimizing resource usage and reducing costs while maintaining flexibility in your machine learning deployments.</p>

        <hr />

        <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Multi-Model Endpoint Security</span></p> <p style="color: #333; font-size: 16px;">Security for multi-model endpoints in Amazon SageMaker involves several key aspects:</p> <ul> <li style="color: #333; font-size: 16px;">Models and data are co-located on instance storage volume and container memory</li> <li style="color: #333; font-size: 16px;">Instances run on single-tenant containers owned by you</li> <li style="color: #333; font-size: 16px;">Only your models can run on your multi-model endpoint</li> <li style="color: #333; font-size: 16px;">You are responsible for managing request-to-model mapping and user access to target models</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">IAM-based Access Control:</span></p> <p style="color: #333; font-size: 16px;">SageMaker uses IAM roles to provide identity-based policies. To restrict <code>InvokeEndpoint</code> access to specific models, you can:</p> <ol> <li style="color: #333; font-size: 16px;">Use the <code>sagemaker:TargetModel</code> IAM condition key</li> <li style="color: #333; font-size: 16px;">Create multi-model endpoints with more restrictive S3 prefixes</li> </ol> <p style="color: #333; font-size: 16px;">Example IAM policy using <code>sagemaker:TargetModel</code>:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
            <pre style="margin: 0; white-space: pre-wrap;">
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "sagemaker:InvokeEndpoint"
            ],
            "Effect": "Allow",
            "Resource":
            "arn:aws:sagemaker:region:account-id:endpoint/endpoint_name",
            "Condition": {
                // TargetModel provided must be from this set of values
                "StringLike": {
                    "sagemaker:TargetModel": ["company_a/*", "common/*"]
                }
            }
        }
    ]
}
            </pre> 
        </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">CloudWatch Metrics for Multi-Model Endpoint Deployments</span></p> <p style="color: #333; font-size: 16px;">Amazon SageMaker provides CloudWatch metrics for monitoring multi-model endpoints. These metrics are available for both CPU and GPU backed endpoints.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Metrics for CPU and GPU Backed Endpoints:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Metric</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelLoadingWaitTime</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time waited for model to be downloaded or loaded</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelUnloadingTime</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time taken to unload a model</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelDownloadingTime</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time taken to download a model from S3</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelLoadingTime</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time taken to load a model</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelCacheHit</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Number of requests for which the model was already loaded</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">LoadedModelCount</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Number of models loaded in the containers</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">CPUUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Sum of individual CPU core utilization</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">MemoryUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Percentage of memory used by containers</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">DiskUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Percentage of disk space used by containers</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Additional Metrics for GPU Backed Endpoints:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Metric</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">GPUUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Percentage of GPU units used by containers</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">GPUMemoryUtilization</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Percentage of GPU memory used by containers</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Important Notes:</span></p> <ul> <li style="color: #333; font-size: 16px;">Metrics are available at a 1-minute frequency</li> <li style="color: #333; font-size: 16px;">Per-model metrics are not supported</li> <li style="color: #333; font-size: 16px;">For information on metric retention, refer to the Amazon CloudWatch API Reference</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging these security measures and monitoring capabilities, you can ensure the safe and efficient operation of your multi-model endpoints in Amazon SageMaker, while maintaining visibility into their performance and resource utilization.</p>

        <hr />

        <p style="color: #333; font-size: 16px;">Amazon SageMaker multi-model endpoints support automatic scaling, which manages model replicas based on traffic patterns. This guide covers how to set up auto-scaling policies for both CPU and GPU backed multi-model endpoints.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">Configure endpoint and instance size based on SageMaker recommendations</li> <li style="color: #333; font-size: 16px;">Set up instance-based auto-scaling for your endpoint</li> <li style="color: #333; font-size: 16px;">Invocation rates for auto-scale events are based on aggregate predictions across all models</li> <li style="color: #333; font-size: 16px;">Metrics are available at one-minute granularity</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Defining a Scaling Policy:</span></p> <p style="color: #333; font-size: 16px;">Use a target-tracking scaling policy, which can be configured using either predefined or custom metrics. The policy is defined as a JSON block and saved in a text file for use with AWS CLI or Application Auto Scaling API.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">1. Using a Predefined Metric:</span></p> <p style="color: #333; font-size: 16px;">The recommended predefined metric is <code>SageMakerVariantInvocationsPerInstance</code>.</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "TargetValue": 70.0, "PredefinedMetricSpecification": { "PredefinedMetricType": "InvocationsPerInstance" } } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">2. Using a Custom Metric:</span></p> <p style="color: #333; font-size: 16px;">Custom metrics can be defined based on production variant metrics that change proportionally to scaling.</p> <p style="color: #333; font-size: 16px;">Example for CPU backed multi-model endpoint:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "TargetValue": 50, "CustomizedMetricSpecification": { "MetricName": "CPUUtilization", "Namespace": "/aws/sagemaker/Endpoints", "Dimensions": [ {"Name": "EndpointName", "Value": "my-endpoint" }, {"Name": "ModelName","Value": "my-model"} ], "Statistic": "Average", "Unit": "Percent" } } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;">Example for GPU backed multi-model endpoint:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "TargetValue": 50, "CustomizedMetricSpecification": { "MetricName": "GPUUtilization", "Namespace": "/aws/sagemaker/Endpoints", "Dimensions": [ {"Name": "EndpointName", "Value": "my-endpoint" }, {"Name": "ModelName","Value": "my-model"} ], "Statistic": "Average", "Unit": "Percent" } } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Adding Cooldown Periods:</span></p> <p style="color: #333; font-size: 16px;">Cooldown periods can be added for scaling out and scaling in operations:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "TargetValue": 70.0, "PredefinedMetricSpecification": { "PredefinedMetricType": "SageMakerVariantInvocationsPerInstance" }, "ScaleInCooldown": 600, "ScaleOutCooldown": 300 } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices and Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use <code>InvocationsPerInstance</code> for multi-model endpoints</li> <li style="color: #333; font-size: 16px;">The <code>TargetValue</code> should be based on your application's latency requirements</li> <li style="color: #333; font-size: 16px;">Perform load testing to determine suitable scaling parameter values</li> <li style="color: #333; font-size: 16px;">Custom metrics must be valid utilization metrics that describe instance busyness</li> <li style="color: #333; font-size: 16px;">The metric value should decrease when the number of instances increases</li> <li style="color: #333; font-size: 16px;">Always test auto-scaling with custom metrics before deploying to production</li> </ul> <p style="color: #333; font-size: 16px;">By implementing these auto-scaling policies, you can ensure that your multi-model endpoints in Amazon SageMaker efficiently handle varying loads, optimizing resource utilization and maintaining performance under different traffic conditions.</p>


		</div>
	</div>
	
	<br/>
	
</div>








<div class="container mt-5">
	<h3 class="text-primary h4">Host multiple models which use different containers behind one endpoint</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Amazon SageMaker multi-container endpoints allow you to deploy multiple containers, each using different models or frameworks, on a single SageMaker endpoint. This approach offers flexibility in how you invoke these containers, either sequentially as an inference pipeline or individually through direct invocation.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;">Multiple containers can be deployed on a single endpoint</li> <li style="color: #333; font-size: 16px;">Containers can be run sequentially or invoked individually</li> <li style="color: #333; font-size: 16px;">Improves endpoint utilization and optimizes costs</li> <li style="color: #333; font-size: 16px;">Supports up to 15 containers on a multi-container endpoint</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating a Multi-Container Endpoint (Boto 3):</span></p> <p style="color: #333; font-size: 16px;">To create a multi-container endpoint, you need to call the <code>CreateModel</code>, <code>CreateEndpointConfig</code>, and <code>CreateEndpoint</code> APIs. Here's an example of creating a multi-container model for direct invocation:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
# Create container elements and InferenceExecutionConfig with direct invocation 
container1 = { 'Image': '123456789012.dkr.ecr.us-east-1.amazonaws.com/myimage1:mytag', 
    'ContainerHostname': 'firstContainer' }
container2 = { 'Image': '123456789012.dkr.ecr.us-east-1.amazonaws.com/myimage2:mytag', 
    'ContainerHostname': 'secondContainer' }

inferenceExecutionConfig = {'Mode': 'Direct'}

# Create the model
import boto3 sm_client = boto3.Session().client('sagemaker')

response = sm_client.create_model( ModelName = 'my-direct-mode-model-name', 
    InferenceExecutionConfig = inferenceExecutionConfig, 
    ExecutionRoleArn = role, 
    Containers = [container1, container2] ) 
            </pre> 
        </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Requirements:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use the <code>Containers</code> parameter instead of <code>PrimaryContainer</code></li> <li style="color: #333; font-size: 16px;">Include more than one container in the <code>Containers</code> parameter</li> <li style="color: #333; font-size: 16px;">Set the <code>ContainerHostname</code> parameter for each container</li> <li style="color: #333; font-size: 16px;">Set the <code>Mode</code> parameter in <code>InferenceExecutionConfig</code> to either <code>Direct</code> or <code>Serial</code></li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Updating a Multi-Container Endpoint:</span></p> <p style="color: #333; font-size: 16px;">To update a multi-container endpoint, you would typically create a new endpoint configuration and update the existing endpoint to use the new configuration. This process is similar to updating any other SageMaker endpoint.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deleting a Multi-Container Endpoint:</span></p> <p style="color: #333; font-size: 16px;">Deleting a multi-container endpoint follows the same process as deleting any other SageMaker endpoint. You would use the <code>DeleteEndpoint</code> API call.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Using a Multi-Container Endpoint with Direct Invocation:</span></p> <p style="color: #333; font-size: 16px;">When using direct invocation, you can specify which container to invoke by including the <code>TargetContainerHostname</code> parameter in your <code>InvokeEndpoint</code> request. This allows you to target a specific container within your multi-container endpoint.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices and Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Carefully plan your container structure and invocation strategy</li> <li style="color: #333; font-size: 16px;">Ensure that your containers are compatible if using them in a serial inference pipeline</li> <li style="color: #333; font-size: 16px;">Monitor the performance and resource utilization of your multi-container endpoint</li> <li style="color: #333; font-size: 16px;">Consider the trade-offs between using multiple single-container endpoints and a multi-container endpoint</li> <li style="color: #333; font-size: 16px;">Test thoroughly to ensure proper functionality and performance of all containers</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging multi-container endpoints in Amazon SageMaker, you can create more flexible and efficient deployment architectures, potentially reducing costs and improving resource utilization while maintaining the ability to use different models or frameworks within a single endpoint.</p>
            
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Host models along with pre-processing logic as serial inference pipeline behind one endpoint</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">An inference pipeline in Amazon SageMaker is a model composed of a linear sequence of two to fifteen containers that process requests for inferences on data. This feature allows you to combine preprocessing, predictions, and post-processing tasks in a single, managed pipeline.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;">Combines pretrained SageMaker built-in algorithms and custom algorithms</li> <li style="color: #333; font-size: 16px;">Supports SageMaker Spark ML Serving and scikit-learn containers</li> <li style="color: #333; font-size: 16px;">Handles invocations as a sequence of HTTP requests</li> <li style="color: #333; font-size: 16px;">Containers are co-located on the same EC2 instances for low latency</li> <li style="color: #333; font-size: 16px;">Supports real-time predictions and batch transforms</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating a Pipeline Model:</span></p> <p style="color: #333; font-size: 16px;">To create an inference pipeline, use the <code>CreateModel</code> operation or the SageMaker console. Instead of setting a single <code>PrimaryContainer</code>, use the <code>Containers</code> parameter to define the sequence of containers:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
import boto3
sm_client = boto3.client('sagemaker')

response = sm_client.create_model( ModelName='MyInferencePipeline', 
    ExecutionRoleArn='arn:aws:iam::123456789012:role/SageMakerRole', 
    Containers=[ { 
        'Image': '123456789012.dkr.ecr.us-west-2.amazonaws.com/mypreprocessor:latest', 
        'ModelDataUrl': 's3://mybucket/preprocessor-model.tar.gz' }, { 
        'Image': '123456789012.dkr.ecr.us-west-2.amazonaws.com/myclassifier:latest', 
        'ModelDataUrl': 's3://mybucket/classifier-model.tar.gz' } ] ) 
                </pre> 
        </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Running Real-time Predictions:</span></p> <p style="color: #333; font-size: 16px;">To run real-time predictions with an inference pipeline, deploy the pipeline model to an endpoint and use the <code>InvokeEndpoint</code> API:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> runtime_client = boto3.client('sagemaker-runtime')
                response = runtime_client.invoke_endpoint( EndpointName='MyInferencePipelineEndpoint', ContentType='text/csv', Body='1,2,3,4,5' )
                
                result = response['Body'].read().decode() print(result) </pre> </td>
                
                </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Running Batch Transforms:</span></p> <p style="color: #333; font-size: 16px;">To run batch transforms with an inference pipeline, use the <code>CreateTransformJob</code> API:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> response = sm_client.create_transform_job( TransformJobName='MyBatchTransformJob', ModelName='MyInferencePipeline', TransformInput={ 'DataSource': { 'S3DataSource': { 'S3DataType': 'S3Prefix', 'S3Uri': 's3://mybucket/input-data/' } }, 'ContentType': 'text/csv', 'SplitType': 'Line' }, TransformOutput={ 'S3OutputPath': 's3://mybucket/output-data/' }, TransformResources={ 'InstanceType': 'ml.m5.xlarge', 'InstanceCount': 1 } ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Logging and Metrics:</span></p> <p style="color: #333; font-size: 16px;">SageMaker provides logs and metrics for inference pipelines through Amazon CloudWatch. Each container in the pipeline has its own log stream, and you can monitor metrics such as invocations, latency, and errors for the entire pipeline.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Troubleshooting:</span></p> <ul> <li style="color: #333; font-size: 16px;">Ensure all containers in the pipeline are compatible in terms of input/output formats</li> <li style="color: #333; font-size: 16px;">Check individual container logs for specific error messages</li> <li style="color: #333; font-size: 16px;">Verify that the IAM role has necessary permissions for all operations</li> <li style="color: #333; font-size: 16px;">Test each container individually before combining them in a pipeline</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Sample Notebooks:</span></p> <p style="color: #333; font-size: 16px;">SageMaker provides sample notebooks demonstrating inference pipelines:</p> <ul> <li style="color: #333; font-size: 16px;">"Inference Pipeline with Scikit-learn and Linear Learner" in the advanced_functionality folder</li> <li style="color: #333; font-size: 16px;">Additional inference pipeline notebooks in the sagemaker-python-sdk folder</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Design your pipeline to minimize data transfer between containers</li> <li style="color: #333; font-size: 16px;">Consider using SageMaker Spark ML Serving for complex preprocessing tasks</li> <li style="color: #333; font-size: 16px;">Optimize each container for performance and resource utilization</li> <li style="color: #333; font-size: 16px;">Use A/B testing to compare different pipeline configurations</li> <li style="color: #333; font-size: 16px;">Regularly update and test your inference pipeline to ensure optimal performance</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging inference pipelines in Amazon SageMaker, you can create sophisticated, end-to-end machine learning workflows that combine preprocessing, prediction, and postprocessing steps in a single, managed endpoint or batch transform job.</p>
                
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Automatically Scale Amazon SageMaker Models</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Amazon SageMaker supports automatic scaling (auto scaling) for hosted models, dynamically adjusting the number of instances in response to workload changes. This feature helps optimize costs and performance by scaling resources up or down as needed.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;">Auto scaling works with existing SageMaker model endpoints</li> <li style="color: #333; font-size: 16px;">Multiple model versions (production variants) can exist for the same endpoint</li> <li style="color: #333; font-size: 16px;">Scaling can be configured via the SageMaker console, AWS CLI, or AWS SDKs</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Scaling Policy Overview:</span></p> <p style="color: #333; font-size: 16px;">Two types of scaling policies are available:</p> <ol> <li style="color: #333; font-size: 16px;"><strong>Target Tracking Scaling (Recommended):</strong> Automatically adjusts based on a target metric value</li> <li style="color: #333; font-size: 16px;"><strong>Step Scaling:</strong> Provides more advanced configuration options</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating a Target Tracking Scaling Policy:</span></p> <p style="color: #333; font-size: 16px;">Specify the following:</p> <ul> <li style="color: #333; font-size: 16px;">Metric: CloudWatch metric to track (e.g., average invocations per instance)</li> <li style="color: #333; font-size: 16px;">Target value: Desired value for the metric</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Scheduled Scaling:</span></p> <p style="color: #333; font-size: 16px;">Create scheduled actions to perform scaling at specific times, either one-time or recurring.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Scaling Limits:</span></p> <ul> <li style="color: #333; font-size: 16px;">Minimum value: At least 1, and equal to or less than the maximum value</li> <li style="color: #333; font-size: 16px;">Maximum value: Equal to or greater than the minimum value (no upper limit enforced)</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Cooldown Period:</span></p> <p style="color: #333; font-size: 16px;">A cooldown period prevents over-scaling by slowing down subsequent scaling activities. Default is 300 seconds for both scale-in and scale-out activities.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Permissions:</span></p> <p style="color: #333; font-size: 16px;">Auto scaling requires specific IAM permissions. The <code>SagemakerFullAccessPolicy</code> includes all necessary permissions. If managing your own policy, include the following:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "sagemaker:DescribeEndpoint", "sagemaker:DescribeEndpointConfig", "sagemaker:UpdateEndpointWeightsAndCapacities" ], "Resource": "*" }, { "Effect": "Allow", "Action": [ "application-autoscaling:*" ], "Resource": "*" }, { "Effect": "Allow", "Action": "iam:CreateServiceLinkedRole", "Resource": "arn:aws:iam::*:role/aws-service-role/sagemaker.application-autoscaling.amazonaws.com/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint", "Condition": { "StringLike": { "iam:AWSServiceName": "sagemaker.application-autoscaling.amazonaws.com" } } }, { "Effect": "Allow", "Action": [ "cloudwatch:PutMetricAlarm", "cloudwatch:DescribeAlarms", "cloudwatch:DeleteAlarms" ], "Resource": "*" } ] } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Service-Linked Role:</span></p> <p style="color: #333; font-size: 16px;">Auto scaling uses the <code>AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint</code> service-linked role, which is created automatically if you have the <code>iam:CreateServiceLinkedRole</code> permission.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Test your auto scaling configuration with expected traffic patterns</li> <li style="color: #333; font-size: 16px;">Adjust cooldown periods based on your model's traffic patterns</li> <li style="color: #333; font-size: 16px;">Use target tracking scaling policies for most use cases</li> <li style="color: #333; font-size: 16px;">Monitor CloudWatch metrics to ensure optimal scaling behavior</li> <li style="color: #333; font-size: 16px;">Regularly review and update your scaling policies as your workload changes</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">New Inference Capabilities:</span></p> <p style="color: #333; font-size: 16px;">SageMaker has introduced new inference capabilities built on real-time inference endpoints, including inference components. These new features can help reduce deployment costs and latency for foundation models.</p> <p style="color: #333; font-size: 16px;">By leveraging auto scaling in Amazon SageMaker, you can optimize your model deployment for both performance and cost-efficiency, ensuring that your endpoints can handle varying workloads while minimizing unnecessary resource usage.</p>
            
            <pre>
Endpoint:
Type: "AWS::SageMaker::Endpoint"
Properties:
    EndpointName: yourEndpointName
    EndpointConfigName: yourEndpointConfigName

ScalingTarget:
Type: "AWS::ApplicationAutoScaling::ScalableTarget"
Properties:
    MaxCapacity: 10
    MinCapacity: 2
    ResourceId: endpoint/my-endpoint/variant/my-variant
    RoleARN: arn
    ScalableDimension: sagemaker:variant:DesiredInstanceCount
    ServiceNamespace: sagemaker

ScalingPolicy:
Type: "AWS::ApplicationAutoScaling::ScalingPolicy"
Properties:
    PolicyName: my-scaling-policy
    PolicyType: TargetTrackingScaling
    ScalingTargetId:
    Ref: ScalingTarget
    TargetTrackingScalingPolicyConfiguration:
    TargetValue: 70.0
    ScaleInCooldown: 600
    ScaleOutCooldown: 30
    PredefinedMetricSpecification:
        PredefinedMetricType: SageMakerVariantInvocationsPerInstance
            </pre>
		</div>
	</div>
	
	<br/>
	
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Safely validate models in production</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker provides powerful features for safely validating models in production environments. This is achieved through the use of variants behind endpoints, allowing for comparison and testing of different models or model versions.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;"><strong>Variants:</strong> Consist of an ML instance and serving components specified in a SageMaker model</li> <li style="color: #333; font-size: 16px;"><strong>Production Variants:</strong> Actively serve inference requests</li> <li style="color: #333; font-size: 16px;"><strong>Shadow Variants:</strong> Receive replicated requests but don't return responses to callers</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Production Variants:</span></p> <p style="color: #333; font-size: 16px;">Production variants allow you to compare and test different models, instances, and containers. There are two main methods for testing with production variants:</p> <ol> <li style="color: #333; font-size: 16px;"><strong>Testing by Specifying Traffic Distribution:</strong></li> <ul> <li style="color: #333; font-size: 16px;">Distribute endpoint invocation requests across multiple variants</li> <li style="color: #333; font-size: 16px;">Specify the percentage of traffic for each variant in the endpoint configuration</li> <li style="color: #333; font-size: 16px;">Use the <code>CreateEndpointConfig</code> API to set up traffic distribution</li> </ul> <li style="color: #333; font-size: 16px;"><strong>Testing by Invoking Specific Variants:</strong></li> <ul> <li style="color: #333; font-size: 16px;">Specify the model version to invoke for each request</li> <li style="color: #333; font-size: 16px;">Use the <code>TargetVariant</code> parameter when calling <code>InvokeEndpoint</code></li> <li style="color: #333; font-size: 16px;">Targeted routing overrides random traffic distribution if both are specified</li> </ul> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Model A/B Testing Example:</span></p> <p style="color: #333; font-size: 16px;">A/B testing is an effective final step in validating a new model:</p> <ul> <li style="color: #333; font-size: 16px;">Test different variants of your models with production traffic</li> <li style="color: #333; font-size: 16px;">Compare performance between new and old model versions</li> <li style="color: #333; font-size: 16px;">Replace the old version with the new version if performance improves</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Shadow Variants:</span></p> <p style="color: #333; font-size: 16px;">SageMaker Model Shadow Deployments allow for long-running shadow variants to validate new components of your model serving stack:</p> <ul> <li style="color: #333; font-size: 16px;">Receive replicated requests from production variants</li> <li style="color: #333; font-size: 16px;">Responses are logged for comparison but not returned to callers</li> <li style="color: #333; font-size: 16px;">Allows testing of new models without exposing callers to potentially inferior results</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices for Safe Model Validation:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Gradual Rollout:</strong> Start with a small percentage of traffic to the new variant and gradually increase it</li> <li style="color: #333; font-size: 16px;"><strong>Monitor Key Metrics:</strong> Track performance, latency, and error rates for all variants</li> <li style="color: #333; font-size: 16px;"><strong>Use Shadow Variants:</strong> Test new models without impacting production traffic</li> <li style="color: #333; font-size: 16px;"><strong>Automated Rollback:</strong> Implement automated rollback mechanisms if the new variant underperforms</li> <li style="color: #333; font-size: 16px;"><strong>Comprehensive Logging:</strong> Ensure detailed logging for both production and shadow variants for post-deployment analysis</li> <li style="color: #333; font-size: 16px;"><strong>Statistically Significant Testing:</strong> Ensure your tests run long enough to gather statistically significant results</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Implementation Example:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 
                <pre style="margin: 0; white-space: pre-wrap;">
import boto3
client = boto3.client('sagemaker')

# Create endpoint configuration with multiple production variants
response = client.create_endpoint_config( 
    EndpointConfigName='MyEndpointConfig', 
    ProductionVariants=[ 
        { 'VariantName': 'VariantA', 'ModelName': 'ModelA', 'InitialInstanceCount': 1, 
            'InstanceType': 'ml.t2.medium', 'InitialVariantWeight': 0.5 }, 
        { 'VariantName': 'VariantB', 'ModelName': 'ModelB', 'InitialInstanceCount': 1, 
            'InstanceType': 'ml.t2.medium', 'InitialVariantWeight': 0.5 } ] )

# Create endpoint
response = client.create_endpoint( EndpointName='MyEndpoint', EndpointConfigName='MyEndpointConfig' )

Invoke endpoint with specific variant
runtime = boto3.client('sagemaker-runtime') 
response = runtime.invoke_endpoint( EndpointName='MyEndpoint', ContentType='text/csv', Body='1,2,3,4,5', TargetVariant='VariantA' ) </pre> 
</td>

</tr> </table> <p style="color: #333; font-size: 16px;">By leveraging these features in Amazon SageMaker, you can safely validate and deploy new models or model versions in production environments, ensuring optimal performance and reliability of your machine learning applications.</p>

            
		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Online Explainability with SageMaker Clarify</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker Clarify provides online explainability for real-time inference endpoints, allowing continuous analysis of model predictions. This feature is part of the "Deploy to production" stage in the SageMaker Machine Learning workflow.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How Clarify Online Explainability Works:</span></p> <ol> <li style="color: #333; font-size: 16px;">Application sends an <code>InvokeEndpoint</code> request to the SageMaker Runtime Service</li> <li style="color: #333; font-size: 16px;">Service routes the request to a SageMaker endpoint</li> <li style="color: #333; font-size: 16px;">Endpoint returns predictions and explanations</li> <li style="color: #333; font-size: 16px;">Service sends the response back to the application</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Steps to Create an Endpoint with Online Explainability:</span></p> <ol> <li style="color: #333; font-size: 16px;">Pre-check model compatibility</li> <li style="color: #333; font-size: 16px;">Create endpoint configuration with Clarify explainer configuration</li> <li style="color: #333; font-size: 16px;">Create endpoint using the configuration</li> <li style="color: #333; font-size: 16px;">Invoke the endpoint for predictions and explanations</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Pre-checking the Model Container:</span></p> <p style="color: #333; font-size: 16px;">Ensure your model container inputs and outputs are compatible with Clarify's requirements:</p> <p style="color: #333; font-size: 16px;"><strong>Model Container Input:</strong></p> <ul> <li style="color: #333; font-size: 16px;">Supports CSV or JSON Lines format</li> <li style="color: #333; font-size: 16px;">CSV inputs should use MIME type: text/csv</li> <li style="color: #333; font-size: 16px;">Can support batch requests for efficiency</li> </ul> <p style="color: #333; font-size: 16px;"><strong>Model Container Output:</strong></p> <ul> <li style="color: #333; font-size: 16px;">Should be in CSV or JSON Lines dense format</li> <li style="color: #333; font-size: 16px;">Must include probabilities of input records</li> <li style="color: #333; font-size: 16px;">For regression and binary classification: single probability value</li> <li style="color: #333; font-size: 16px;">For multi-class problems: list of probabilities</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Model Container Validation:</span></p> <p style="color: #333; font-size: 16px;">Deploy your model to a SageMaker real-time inference endpoint and test it using the AWS CLI:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws sagemaker-runtime invoke-endpoint \ --endpoint-name $ENDPOINT_NAME \ --content-type $CONTENT_TYPE \ --accept $ACCEPT_TYPE \ --body $REQUEST_DATA \ $CLI_BINARY_FORMAT \ /dev/stderr 1>/dev/null </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Example Validations:</span></p> <p style="color: #333; font-size: 16px;"><strong>1. Single record request (CSV):</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws sagemaker-runtime invoke-endpoint \ --endpoint-name test-endpoint-sagemaker-xgboost-model \ --content-type text/csv \ --accept text/csv \ --body '1,2,3,4' \ /dev/stderr 1>/dev/null
Expected output: 0.6
</pre>
</td>

</tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. Multi-record request (CSV):</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws sagemaker-runtime invoke-endpoint \ --endpoint-name test-endpoint-sagemaker-xgboost-model \ --content-type text/csv \ --accept text/csv \ --body $'1,2,3,4\n5,6,7,8' \ /dev/stderr 1>/dev/null
Expected output: 0.6,0.3
</pre>
</td>

</tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. Multi-class model request (CSV):</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws sagemaker-runtime invoke-endpoint \ --endpoint-name test-endpoint-csv-1 \ --content-type text/csv \ --accept text/csv \ --body '1,2,3,4' \ /dev/stderr 1>/dev/null
Expected output: 0.1,0.6,0.3
</pre>
</td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Ensure your model container supports batch requests for efficiency</li> <li style="color: #333; font-size: 16px;">Validate both single record and multi-record requests</li> <li style="color: #333; font-size: 16px;">Test with different input and output formats (CSV and JSON Lines)</li> <li style="color: #333; font-size: 16px;">Verify that probabilities are correctly included in the output</li> <li style="color: #333; font-size: 16px;">Delete testing endpoints after validation is complete</li> </ul> <p style="color: #333; font-size: 16px;">By following these steps and best practices, you can ensure that your model is compatible with SageMaker Clarify's online explainability feature, allowing for real-time insights into your model's predictions in production environments.</p>

<hr />

<p style="color: #333; font-size: 16px;">This guide provides code examples for creating and invoking endpoints that use SageMaker Clarify online explainability, using the AWS SDK for Python. We'll cover examples for both tabular and text data.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Tabular Data Example:</span></p> <p style="color: #333; font-size: 16px;">This example uses a SageMaker model that accepts CSV data with four numerical features.</p> <p style="color: #333; font-size: 16px;"><strong>1. Configure the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_config_name = 'tabular_explainer_endpoint_config' response = sagemaker_client.create_endpoint_config( EndpointConfigName=endpoint_config_name, ProductionVariants=[{ 'VariantName': 'AllTraffic', 'ModelName': model_name, 'InitialInstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', }], ExplainerConfig={ 'ClarifyExplainerConfig': { 'ShapConfig': { 'ShapBaselineConfig': { 'ShapBaseline': '0,0,0,0', }, }, }, }, ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. Create the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_name = 'tabular_explainer_endpoint' response = sagemaker_client.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name, ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. Invoke the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> response = sagemaker_runtime_client.invoke_endpoint( EndpointName=endpoint_name, ContentType='text/csv', Accept='text/csv', Body='1,2,3,4', ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>4. Parse the response:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import codecs import json json.load(codecs.getreader('utf-8')(response['Body'])) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Text Data Example:</span></p> <p style="color: #333; font-size: 16px;">This example uses a SageMaker model that accepts CSV data with a single string feature.</p> <p style="color: #333; font-size: 16px;"><strong>1. Configure the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_config_name = 'text_explainer_endpoint_config' response = sagemaker_client.create_endpoint_config( EndpointConfigName=endpoint_config_name, ProductionVariants=[{ 'VariantName': 'AllTraffic', 'ModelName': model_name, 'InitialInstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', }], ExplainerConfig={ 'ClarifyExplainerConfig': { 'InferenceConfig': { 'FeatureTypes': ['text'], 'MaxRecordCount': 100, }, 'ShapConfig': { 'ShapBaselineConfig': { 'ShapBaseline': '"<MASK>"', }, 'TextConfig': { 'Granularity': 'token', 'Language': 'en', }, 'NumberOfSamples': 100, }, }, }, ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. Create the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_name = 'text_explainer_endpoint' response = sagemaker_client.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name, ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. Invoke the endpoint:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> response = sagemaker_runtime_client.invoke_endpoint( EndpointName=endpoint_name, ContentType='text/csv', Accept='text/csv', Body='"This is a good product"', ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">For tabular data, set <code>ShapBaseline</code> appropriately (not just zeros as in the example)</li> <li style="color: #333; font-size: 16px;">For text data, use <code>FeatureTypes</code> to identify text features</li> <li style="color: #333; font-size: 16px;">Use <code>TextConfig</code> to specify granularity and language for text analysis</li> <li style="color: #333; font-size: 16px;">Set <code>NumberOfSamples</code> to limit the size of the synthetic dataset</li> <li style="color: #333; font-size: 16px;">Use <code>MaxRecordCount</code> to stabilize performance</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Visualization:</span></p> <p style="color: #333; font-size: 16px;">Use visualization tools to interpret the returned explanations:</p> <ul> <li style="color: #333; font-size: 16px;">For tabular data, SHAP plots can show how each feature contributes to the prediction</li> <li style="color: #333; font-size: 16px;">For text data, tools like the captum visualization utility can highlight word importance</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Always check the endpoint status before invoking it</li> <li style="color: #333; font-size: 16px;">Handle errors and exceptions appropriately</li> <li style="color: #333; font-size: 16px;">For multi-model endpoints, use the <code>TargetModel</code> parameter in the request</li> <li style="color: #333; font-size: 16px;">Use <code>EnableExplanations</code> for on-demand explanations based on prediction thresholds</li> <li style="color: #333; font-size: 16px;">Regularly monitor and analyze the explanations to ensure model fairness and reliability</li> </ul> <p style="color: #333; font-size: 16px;">By following these examples and best practices, you can effectively implement and use SageMaker Clarify's online explainability feature in your machine learning workflows, providing real-time insights into your model's predictions.</p>



		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Deploy models with Amazon SageMaker Serverless Inference</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            <p style="color: #333; font-size: 16px;">Amazon SageMaker Serverless Inference is a purpose-built inference option that enables you to deploy and scale ML models without managing the underlying infrastructure. It's ideal for workloads with idle periods between traffic spurts and can tolerate cold starts.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">Automatic scaling based on traffic</li> <li style="color: #333; font-size: 16px;">Pay-per-use model for cost-effectiveness</li> <li style="color: #333; font-size: 16px;">Integration with AWS Lambda for high availability and fault tolerance</li> <li style="color: #333; font-size: 16px;">Optional Provisioned Concurrency for predictable performance</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How it Works:</span></p> <ol> <li style="color: #333; font-size: 16px;">Create a serverless endpoint</li> <li style="color: #333; font-size: 16px;">SageMaker provisions and manages compute resources</li> <li style="color: #333; font-size: 16px;">Make inference requests to the endpoint</li> <li style="color: #333; font-size: 16px;">SageMaker scales resources based on request traffic</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Container Support:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use SageMaker-provided containers or bring your own</li> <li style="color: #333; font-size: 16px;">Maximum container image size: 10 GB</li> <li style="color: #333; font-size: 16px;">Recommend creating only one worker in the container</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Memory Size:</span></p> <ul> <li style="color: #333; font-size: 16px;">Minimum: 1024 MB (1 GB)</li> <li style="color: #333; font-size: 16px;">Maximum: 6144 MB (6 GB)</li> <li style="color: #333; font-size: 16px;">Available sizes: 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, 6144 MB</li> <li style="color: #333; font-size: 16px;">5 GB of ephemeral disk storage available</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Concurrent Invocations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Total concurrency per region: 1000 or 500 (depending on the region)</li> <li style="color: #333; font-size: 16px;">Maximum concurrency for a single endpoint: 200</li> <li style="color: #333; font-size: 16px;">Maximum number of serverless endpoints per region: 50</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Minimizing Cold Starts:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use Provisioned Concurrency to keep endpoints warm</li> <li style="color: #333; font-size: 16px;">Monitor cold start time using the <code>OverheadLatency</code> CloudWatch metric</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Feature Exclusions:</span></p> <p style="color: #333; font-size: 16px;">Some features available for SageMaker Real-time Inference are not supported for Serverless Inference, including GPUs, AWS marketplace model packages, private Docker registries, Multi-Model Endpoints, VPC configuration, network isolation, data capture, multiple production variants, Model Monitor, and inference pipelines.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Getting Started:</span></p> <p style="color: #333; font-size: 16px;">You can create, update, describe, and delete a serverless endpoint using the SageMaker console, AWS SDKs, Amazon SageMaker Python SDK, and AWS CLI. For serverless endpoints with Provisioned Concurrency, you can use Application Auto Scaling to manage Provisioned Concurrency based on a target metric or a schedule.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Automatically Scaling Provisioned Concurrency:</span></p> <ol> <li style="color: #333; font-size: 16px;">Register a model</li> <li style="color: #333; font-size: 16px;">Define a scaling policy</li> <li style="color: #333; font-size: 16px;">Apply the scaling policy</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Example: Registering a Model (AWS CLI):</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> aws application-autoscaling register-scalable-target \ --service-namespace sagemaker \ --scalable-dimension sagemaker:variant:DesiredProvisionedConcurrency \ --resource-id endpoint/MyEndpoint/variant/MyVariant \ --min-capacity 1 \ --max-capacity 10 </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Choose memory size based on your model size and latency requirements</li> <li style="color: #333; font-size: 16px;">Use Provisioned Concurrency for predictable performance</li> <li style="color: #333; font-size: 16px;">Monitor and optimize your endpoints using CloudWatch metrics</li> <li style="color: #333; font-size: 16px;">Implement appropriate auto-scaling policies for cost-effective operations</li> <li style="color: #333; font-size: 16px;">Test your serverless endpoints thoroughly before production deployment</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging Amazon SageMaker Serverless Inference, you can deploy your machine learning models in a cost-effective, scalable manner without the overhead of managing infrastructure, making it easier to focus on your core machine learning tasks and applications.</p>
            
		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Asynchronous inference</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker Asynchronous Inference is a capability that allows you to queue incoming requests and process them asynchronously. It's ideal for requests with large payload sizes, long processing times, and near real-time latency requirements.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">Supports payloads up to 1GB</li> <li style="color: #333; font-size: 16px;">Processing times up to one hour</li> <li style="color: #333; font-size: 16px;">Cost-effective with autoscaling to zero when idle</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How It Works:</span></p> <ol> <li style="color: #333; font-size: 16px;">Place request payload in Amazon S3</li> <li style="color: #333; font-size: 16px;">Invoke endpoint with <code>InvokeEndpointAsync</code></li> <li style="color: #333; font-size: 16px;">SageMaker queues the request and returns an identifier</li> <li style="color: #333; font-size: 16px;">Results are placed in the specified S3 location</li> <li style="color: #333; font-size: 16px;">Optional notifications via Amazon SNS</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating an Asynchronous Inference Endpoint:</span></p> <p style="color: #333; font-size: 16px;">The process involves three main steps:</p> <ol> <li style="color: #333; font-size: 16px;">Create a model with <code>CreateModel</code></li> <li style="color: #333; font-size: 16px;">Create an endpoint configuration with <code>CreateEndpointConfig</code></li> <li style="color: #333; font-size: 16px;">Create an HTTPS endpoint with <code>CreateEndpoint</code></li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Example: Creating a Model</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import boto3
aws_region='<aws_region>' sagemaker_client = boto3.client('sagemaker', region_name=aws_region) sagemaker_role= "arn:aws:iam::<account>:role/*"

model_url = f"s3://{s3_bucket}/{model_s3_key}" container = image_uris.retrieve(region=aws_region, framework='xgboost', version='0.90-1')

create_model_response = sagemaker_client.create_model( ModelName = model_name, ExecutionRoleArn = sagemaker_role, PrimaryContainer = { 'Image': container, 'ModelDataUrl': model_url, }) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating an Endpoint Configuration:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> create_endpoint_config_response = sagemaker_client.create_endpoint_config( EndpointConfigName=endpoint_config_name, ProductionVariants=[ { "VariantName": "variant1", "ModelName": model_name, "InstanceType": "ml.m5.xlarge", "InitialInstanceCount": 1 } ], AsyncInferenceConfig={ "OutputConfig": { "S3OutputPath": f"s3://{s3_bucket}/{bucket_prefix}/output", "NotificationConfig": { "SuccessTopic": "arn:aws:sns:aws-region:account-id:topic-name", "ErrorTopic": "arn:aws:sns:aws-region:account-id:topic-name", } }, "ClientConfig": { "MaxConcurrentInvocationsPerInstance": 4 } } ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating an Endpoint:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> create_endpoint_response = sagemaker_client.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Invoking an Asynchronous Endpoint:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> sagemaker_runtime = boto3.client("sagemaker-runtime", region_name=<aws_region>)
response = sagemaker_runtime.invoke_endpoint_async( EndpointName=endpoint_name, InputLocation=input_location, InvocationTimeoutSeconds=3600 ) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use appropriate timeout values based on your model's processing time</li> <li style="color: #333; font-size: 16px;">Implement proper error handling and retries for asynchronous requests</li> <li style="color: #333; font-size: 16px;">Monitor your S3 buckets and SNS topics for results and notifications</li> <li style="color: #333; font-size: 16px;">Optimize your model for asynchronous processing to take full advantage of the capability</li> <li style="color: #333; font-size: 16px;">Use appropriate IAM roles and permissions for S3 and SNS access</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging Amazon SageMaker Asynchronous Inference, you can efficiently handle large-scale, time-consuming inference tasks while optimizing costs and resources. This capability is particularly useful for batch processing, large payload handling, and scenarios where near real-time responses are acceptable.</p>


<hr />

<p style="color: #333; font-size: 16px;">Monitoring asynchronous endpoints in Amazon SageMaker is crucial for ensuring optimal performance and troubleshooting issues. This guide covers monitoring using Amazon CloudWatch and checking prediction results.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Monitoring with CloudWatch:</span></p> <p style="color: #333; font-size: 16px;">Amazon CloudWatch collects raw data and processes it into readable, near real-time metrics. Metrics for asynchronous endpoints are in the AWS/SageMaker namespace.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Common Endpoint Metrics:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Metric Name</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Invocation4XXErrors</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Number of requests where the model returned a 4xx HTTP response code</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Invocation5XXErrors</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Number of requests where the model returned a 5xx HTTP response code</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ModelLatency</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Time taken by a model to respond as viewed from SageMaker</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Asynchronous Inference Endpoint Metrics:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Metric Name</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ApproximateBacklogSize</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Number of items in the queue for an endpoint</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">ApproximateAgeOfOldestRequest</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Age of the oldest request in the queue</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">InvocationsProcesssed</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Number of async invocations processed by the endpoint</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Logs:</span></p> <p style="color: #333; font-size: 16px;">In addition to model container logs, a new platform log for tracing and debugging inference requests is available under the Endpoint Log Group:</p> <pre style="color: #333; font-size: 16px; background-color: #f5f5f5; padding: 10px;"> /aws/sagemaker/Endpoints/[EndpointName] </pre> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Checking Prediction Results:</span></p> <p style="color: #333; font-size: 16px;">There are two main ways to check prediction results:</p> <ol> <li style="color: #333; font-size: 16px;">Amazon SNS Topics</li> <li style="color: #333; font-size: 16px;">Checking outputs in your Amazon S3 bucket</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Using Amazon SNS Topics:</span></p> <ol> <li style="color: #333; font-size: 16px;">Create an SNS topic</li> <li style="color: #333; font-size: 16px;">Subscribe to the topic</li> <li style="color: #333; font-size: 16px;">Confirm your subscription</li> <li style="color: #333; font-size: 16px;">Note the Amazon Resource Name (ARN) of the topic</li> <li style="color: #333; font-size: 16px;">Provide the ARN in the <code>AsyncInferenceConfig</code> field when creating an endpoint configuration</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Example of creating an endpoint configuration with SNS topics:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> sagemaker_client.create_endpoint_config( EndpointConfigName=<endpoint_config_name>, ProductionVariants=[...], AsyncInferenceConfig={ "OutputConfig": { "S3OutputPath": "s3://<bucket>/<output_directory>", "NotificationConfig": { "SuccessTopic": "arn:aws:sns:aws-region:account-id:topic-name", "ErrorTopic": "arn:aws:sns:aws-region:account-id:topic-name", } } } ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Checking Your S3 Bucket:</span></p> <p style="color: #333; font-size: 16px;">You can programmatically check for outputs in your S3 bucket using the SageMaker Python SDK:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import sagemaker import urllib, time from botocore.exceptions import ClientError
sagemaker_session = sagemaker.session.Session()

def get_output(output_location): output_url = urllib.parse.urlparse(output_location) bucket = output_url.netloc key = output_url.path[1:] while True: try: return sagemaker_session.read_s3_file( bucket=output_url.netloc, key_prefix=output_url.path[1:] ) except ClientError as e: if e.response['Error']['Code'] == 'NoSuchKey': print("waiting for output...") time.sleep(2) continue raise

output = get_output(output_location) print(f"Output: {output}") </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Regularly monitor CloudWatch metrics to identify performance issues</li> <li style="color: #333; font-size: 16px;">Set up alarms for critical metrics to get notified of potential problems</li> <li style="color: #333; font-size: 16px;">Use SNS topics for real-time notifications of inference results</li> <li style="color: #333; font-size: 16px;">Implement proper error handling when checking S3 for outputs</li> <li style="color: #333; font-size: 16px;">Ensure your IAM roles have the necessary permissions for SNS and S3 access</li> </ul> <p style="color: #333; font-size: 16px;">By effectively monitoring your asynchronous endpoints and checking prediction results, you can ensure the reliability and performance of your SageMaker asynchronous inference deployments.</p>

<hr />
<p style="color: #333; font-size: 16px;">Amazon SageMaker supports automatic scaling (autoscaling) for asynchronous endpoints, allowing you to dynamically adjust the number of instances in response to workload changes. Uniquely, asynchronous endpoints can scale down to zero instances, with requests queued for processing when the endpoint scales up again.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Steps to Autoscale an Asynchronous Endpoint:</span></p> <ol> <li style="color: #333; font-size: 16px;">Register a deployed model (production variant)</li> <li style="color: #333; font-size: 16px;">Define a scaling policy</li> <li style="color: #333; font-size: 16px;">Apply the autoscaling policy</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Defining a Scaling Policy:</span></p> <p style="color: #333; font-size: 16px;">For asynchronous endpoints, it's recommended to use a target-tracking scaling policy with a custom metric. Here's an example configuration:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> TargetTrackingScalingPolicyConfiguration={ 'TargetValue': 5.0, 'CustomizedMetricSpecification': { 'MetricName': 'ApproximateBacklogSizePerInstance', 'Namespace': 'AWS/SageMaker', 'Dimensions': [ {'Name': 'EndpointName', 'Value': <endpoint_name> } ], 'Statistic': 'Average', } } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Defining a Scaling Policy that Scales to Zero:</span></p> <p style="color: #333; font-size: 16px;">To enable scaling to zero instances, use the AWS SDK for Python (Boto3) to register your endpoint variant with application autoscaling:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> client = boto3.client('application-autoscaling')
resource_id = 'endpoint/' + <endpoint_name> + '/variant/' + <'variant1'>

response = client.register_scalable_target( ServiceNamespace='sagemaker', ResourceId=resource_id, ScalableDimension='sagemaker:variant:DesiredInstanceCount', MinCapacity=0, MaxCapacity=5 ) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Optional: Scaling Up from Zero for New Requests</span></p> <p style="color: #333; font-size: 16px;">To scale up from zero instances when new requests arrive (before the queue size exceeds the target), you can create an additional scaling policy:</p> <ol> <li style="color: #333; font-size: 16px;">Create a scaling policy:</li> </ol> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> response = client.put_scaling_policy( PolicyName="HasBacklogWithoutCapacity-ScalingPolicy", ServiceNamespace="sagemaker", ResourceId=resource_id, ScalableDimension="sagemaker:variant:DesiredInstanceCount", PolicyType="StepScaling", StepScalingPolicyConfiguration={ "AdjustmentType": "ChangeInCapacity", "MetricAggregationType": "Average", "Cooldown": 300, "StepAdjustments": [ { "MetricIntervalLowerBound": 0, "ScalingAdjustment": 1 } ] }, ) </pre> </td> </tr> </table> <ol start="2"> <li style="color: #333; font-size: 16px;">Create a CloudWatch alarm:</li> </ol> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> response = cw_client.put_metric_alarm( AlarmName=step_scaling_policy_alarm_name, MetricName='HasBacklogWithoutCapacity', Namespace='AWS/SageMaker', Statistic='Average', EvaluationPeriods=2, DatapointsToAlarm=2, Threshold=1, ComparisonOperator='GreaterThanOrEqualToThreshold', TreatMissingData='missing', Dimensions=[ { 'Name':'EndpointName', 'Value':endpoint_name }, ], Period=60, AlarmActions=[step_scaling_policy_arn] ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices and Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Choose appropriate metrics for autoscaling, such as <code>ApproximateBacklogSize</code> or <code>InvocationsPerInstance</code></li> <li style="color: #333; font-size: 16px;">Monitor instance-level metrics like CPU/GPU utilization to determine when to scale out</li> <li style="color: #333; font-size: 16px;">Avoid using multiple scaling policies as they can conflict and cause delays</li> <li style="color: #333; font-size: 16px;">Ensure your container can handle concurrent ping and invoke requests</li> <li style="color: #333; font-size: 16px;">Use Amazon SNS for tracking success and failures of invocation requests</li> <li style="color: #333; font-size: 16px;">Be aware of instance type support for Asynchronous Inference in your region</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Troubleshooting Tips:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use <code>DescribeEndpoint</code> API or CloudWatch metrics to check instance count</li> <li style="color: #333; font-size: 16px;">Review container logs and CPU/Memory utilization metrics for model server errors</li> <li style="color: #333; font-size: 16px;">Check if <code>MaxConcurrentInvocationsPerInstance</code> is set correctly for your container</li> <li style="color: #333; font-size: 16px;">Use the <code>InvocationsProcesssed</code> metric to verify single concurrency settings</li> </ul> <p style="color: #333; font-size: 16px;">By implementing these autoscaling strategies for your asynchronous endpoints, you can optimize resource utilization and cost-efficiency while ensuring your endpoint can handle varying workloads effectively.</p>


		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Use batch transform to run inference with Amazon SageMaker</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker's batch transform feature allows you to run inference on large datasets, preprocess data, and perform inferences without needing a persistent endpoint. It's particularly useful for scenarios where you need to associate input records with inferences.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Use Cases:</span></p> <ul> <li style="color: #333; font-size: 16px;">Preprocess datasets to remove noise or bias</li> <li style="color: #333; font-size: 16px;">Get inferences from large datasets</li> <li style="color: #333; font-size: 16px;">Run inference without a persistent endpoint</li> <li style="color: #333; font-size: 16px;">Associate input records with inferences</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How Batch Transform Works:</span></p> <ol> <li style="color: #333; font-size: 16px;">SageMaker starts compute instances and distributes the workload</li> <li style="color: #333; font-size: 16px;">Input files are partitioned and mapped to instances</li> <li style="color: #333; font-size: 16px;">Files can be split into mini-batches for processing</li> <li style="color: #333; font-size: 16px;">Output files are created with the same name and a .out extension</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Parameters:</span></p> <ul> <li style="color: #333; font-size: 16px;"><code>SplitType</code>: Set to 'Line' to split input files into mini-batches</li> <li style="color: #333; font-size: 16px;"><code>BatchStrategy</code> and <code>MaxPayloadInMB</code>: Control mini-batch size</li> <li style="color: #333; font-size: 16px;"><code>MaxConcurrentTransforms</code>: Limits concurrent processing</li> <li style="color: #333; font-size: 16px;"><code>AssembleWith</code>: Determines how output records are concatenated</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Speeding Up Batch Transform Jobs:</span></p> <ul> <li style="color: #333; font-size: 16px;">Optimize <code>MaxPayloadInMB</code>, <code>MaxConcurrentTransforms</code>, and <code>BatchStrategy</code></li> <li style="color: #333; font-size: 16px;">Set <code>MaxConcurrentTransforms</code> equal to the number of compute workers</li> <li style="color: #333; font-size: 16px;">Use the SageMaker console's "Additional configuration" section for built-in algorithms</li> <li style="color: #333; font-size: 16px;">Provide optimal values through an execution-parameters endpoint for custom algorithms</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Testing Production Variants:</span></p> <ol> <li style="color: #333; font-size: 16px;">Create separate transform jobs for each model variant</li> <li style="color: #333; font-size: 16px;">Use a validation dataset for testing</li> <li style="color: #333; font-size: 16px;">Specify unique model names and S3 locations for output files</li> <li style="color: #333; font-size: 16px;">Analyze results using Inference Pipeline Logs and Metrics</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Sample Code for Creating a Batch Transform Job:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import boto3
sagemaker = boto3.client('sagemaker')

response = sagemaker.create_transform_job( TransformJobName='my-transform-job', ModelName='my-model', MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy='MultiRecord', TransformInput={ 'DataSource': { 'S3DataSource': { 'S3DataType': 'S3Prefix', 'S3Uri': 's3://my-bucket/input-data/' } }, 'ContentType': 'text/csv', 'SplitType': 'Line' }, TransformOutput={ 'S3OutputPath': 's3://my-bucket/output-data/' }, TransformResources={ 'InstanceType': 'ml.m4.xlarge', 'InstanceCount': 1 } ) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use appropriate <code>SplitType</code> and <code>AssembleWith</code> settings for your data</li> <li style="color: #333; font-size: 16px;">Monitor and optimize resource utilization with CloudWatch metrics</li> <li style="color: #333; font-size: 16px;">Use streaming for very large datasets by setting <code>MaxPayloadInMB</code> to 0</li> <li style="color: #333; font-size: 16px;">Test different configurations to find the optimal performance for your use case</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Sample Notebooks:</span></p> <p style="color: #333; font-size: 16px;">Explore the "Batch Transform with PCA and DBSCAN Movie Clusters" notebook for a practical example of using batch transform with PCA and DBSCAN algorithms.</p> <p style="color: #333; font-size: 16px;">By leveraging batch transform in Amazon SageMaker, you can efficiently process large datasets, test different model variants, and perform inferences without the need for persistent endpoints, providing flexibility and cost-effectiveness in your machine learning workflows.</p>

<hr />

<p style="color: #333; font-size: 16px;">Amazon SageMaker Batch Transform allows you to associate prediction results with input records, enabling you to exclude unnecessary attributes for prediction and later combine them with the results. This feature is particularly useful for data processing and reporting.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Workflow for Associating Inferences with Input Records:</span></p> <ol> <li style="color: #333; font-size: 16px;">Filter input data not needed for inference</li> <li style="color: #333; font-size: 16px;">Associate input data with inference results</li> <li style="color: #333; font-size: 16px;">Filter joined data to retain necessary inputs for context</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Parameters in CreateTransformJob:</span></p> <ul> <li style="color: #333; font-size: 16px;"><code>InputFilter</code>: Specifies which portion of the input to pass to the model</li> <li style="color: #333; font-size: 16px;"><code>JoinSource</code>: Joins raw input data with transformed data</li> <li style="color: #333; font-size: 16px;"><code>OutputFilter</code>: Specifies which portion of the joined data to include in the output</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Supported File Formats:</span></p> <ul> <li style="color: #333; font-size: 16px;">JSON and JSON Lines</li> <li style="color: #333; font-size: 16px;">CSV</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Supported JSONPath Operators:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Operator</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Example</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">$</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Root element</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">$</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">.<name></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Dot-notated child element</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">$.id</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">*</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Wildcard</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">$.id.*</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">['<name>']</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Bracket-notated element(s)</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">$['id','SageMakerOutput']</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">[<number>]</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Index or array of indexes</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">$[1], $[1,3,5]</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">[<start>:<end>]</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Array slice operator</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">$[2:5], $[:5], $[2:]</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Examples:</span></p> <p style="color: #333; font-size: 16px;"><strong>1. Output Only Inferences:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> sm_transformer = sagemaker.transformer.Transformer() sm_transformer.transform(, input_filter="$", join_source="None", output_filter="$") </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. Output Inferences Joined with Input Data:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> sm_transformer = sagemaker.transformer.Transformer(, assemble_with="Line", accept="text/csv") sm_transformer.transform(, join_source="Input", split_type="Line", content_type="text/csv") </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. Output Inferences Joined with Input Data, Excluding ID Column (CSV):</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> sm_transformer = sagemaker.transformer.Transformer(, assemble_with="Line", accept="text/csv") sm_transformer.transform(, split_type="Line", content_type="text/csv", input_filter="$[1:]", join_source="Input") </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>4. Output Inferences Joined with ID Column, Excluding ID from Input (CSV):</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> sm_transformer = sagemaker.transformer.Transformer(, assemble_with="Line", accept="text/csv") sm_transformer.transform(, split_type="Line", content_type="text/csv", input_filter="$[1:]", join_source="Input", output_filter="$[0,-1]") </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices and Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Ensure input and output formats are consistent (CSV or JSON)</li> <li style="color: #333; font-size: 16px;">Be cautious with the <code>SageMakerOutput</code> attribute name in JSON inputs</li> <li style="color: #333; font-size: 16px;">Use appropriate JSONPath expressions for filtering and joining data</li> <li style="color: #333; font-size: 16px;">Consider storage implications and encryption options for your models</li> <li style="color: #333; font-size: 16px;">Handle incomplete outputs and failed jobs appropriately</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Troubleshooting Tips:</span></p> <ul> <li style="color: #333; font-size: 16px;">For max timeout errors, start with single-record BatchStrategy and small batch sizes</li> <li style="color: #333; font-size: 16px;">Tune <code>InvocationsTimeoutInSeconds</code> and <code>MaxPayloadInMB</code> parameters</li> <li style="color: #333; font-size: 16px;">Use S3 bucket policies to manage incomplete multipart uploads</li> <li style="color: #333; font-size: 16px;">For custom algorithms, consider using placeholder text for bad records</li> </ul> <p style="color: #333; font-size: 16px;">By effectively using these features and best practices, you can create more efficient and flexible batch transform jobs in Amazon SageMaker, enabling better data processing and analysis workflows.</p>

            
		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Update models in production</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker Inference offers deployment guardrails, a set of model deployment options to update machine learning models in production. These guardrails provide controlled switching from the current production model to a new one, with features like traffic shifting modes and auto-rollbacks.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Benefits of Deployment Guardrails:</span></p> <ul> <li style="color: #333; font-size: 16px;">Enhanced deployment safety in production environments</li> <li style="color: #333; font-size: 16px;">Fully managed deployment process</li> <li style="color: #333; font-size: 16px;">Increased visibility into deployment progress</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Types of Deployments:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Blue/Green Deployments</strong></li> <li style="color: #333; font-size: 16px;"><strong>Rolling Deployments</strong></li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Blue/Green Deployments:</span></p> <p style="color: #333; font-size: 16px;">This deployment type shifts traffic from the old fleet (blue) to a new fleet (green) with updates. It offers three traffic shifting modes:</p> <ol> <li style="color: #333; font-size: 16px;"><strong>All At Once Traffic Shifting:</strong> Shifts all traffic from blue to green fleet at once.</li> <li style="color: #333; font-size: 16px;"><strong>Canary Traffic Shifting:</strong> Shifts a small portion of traffic to the green fleet, monitors it, then shifts the rest if successful.</li> <li style="color: #333; font-size: 16px;"><strong>Linear Traffic Shifting:</strong> Provides customizable, multi-step traffic shifting.</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Rolling Deployments:</span></p> <p style="color: #333; font-size: 16px;">This type updates the endpoint by incrementally provisioning capacity and shifting traffic to a new fleet in specified batch sizes.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Implementation:</span></p> <p style="color: #333; font-size: 16px;">You can create and manage deployments using the <code>UpdateEndpoint</code> and <code>CreateEndpoint</code> SageMaker API and AWS CLI commands.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Example: Canary Traffic Shifting</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import boto3
client = boto3.client('sagemaker')

response = client.update_endpoint( EndpointName='your-endpoint-name', EndpointConfigName='your-new-endpoint-config-name', DeploymentConfig={ 'BlueGreenUpdatePolicy': { 'TrafficRoutingConfiguration': { 'Type': 'CANARY', 'CanarySize': { 'Type': 'PERCENT', 'Value': 10 }, 'WaitIntervalInSeconds': 300 }, 'TerminationWaitInSeconds': 600, 'MaximumExecutionTimeoutInSeconds': 1800 }, 'AutoRollbackConfiguration': { 'Alarms': [ { 'AlarmName': 'your-cloudwatch-alarm-name' } ] } } ) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Components:</span></p> <ul> <li style="color: #333; font-size: 16px;"><strong>TrafficRoutingConfiguration:</strong> Specifies the traffic shifting mode and parameters</li> <li style="color: #333; font-size: 16px;"><strong>TerminationWaitInSeconds:</strong> Time to wait before terminating the old fleet</li> <li style="color: #333; font-size: 16px;"><strong>MaximumExecutionTimeoutInSeconds:</strong> Maximum time for the entire deployment</li> <li style="color: #333; font-size: 16px;"><strong>AutoRollbackConfiguration:</strong> Specifies CloudWatch alarms for auto-rollback</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Start with canary deployments for safer initial testing</li> <li style="color: #333; font-size: 16px;">Use appropriate CloudWatch alarms to monitor deployment health</li> <li style="color: #333; font-size: 16px;">Adjust baking periods based on your application's characteristics</li> <li style="color: #333; font-size: 16px;">Regularly review and update your deployment strategies</li> <li style="color: #333; font-size: 16px;">Use the DescribeEndpoint API to track deployment progress</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Limitations and Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Deployment guardrails only apply to Asynchronous inference and Real-time inference endpoint types</li> <li style="color: #333; font-size: 16px;">Some SageMaker features may be incompatible with deployment guardrails (check the Exclusions page)</li> <li style="color: #333; font-size: 16px;">Ensure your application can handle potential inconsistencies during the deployment process</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Additional Resources:</span></p> <ul> <li style="color: #333; font-size: 16px;">Refer to the SageMaker documentation for detailed API references</li> <li style="color: #333; font-size: 16px;">Explore example Jupyter notebooks for guided implementations of canary and linear traffic shifting modes</li> <li style="color: #333; font-size: 16px;">Use Amazon CloudWatch Events to track endpoint deployment state changes</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging SageMaker's deployment guardrails, you can significantly reduce the risks associated with updating models in production, ensuring a smoother and more controlled deployment process for your machine learning applications.</p>
            
<hr />

<p style="color: #333; font-size: 16px;">Auto-rollback functionality in Amazon SageMaker's deployment guardrails relies on Amazon CloudWatch alarms to monitor endpoints during deployment. If any alarm trips during the specified monitoring period, SageMaker automatically initiates a rollback to the old endpoint to protect your application.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Points:</span></p> <ul> <li style="color: #333; font-size: 16px;">CloudWatch alarms are prerequisite for using baking periods in deployment guardrails</li> <li style="color: #333; font-size: 16px;">Auto-rollback only works if CloudWatch alarms are set up to monitor the endpoint</li> <li style="color: #333; font-size: 16px;">IAM execution role must have permission to perform <code>cloudwatch:DescribeAlarms</code> action</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Alarm Examples:</span></p> <p style="color: #333; font-size: 16px;">1. Monitor invocation errors on both old and new fleets:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "AlarmName": "EndToEndDeploymentHighErrorRateAlarm", "AlarmDescription": "Monitors the error rate of 4xx errors", "MetricName": "Invocation4XXErrors", "Namespace": "AWS/SageMaker", "Statistic": "Average", "Dimensions": [ { "Name": "EndpointName", "Value": <your-endpoint-name> }, { "Name": "VariantName", "Value": "AllTraffic" } ], "Period": 600, "EvaluationPeriods": 2, "Threshold": 1, "ComparisonOperator": "GreaterThanThreshold", "TreatMissingData": "notBreaching" } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;">Key components of this alarm:</p> <ul> <li style="color: #333; font-size: 16px;"><code>MetricName</code>: "Invocation4XXErrors" monitors 4xx errors</li> <li style="color: #333; font-size: 16px;"><code>Namespace</code>: "AWS/SageMaker" for SageMaker metrics</li> <li style="color: #333; font-size: 16px;"><code>Dimensions</code>: Specifies the endpoint and "AllTraffic" for the variant</li> <li style="color: #333; font-size: 16px;"><code>Period</code>: 600 seconds (10 minutes) evaluation period</li> <li style="color: #333; font-size: 16px;"><code>EvaluationPeriods</code>: 2 periods considered for alarm status</li> </ul> <p style="color: #333; font-size: 16px;">2. Monitor model latency on the new fleet:</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> { "AlarmName": "NewEndpointConfigVersionHighModelLatencyAlarm", "AlarmDescription": "Monitors the model latency on new fleet", "MetricName": "ModelLatency", "Namespace": "AWS/SageMaker", "Statistic": "Average", "Dimensions": [ { "Name": "EndpointName", "Value": <your-endpoint-name> }, { "Name": "VariantName", "Value": "AllTraffic" }, { "Name": "EndpointConfigName", "Value": <your-config-name> } ], "Period": 300, "EvaluationPeriods": 2, "Threshold": 100000, # 100ms "ComparisonOperator": "GreaterThanThreshold", "TreatMissingData": "notBreaching" } </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;">Key components of this alarm:</p> <ul> <li style="color: #333; font-size: 16px;"><code>MetricName</code>: "ModelLatency" monitors model response time</li> <li style="color: #333; font-size: 16px;"><code>Dimensions</code>: Includes "EndpointConfigName" to specify the new fleet</li> <li style="color: #333; font-size: 16px;"><code>Period</code>: 300 seconds (5 minutes) evaluation period</li> <li style="color: #333; font-size: 16px;"><code>Threshold</code>: 100000 milliseconds (100ms) for model latency</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ol> <li style="color: #333; font-size: 16px;">Customize alarms based on your specific application requirements</li> <li style="color: #333; font-size: 16px;">Use multiple alarms to monitor different aspects of your deployment</li> <li style="color: #333; font-size: 16px;">Adjust thresholds based on historical performance data</li> <li style="color: #333; font-size: 16px;">Consider monitoring both old and new fleets separately</li> <li style="color: #333; font-size: 16px;">Regularly review and update your alarm configurations</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Implementation Steps:</span></p> <ol> <li style="color: #333; font-size: 16px;">Create CloudWatch alarms using the AWS Management Console, AWS CLI, or AWS SDKs</li> <li style="color: #333; font-size: 16px;">Ensure your IAM role has the necessary permissions</li> <li style="color: #333; font-size: 16px;">Include the alarm ARNs in your deployment configuration</li> <li style="color: #333; font-size: 16px;">Test your alarms to ensure they trigger correctly</li> <li style="color: #333; font-size: 16px;">Monitor the deployment process and verify auto-rollback functionality</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Additional Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">You can create custom metrics for more specific monitoring needs</li> <li style="color: #333; font-size: 16px;">Consider using composite alarms for more complex monitoring scenarios</li> <li style="color: #333; font-size: 16px;">Integrate with other AWS services like AWS Lambda for advanced auto-rollback actions</li> <li style="color: #333; font-size: 16px;">Regularly review CloudWatch logs for deeper insights into your deployment process</li> </ul> <p style="color: #333; font-size: 16px;">By effectively configuring and monitoring auto-rollback alarms, you can significantly enhance the safety and reliability of your model deployments in Amazon SageMaker, ensuring quick response to any issues that may arise during the deployment process.</p>

<hr />

<p style="color: #333; font-size: 16px;">Blue/Green Deployments in Amazon SageMaker provide a safe and controlled way to update your endpoints while maximizing availability. This approach involves provisioning a new fleet with updates (green fleet) and gradually shifting traffic from the old fleet (blue fleet) to the new one.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">Traffic shifting modes</li> <li style="color: #333; font-size: 16px;">Baking period</li> <li style="color: #333; font-size: 16px;">Auto-rollbacks</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Traffic Shifting Modes:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Mode</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Pros</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Cons</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">All at once</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Shifts all traffic in one step</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Minimizes update duration</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Regressive updates affect all traffic</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Canary</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Shifts traffic in two steps</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Limits impact of regressive updates</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Both fleets run in parallel</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Linear</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Shifts traffic in multiple steps</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Minimizes risk of regressive updates</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Longer update duration and higher cost</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Implementation Steps:</span></p> <ol> <li style="color: #333; font-size: 16px;">Set up Amazon CloudWatch alarms for monitoring</li> <li style="color: #333; font-size: 16px;">Choose a traffic shifting mode</li> <li style="color: #333; font-size: 16px;">Configure the deployment using UpdateEndpoint API or AWS CLI</li> <li style="color: #333; font-size: 16px;">Monitor the deployment process</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Example: Canary Traffic Shifting (API)</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import boto3 client = boto3.client("sagemaker")
response = client.update_endpoint( EndpointName="<your-endpoint-name>", EndpointConfigName="<your-config-name>", DeploymentConfig={ "BlueGreenUpdatePolicy": { "TrafficRoutingConfiguration": { "Type": "CANARY", "CanarySize": { "Type": "CAPACITY_PERCENT", "Value": 30 }, "WaitIntervalInSeconds": 600 }, "TerminationWaitInSeconds": 600, "MaximumExecutionTimeoutInSeconds": 1800 }, "AutoRollbackConfiguration": { "Alarms": [ { "AlarmName": "<your-cw-alarm>" } ] } } ) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Configuration Parameters:</span></p> <ul> <li style="color: #333; font-size: 16px;"><code>Type</code>: Specifies the traffic shifting mode (ALL_AT_ONCE, CANARY, or LINEAR)</li> <li style="color: #333; font-size: 16px;"><code>CanarySize</code> or <code>LinearStepSize</code>: Defines the portion of traffic to shift in each step</li> <li style="color: #333; font-size: 16px;"><code>WaitIntervalInSeconds</code>: Sets the duration of the baking period</li> <li style="color: #333; font-size: 16px;"><code>TerminationWaitInSeconds</code>: Specifies the wait time before terminating the old fleet</li> <li style="color: #333; font-size: 16px;"><code>MaximumExecutionTimeoutInSeconds</code>: Sets the maximum deployment duration</li> <li style="color: #333; font-size: 16px;"><code>AutoRollbackConfiguration</code>: Specifies CloudWatch alarms for monitoring</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Choose the appropriate traffic shifting mode based on your risk tolerance and update urgency</li> <li style="color: #333; font-size: 16px;">Set up comprehensive CloudWatch alarms to monitor critical metrics</li> <li style="color: #333; font-size: 16px;">Adjust baking periods based on your application's characteristics</li> <li style="color: #333; font-size: 16px;">Test your deployment configuration in a non-production environment first</li> <li style="color: #333; font-size: 16px;">Use the RetainDeploymentConfig option for consistent updates</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Blue/Green deployments with multiple stages or baking periods incur costs for both fleets</li> <li style="color: #333; font-size: 16px;">Ensure your IAM roles have necessary permissions for CloudWatch alarm actions</li> <li style="color: #333; font-size: 16px;">Be aware of any feature exclusions that may prevent the use of deployment guardrails</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging Blue/Green Deployments in Amazon SageMaker, you can significantly reduce the risks associated with updating models in production, ensuring a smooth transition while maintaining high availability and performance of your machine learning endpoints.</p>

<hr />

<p style="color: #333; font-size: 16px;">Rolling Deployments in Amazon SageMaker provide a gradual approach to updating your endpoint, allowing you to shift traffic from the old fleet to a new fleet in controlled batches. This method offers more granular control and reduced capacity requirements compared to blue/green deployments.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features:</span></p> <ul> <li style="color: #333; font-size: 16px;">Baking period for monitoring new instances</li> <li style="color: #333; font-size: 16px;">Configurable rolling batch size</li> <li style="color: #333; font-size: 16px;">Auto-rollbacks using CloudWatch alarms</li> <li style="color: #333; font-size: 16px;">Reduced capacity requirements compared to blue/green deployments</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How It Works:</span></p> <ol> <li style="color: #333; font-size: 16px;">SageMaker provisions the first batch of new instances</li> <li style="color: #333; font-size: 16px;">Traffic shifts partially to the new instances</li> <li style="color: #333; font-size: 16px;">After the baking period, if no alarms trip, old instances are cleaned up</li> <li style="color: #333; font-size: 16px;">Process repeats in batches until deployment is complete</li> <li style="color: #333; font-size: 16px;">If an alarm trips, traffic rolls back to the old fleet</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Prerequisites:</span></p> <ul> <li style="color: #333; font-size: 16px;">Set up Amazon CloudWatch alarms for endpoint monitoring</li> <li style="color: #333; font-size: 16px;">Review exclusions to ensure your endpoint is compatible</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Determining Rolling Batch Size:</span></p> <ul> <li style="color: #333; font-size: 16px;">Batch size can be 5-50% of fleet capacity</li> <li style="color: #333; font-size: 16px;">Larger batch size: Faster deployment, higher capacity overhead</li> <li style="color: #333; font-size: 16px;">Smaller batch size: Longer deployment, lower capacity overhead</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Configuring a Rolling Deployment:</span></p> <p style="color: #333; font-size: 16px;">Use the <code>UpdateEndpoint</code> API or <code>update-endpoint</code> CLI command. Here's an example using the AWS SDK for Python (Boto3):</p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import boto3 client = boto3.client("sagemaker")
response = client.update_endpoint( EndpointName="<your-endpoint-name>", EndpointConfigName="<your-config-name>", DeploymentConfig={ "AutoRollbackConfiguration": { "Alarms": [ { "AlarmName": "<your-cw-alarm>" }, ] }, "RollingUpdatePolicy": { "MaximumExecutionTimeoutInSeconds": 28800, "WaitIntervalInSeconds": 600, "MaximumBatchSize": { "Type": "CAPACITY_PERCENTAGE", "Value": 25 }, "RollbackMaximumBatchSize": { "Type": "CAPACITY_PERCENTAGE", "Value": 25 }, }
} ) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Configuration Parameters:</span></p> <ul> <li style="color: #333; font-size: 16px;"><code>MaximumExecutionTimeoutInSeconds</code>: Total deployment time limit (max 28800 seconds)</li> <li style="color: #333; font-size: 16px;"><code>WaitIntervalInSeconds</code>: Baking period duration</li> <li style="color: #333; font-size: 16px;"><code>MaximumBatchSize</code>: Size of each update batch</li> <li style="color: #333; font-size: 16px;"><code>RollbackMaximumBatchSize</code>: Size of each rollback batch if an alarm trips</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Monitoring Deployment Status:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use the SageMaker console or <code>DescribeEndpoint</code> API</li> <li style="color: #333; font-size: 16px;">Check the <code>Status</code> field in the <code>VariantStatus</code> object</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Failure Handling:</span></p> <ul> <li style="color: #333; font-size: 16px;">If deployment and auto-rollback fail, endpoint status may be <code>UpdateRollbackFailed</code></li> <li style="color: #333; font-size: 16px;">Make another call to <code>UpdateEndpoint</code> to return to a healthy state</li> <li style="color: #333; font-size: 16px;">Specify desired endpoint and deployment configuration</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Exclusions:</span></p> <p style="color: #333; font-size: 16px;">The following features are incompatible with deployment guardrails:</p> <ul> <li style="color: #333; font-size: 16px;">Marketplace containers</li> <li style="color: #333; font-size: 16px;">Endpoints using Inf1 (Inferentia-based) instances</li> <li style="color: #333; font-size: 16px;">Amazon Elastic Inference endpoints</li> </ul> <p style="color: #333; font-size: 16px;">Additional exclusions for rolling deployments:</p> <ul> <li style="color: #333; font-size: 16px;">Serverless inference endpoints</li> <li style="color: #333; font-size: 16px;">Multi-variant inference endpoints</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Choose an appropriate batch size based on your risk tolerance and update urgency</li> <li style="color: #333; font-size: 16px;">Set up comprehensive CloudWatch alarms to monitor critical metrics</li> <li style="color: #333; font-size: 16px;">Test your deployment configuration in a non-production environment first</li> <li style="color: #333; font-size: 16px;">Regularly monitor the deployment progress using the <code>DescribeEndpoint</code> API</li> <li style="color: #333; font-size: 16px;">Have a rollback plan in case of unexpected issues</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging Rolling Deployments in Amazon SageMaker, you can update your endpoints with minimal risk and optimal resource utilization, especially for large models or endpoints with many instances. This approach provides a balance between deployment speed and safety, allowing you to confidently update your machine learning models in production.</p>

<hr />

<p style="color: #333; font-size: 16px;">Amazon SageMaker's Shadow Tests allow you to evaluate changes to your model serving infrastructure by comparing its performance against the currently deployed infrastructure without impacting end users. This feature helps catch potential configuration errors and performance issues before they affect production.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Benefits:</span></p> <ul> <li style="color: #333; font-size: 16px;">Validate changes without end-user impact</li> <li style="color: #333; font-size: 16px;">Evaluate operational performance metrics</li> <li style="color: #333; font-size: 16px;">Assess impact of infrastructure changes</li> <li style="color: #333; font-size: 16px;">No need to build custom shadow testing infrastructure</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Use Cases:</span></p> <ul> <li style="color: #333; font-size: 16px;">Evaluating a new model before promotion to production</li> <li style="color: #333; font-size: 16px;">Assessing impact of container changes (e.g., patching, upgrades)</li> <li style="color: #333; font-size: 16px;">Evaluating performance of new ML instances</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How It Works:</span></p> <ol> <li style="color: #333; font-size: 16px;">Set up a shadow test for a predefined duration</li> <li style="color: #333; font-size: 16px;">SageMaker deploys the new variant in shadow mode</li> <li style="color: #333; font-size: 16px;">A copy of inference requests is routed to the shadow variant</li> <li style="color: #333; font-size: 16px;">Only production variant responses are returned to the calling application</li> <li style="color: #333; font-size: 16px;">Monitor progress through a live dashboard</li> <li style="color: #333; font-size: 16px;">Clean up automatically upon completion</li> <li style="color: #333; font-size: 16px;">Act on the results (promote or retain existing variant)</li> </ul> </ol>
    
<p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Creating a Shadow Test:</span></p> <p style="color: #333; font-size: 16px;">To create a shadow test, you need to specify:</p> <ul> <li style="color: #333; font-size: 16px;">A production variant (receives and responds to 100% of requests)</li> <li style="color: #333; font-size: 16px;">A shadow variant (receives a percentage of replicated requests, doesn't return responses)</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Configuration Options:</span></p> <ul> <li style="color: #333; font-size: 16px;">Control model, instance type, and instance count for each variant</li> <li style="color: #333; font-size: 16px;">Set traffic sampling percentage for the shadow variant</li> <li style="color: #333; font-size: 16px;">Schedule test start time and duration (default 7 days, max 30 days)</li> <li style="color: #333; font-size: 16px;">Optionally enable Data Capture to log requests and responses</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Prerequisites:</span></p> <ul> <li style="color: #333; font-size: 16px;">A SageMaker model ready to use</li> <li style="color: #333; font-size: 16px;">An existing endpoint or models to compare</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Steps to Create a Shadow Test:</span></p> <ol> <li style="color: #333; font-size: 16px;">Enter shadow test details</li> <li style="color: #333; font-size: 16px;">Enter shadow test settings</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Monitoring and Analysis:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use the SageMaker console dashboard for live monitoring</li> <li style="color: #333; font-size: 16px;">Compare invocation metrics and instance metrics between variants</li> <li style="color: #333; font-size: 16px;">Review tabular view with relevant metric statistics</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Post-Test Actions:</span></p> <ul> <li style="color: #333; font-size: 16px;">Promote the shadow variant to be the new production variant</li> <li style="color: #333; font-size: 16px;">Retain the existing production variant</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Limitations and Exclusions:</span></p> <p style="color: #333; font-size: 16px;">Shadow tests are not compatible with endpoints using:</p> <ul> <li style="color: #333; font-size: 16px;">Serverless inference</li> <li style="color: #333; font-size: 16px;">Asynchronous inference</li> <li style="color: #333; font-size: 16px;">Marketplace containers</li> <li style="color: #333; font-size: 16px;">Multiple-container endpoints</li> <li style="color: #333; font-size: 16px;">Multi-model endpoints</li> <li style="color: #333; font-size: 16px;">Inf1 (Inferentia-based) instances</li> <li style="color: #333; font-size: 16px;">Amazon Elastic Inference endpoints</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Start with a low traffic sampling percentage and gradually increase</li> <li style="color: #333; font-size: 16px;">Use Data Capture for detailed analysis of requests and responses</li> <li style="color: #333; font-size: 16px;">Monitor both performance metrics and business KPIs during the test</li> <li style="color: #333; font-size: 16px;">Plan for sufficient test duration to capture various traffic patterns</li> <li style="color: #333; font-size: 16px;">Review results thoroughly before promoting a shadow variant</li> </ul> <p style="color: #333; font-size: 16px;">By utilizing Shadow Tests in Amazon SageMaker, you can confidently evaluate and implement changes to your model serving infrastructure, ensuring optimal performance and reliability for your machine learning applications in production.</p>

<hr />

<p style="color: #333; font-size: 16px;">After running a shadow test in Amazon SageMaker, it's important to properly complete the test, analyze the results, and potentially promote the shadow variant to production. This guide covers the process of completing a shadow test, promoting a shadow variant, and best practices for shadow testing.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Completing a Shadow Test:</span></p> <ol> <li style="color: #333; font-size: 16px;">Tests automatically complete at the end of the scheduled duration</li> <li style="color: #333; font-size: 16px;">You can stop an in-progress test early if needed</li> <li style="color: #333; font-size: 16px;">After completion, the test status shows as "Complete" in the Shadow tests section</li> <li style="color: #333; font-size: 16px;">Review and analyze the final metrics using the metrics dashboard</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Completing a Test Early:</span></p> <ol> <li style="color: #333; font-size: 16px;">Select the test from the Shadow tests section</li> <li style="color: #333; font-size: 16px;">Choose "Complete" from the Actions dropdown</li> <li style="color: #333; font-size: 16px;">Select to deploy or remove the shadow variant</li> <li style="color: #333; font-size: 16px;">Optionally, add a comment explaining the early completion</li> <li style="color: #333; font-size: 16px;">Confirm the action</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Promoting a Shadow Variant:</span></p> <p style="color: #333; font-size: 16px;">If you decide to replace your production variant with the shadow variant, follow these steps:</p> <ol> <li style="color: #333; font-size: 16px;">Complete the shadow test if it's still in progress</li> <li style="color: #333; font-size: 16px;">Select the test and choose "View" from the Actions dropdown</li> <li style="color: #333; font-size: 16px;">Click "Deploy shadow variant" in the Environment section</li> <li style="color: #333; font-size: 16px;">Choose one of the following options for instance settings: <ul> <li style="color: #333; font-size: 16px;">Retain production settings</li> <li style="color: #333; font-size: 16px;">Retain shadow settings</li> <li style="color: #333; font-size: 16px;">Custom instance settings</li> </ul> </li> <li style="color: #333; font-size: 16px;">If choosing custom settings, select instance type and count</li> <li style="color: #333; font-size: 16px;">Confirm the deployment by typing 'deploy'</li> <li style="color: #333; font-size: 16px;">Click "Deploy shadow variant"</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices for Shadow Testing:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Practice</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Description</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Traffic Sampling Percentage</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Start with a lower percentage and gradually increase</li> <li>Aim for 100% sampling before promotion to ensure the variant can handle full production traffic</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Instance Type</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Use the same instance type, size, and count as production unless specifically testing alternatives</li> <li>Ensures the shadow variant can handle the volume of inference requests after promotion</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Auto Scaling</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Configure auto scaling on shadow variants</li> <li>Helps handle spikes in inference requests or changes in request patterns</li> <li>Allows validation of auto scaling policies without impacting users</li> </ul> </td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Metrics Monitoring</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <ul> <li>Regularly monitor the metrics dashboard after initiating the experiment</li> <li>Ensure metrics like latency and error rate are within acceptable bounds</li> <li>Helps catch misconfigurations early for corrective action</li> </ul> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Additional Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Thoroughly analyze metrics before deciding to promote a shadow variant</li> <li style="color: #333; font-size: 16px;">Consider the impact on your application when changing instance types or counts</li> <li style="color: #333; font-size: 16px;">Have a rollback plan in case issues arise after promoting a shadow variant</li> <li style="color: #333; font-size: 16px;">Document the reasons for completing tests early or promoting variants for future reference</li> </ul> <p style="color: #333; font-size: 16px;">By following these guidelines for completing shadow tests, promoting shadow variants, and adhering to best practices, you can effectively use Amazon SageMaker's shadow testing capabilities to validate and improve your machine learning model deployments with minimal risk to your production environment.</p>


		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Optimize model performance using Neo</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker Neo is a capability that enables machine learning models to be trained once and run anywhere in the cloud and at the edge. It automatically optimizes models for inference on various platforms, simplifying the process of deploying efficient models across different hardware and software configurations.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Features of SageMaker Neo:</span></p> <ul> <li style="color: #333; font-size: 16px;">Automatic optimization for multiple platforms</li> <li style="color: #333; font-size: 16px;">Support for various ML frameworks and hardware architectures</li> <li style="color: #333; font-size: 16px;">Compiler and runtime components for optimization and execution</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How Neo Works:</span></p> <ol> <li style="color: #333; font-size: 16px;">Reads models exported from various frameworks</li> <li style="color: #333; font-size: 16px;">Converts framework-specific functions to a framework-agnostic representation</li> <li style="color: #333; font-size: 16px;">Performs a series of optimizations</li> <li style="color: #333; font-size: 16px;">Generates optimized binary code</li> <li style="color: #333; font-size: 16px;">Provides a runtime for each target platform</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Supported Platforms and Frameworks:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Category</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px; background-color: #f2f2f2;">Supported Options</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Frameworks</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Gluon, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Platforms</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Android, Linux, Windows</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Processors</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Ambarella, ARM, Intel, Nvidia, NXP, Qualcomm, Texas Instruments, Xilinx</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Using Neo to Compile a Model:</span></p> <p style="color: #333; font-size: 16px;">You can create a Neo compilation job using:</p> <ul> <li style="color: #333; font-size: 16px;">SageMaker console</li> <li style="color: #333; font-size: 16px;">AWS Command Line Interface (CLI)</li> <li style="color: #333; font-size: 16px;">Python notebook</li> <li style="color: #333; font-size: 16px;">SageMaker SDK</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deploying Neo-compiled Models:</span></p> <p style="color: #333; font-size: 16px;">You can deploy Neo-compiled models to:</p> <ul> <li style="color: #333; font-size: 16px;">Cloud instances (including AWS Inferentia)</li> <li style="color: #333; font-size: 16px;">Edge devices</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Deployment Steps:</span></p> <ol> <li style="color: #333; font-size: 16px;">Configure and create an HTTPS endpoint using SageMaker hosting services</li> <li style="color: #333; font-size: 16px;">Use the same instance type for deployment as used for compilation</li> <li style="color: #333; font-size: 16px;">Deploy using SageMaker SDK, Boto3, AWS CLI, or SageMaker console</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Inference Container Images:</span></p> <p style="color: #333; font-size: 16px;">For deployments using AWS CLI, console, or Boto3, you need to select a Docker image Amazon ECR URI for your primary container. The URI template is:</p> <pre style="color: #333; font-size: 16px; background-color: #f5f5f5; padding: 10px;"> aws_account_id.dkr.ecr.aws_region.amazonaws.com/xgboost-neo:latest </pre> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Test compiled models thoroughly before deployment</li> <li style="color: #333; font-size: 16px;">Use the same instance type for deployment as used during compilation</li> <li style="color: #333; font-size: 16px;">Monitor performance metrics after deployment to ensure optimization benefits</li> <li style="color: #333; font-size: 16px;">Regularly update and recompile models to leverage the latest Neo optimizations</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Limitations and Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Not all models and frameworks are supported for all target platforms</li> <li style="color: #333; font-size: 16px;">Compilation for Inferentia and Trainium instances requires specific targeting</li> <li style="color: #333; font-size: 16px;">Elastic Inference accelerators require models compiled specifically for ml_eia2 devices</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging Amazon SageMaker Neo, you can significantly improve the performance and portability of your machine learning models across various deployment targets, streamlining the process of optimizing models for inference in both cloud and edge environments.</p>


            <hr />

            <p style="color: #333; font-size: 16px;">Amazon SageMaker Neo enables you to deploy machine learning models to various edge devices, optimizing performance for resource-constrained environments. This guide covers the process of deploying Neo-compiled models to edge devices using two methods: Direct deployment with Deep Learning Runtime (DLR) and deployment through AWS IoT Greengrass.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Supported Edge Devices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Raspberry Pi 3</li> <li style="color: #333; font-size: 16px;">Texas Instruments' Sitara</li> <li style="color: #333; font-size: 16px;">Jetson TX1</li> <li style="color: #333; font-size: 16px;">And more (refer to the full list of supported devices)</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Prerequisites:</span></p> <ol> <li style="color: #333; font-size: 16px;">Ensure your edge device is supported by SageMaker Neo</li> <li style="color: #333; font-size: 16px;">Compile your model for the specific target edge device</li> <li style="color: #333; font-size: 16px;">Configure your edge device to use AWS services</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Method 1: Deploy using Deep Learning Runtime (DLR)</span></p> <p style="color: #333; font-size: 16px;">DLR is a compact runtime for deep learning and decision tree models, providing unified Python/C++ APIs for loading and running compiled models.</p> <p style="color: #333; font-size: 16px;"><strong>Steps:</strong></p> <ol> <li style="color: #333; font-size: 16px;">Install DLR on your edge device: <pre style="background-color: #f5f5f5; padding: 10px;">pip install dlr</pre> </li> <li style="color: #333; font-size: 16px;">For specific devices (e.g., Raspberry Pi 3), use a pre-built binary: <pre style="background-color: #f5f5f5; padding: 10px;">pip install https://neo-ai-dlr-release.s3-us-west-2.amazonaws.com/v1.3.0/pi-armv7l-raspbian4.14.71-glibc2_24-libstdcpp3_4/dlr-1.3.0-py3-none-any.whl</pre> </li> <li style="color: #333; font-size: 16px;">Download the compiled model from Amazon S3 to your edge device</li> <li style="color: #333; font-size: 16px;">Use DLR to load and run the compiled model on your device</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Method 2: Deploy using AWS IoT Greengrass</span></p> <p style="color: #333; font-size: 16px;">AWS IoT Greengrass extends cloud capabilities to local devices, enabling machine learning inference at the edge using cloud-trained models.</p> <p style="color: #333; font-size: 16px;"><strong>Supported Devices:</strong></p> <ul> <li style="color: #333; font-size: 16px;">AWS IoT Greengrass devices based on: <ul> <li style="color: #333; font-size: 16px;">ARM Cortex-A</li> <li style="color: #333; font-size: 16px;">Intel Atom</li> <li style="color: #333; font-size: 16px;">Nvidia Jetson series processors</li> </ul> </li> </ul> <p style="color: #333; font-size: 16px;"><strong>Deployment Process:</strong></p> <ol> <li style="color: #333; font-size: 16px;">Set up AWS IoT Greengrass on your edge device</li> <li style="color: #333; font-size: 16px;">Create a Lambda inference application for your model</li> <li style="color: #333; font-size: 16px;">Configure and deploy the Lambda function to your Greengrass device</li> <li style="color: #333; font-size: 16px;">Use the AWS Management Console to configure optimized machine learning inference</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Ensure your edge device meets the minimum requirements for running Neo-compiled models</li> <li style="color: #333; font-size: 16px;">Regularly update DLR to benefit from the latest optimizations and bug fixes</li> <li style="color: #333; font-size: 16px;">Monitor resource usage on your edge device to ensure optimal performance</li> <li style="color: #333; font-size: 16px;">Implement proper error handling and logging in your inference application</li> <li style="color: #333; font-size: 16px;">Consider using AWS IoT Greengrass for easier management and updates of deployed models</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Troubleshooting Tips:</span></p> <ul> <li style="color: #333; font-size: 16px;">Verify that your model was compiled for the correct target device</li> <li style="color: #333; font-size: 16px;">Check device compatibility and system requirements</li> <li style="color: #333; font-size: 16px;">Ensure proper AWS credentials are set up on the edge device</li> <li style="color: #333; font-size: 16px;">Review logs for any errors during model loading or inference</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Getting Started Guide:</span></p> <p style="color: #333; font-size: 16px;">For first-time users, Amazon SageMaker provides a comprehensive getting started guide that covers:</p> <ol> <li style="color: #333; font-size: 16px;">Setting up AWS credentials</li> <li style="color: #333; font-size: 16px;">Compiling a model for edge deployment</li> <li style="color: #333; font-size: 16px;">Deploying the model to a Raspberry Pi 3</li> <li style="color: #333; font-size: 16px;">Making inferences on images using the deployed model</li> </ol> <p style="color: #333; font-size: 16px;">By following these guidelines and leveraging the capabilities of Amazon SageMaker Neo, you can efficiently deploy optimized machine learning models to edge devices, enabling powerful inference capabilities in resource-constrained environments.</p>

            <hr />

            <p style="color: #333; font-size: 16px;"><strong style="color: #d93025;">Important Note:</strong> Starting April 15, 2023, AWS will not onboard new customers to Amazon Elastic Inference (EI) and will help current customers migrate their workloads to options that offer better price and performance.</p> <p style="color: #333; font-size: 16px;">Amazon SageMaker Elastic Inference (EI) allows you to attach low-cost GPU-powered acceleration to Amazon EC2 and Amazon SageMaker instances to reduce the cost of running deep learning inference.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;">EI attaches fractional GPUs to instances for inference acceleration</li> <li style="color: #333; font-size: 16px;">Supports TensorFlow, Apache MXNet, and PyTorch frameworks</li> <li style="color: #333; font-size: 16px;">Can be used with SageMaker notebook instances and hosted endpoints</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Migration Recommendations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use Amazon SageMaker Inference Recommender to identify cost-effective alternatives</li> <li style="color: #333; font-size: 16px;">Consider AWS Inferentia (e.g., ml.inf1.xlarge) for better performance and cost savings</li> <li style="color: #333; font-size: 16px;">Evaluate Amazon EC2 G5 instances as an alternative</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Using EI with Different Frameworks:</span></p> <p style="color: #333; font-size: 16px;"><strong>1. TensorFlow:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> predictor = tensorflow_model.deploy( instance_type="ml.g4dn.2xlarge" # instance_type="ml.c5.xlarge", # accelerator_type="ml.eia1.medium" ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. MXNet:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> predictor = mxnet_model.deploy( instance_type="ml.g4dn.2xlarge" # instance_type="ml.c5.xlarge", # accelerator_type="ml.eia1.medium" ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. PyTorch:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> predictor = pytorch_model.deploy( instance_type="ml.g4dn.2xlarge", # accelerator_type="ml.eia1.medium" ) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Migrating from EI:</span></p> <ol> <li style="color: #333; font-size: 16px;">Choose the right instance type (e.g., g4dn.xlarge or inf1.xlarge)</li> <li style="color: #333; font-size: 16px;">Modify your inference.py file to remove EI-specific changes</li> <li style="color: #333; font-size: 16px;">Create a new model pointing to the modified inference.py</li> <li style="color: #333; font-size: 16px;">Deploy the model to a new endpoint or update an existing one</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Updating an Existing Endpoint:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> endpoint_config_response = sagemaker_client.create_endpoint_config( EndpointConfigName=endpoint_config_name, ProductionVariants=[{ "VariantName": "variant1", "ModelName": model_name, "InstanceType": instance_type, "InitialInstanceCount": 1 }] )
endpoint_config_response = sagemaker_client.update_endpoint( EndpointConfigName=endpoint_config_name, EndpointName=endpoint_name ) </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use SageMaker Inference Recommender to find the best alternative instance type</li> <li style="color: #333; font-size: 16px;">Test thoroughly after migration to ensure performance meets requirements</li> <li style="color: #333; font-size: 16px;">Consider using the latest framework versions available on new instance types</li> <li style="color: #333; font-size: 16px;">Implement proper error handling and logging in your inference code</li> <li style="color: #333; font-size: 16px;">Monitor your endpoints closely after migration to ensure optimal performance</li> </ul> <p style="color: #333; font-size: 16px;">By following these guidelines, you can successfully migrate from Amazon SageMaker Elastic Inference to more cost-effective and performant alternatives, ensuring your machine learning inference workloads continue to run efficiently.</p>


		</div>
	</div>
	
	<br/>
	
</div>



<div class="container mt-5">
	<h3 class="text-primary h4">Stateful sessions with Amazon SageMaker models</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">Amazon SageMaker now supports stateful sessions for inference endpoints, allowing you to route multiple requests to the same ML instance and maintain context between requests. This feature is particularly useful for stateful models that cache context data from inference requests.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Concepts:</span></p> <ul> <li style="color: #333; font-size: 16px;">Stateful models cache context data from inference requests</li> <li style="color: #333; font-size: 16px;">Ideal for large context data (e.g., large text files, long chat histories, multimedia data)</li> <li style="color: #333; font-size: 16px;">Reduces network latency by not requiring full context with every request</li> <li style="color: #333; font-size: 16px;">Implementation of the stateful model is owned by the user</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">How Stateful Sessions Work:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Starting a Stateful Session:</strong></li> <ul> <li style="color: #333; font-size: 16px;">Client sends an <code>InvokeEndpoint</code> request with <code>SessionID=NEW_SESSION</code></li> <li style="color: #333; font-size: 16px;">Container starts a new session, caches data, creates a session ID, and sets a TTL timestamp</li> <li style="color: #333; font-size: 16px;">Container returns session ID and timestamp in HTTP header</li> <li style="color: #333; font-size: 16px;">SageMaker provides session ID and TTL in the response</li> </ul> <li style="color: #333; font-size: 16px;"><strong>Continuing a Stateful Session:</strong></li> <ul> <li style="color: #333; font-size: 16px;">Client sends subsequent <code>InvokeEndpoint</code> requests with the session ID</li> <li style="color: #333; font-size: 16px;">SageMaker routes the request to the same ML instance</li> <li style="color: #333; font-size: 16px;">Container uses cached context data for inference</li> </ul> <li style="color: #333; font-size: 16px;"><strong>Closing a Stateful Session:</strong></li> <ul> <li style="color: #333; font-size: 16px;">Client sends a final <code>InvokeEndpoint</code> request with session ID and close instruction</li> <li style="color: #333; font-size: 16px;">Container closes the session and returns the session ID</li> <li style="color: #333; font-size: 16px;">SageMaker provides the closed session ID in the response</li> </ul> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Code Examples:</span></p> <p style="color: #333; font-size: 16px;"><strong>1. Starting a New Session:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> import boto3 import json
payload = { "requestType": "NEW_SESSION" } payload = json.dumps(payload)

smr = boto3.client('sagemaker-runtime', region_name="region_name", endpoint_url="endpoint_url")

create_session_response = smr.invoke_endpoint( EndpointName="endpoint_name", Body=payload, ContentType="application/json", SessionId="NEW_SESSION")

session_id = create_session_response['ResponseMetadata']['HTTPHeaders']['x-amzn-sagemaker-new-session-id'].split(';')[0] </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><strong>2. Continuing a Session:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> smr.invoke_endpoint( EndpointName="endpoint_name", Body=payload, ContentType="application/json", SessionId=session_id) </pre> </td> </tr> </table> <p style="color: #333; font-size: 16px;"><strong>3. Closing a Session:</strong></p> <table style="border-collapse: collapse; width: 100%;"> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> <pre style="margin: 0; white-space: pre-wrap;"> payload = { "requestType": "CLOSE" } payload = json.dumps(payload)
closeSessionResponse = smr.invoke_endpoint( EndpointName="endpoint_name", Body=payload, ContentType="application/json", SessionId=session_id)

closed_session_id = closeSessionResponse['ResponseMetadata']['HTTPHeaders']['x-amzn-sagemaker-closed-session-id'].split(';')[0] </pre> </td>

</tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Example Implementation:</span></p> <p style="color: #333; font-size: 16px;">An example notebook demonstrating the implementation of a stateful model container and client application is available:</p> <ul> <li style="color: #333; font-size: 16px;">LLaVA stateful inference with SageMaker</li> <li style="color: #333; font-size: 16px;">Uses the LLaVA: Large Language and Vision Assistant model</li> <li style="color: #333; font-size: 16px;">Demonstrates uploading an image and asking questions without resending the image</li> <li style="color: #333; font-size: 16px;">Uses TorchServe framework and caches image data in GPU memory</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Implement proper error handling for session management</li> <li style="color: #333; font-size: 16px;">Consider session timeout and cleanup mechanisms</li> <li style="color: #333; font-size: 16px;">Monitor resource usage, especially memory, for long-running sessions</li> <li style="color: #333; font-size: 16px;">Implement secure session handling to prevent unauthorized access</li> <li style="color: #333; font-size: 16px;">Test thoroughly with various session scenarios and edge cases</li> </ul> <p style="color: #333; font-size: 16px;">By leveraging stateful sessions in Amazon SageMaker, you can significantly improve the performance and user experience of applications that require maintaining context across multiple inference requests, such as chatbots, interactive AI assistants, and multimodal models.</p>


		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Best Practices</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
        <p style="color: #333; font-size: 16px;">When deploying models using Amazon SageMaker hosting services, it's important to follow best practices to ensure optimal performance, security, and reliability. This guide covers key considerations and recommendations for model deployment.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">General Considerations:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use HTTPS endpoints for client applications to send inference requests</li> <li style="color: #333; font-size: 16px;">Be aware of algorithm-specific output formats when deploying to custom targets</li> <li style="color: #333; font-size: 16px;">Consider using multiple model variants for A/B testing</li> <li style="color: #333; font-size: 16px;">Use Application Auto Scaling for efficient resource management</li> <li style="color: #333; font-size: 16px;">Modify endpoints without downtime using new endpoint configurations</li> <li style="color: #333; font-size: 16px;">Avoid changing model artifacts or inference code after deployment</li> <li style="color: #333; font-size: 16px;">Consider batch transform for large dataset inferences</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">High Availability and Fault Tolerance:</span></p> <ol> <li style="color: #333; font-size: 16px;">Deploy multiple instances across Availability Zones: <ul> <li style="color: #333; font-size: 16px;">Use at least two subnets in different Availability Zones for VPCs</li> <li style="color: #333; font-size: 16px;">Use smaller instance types spread across zones for better reliability</li> </ul> </li> <li style="color: #333; font-size: 16px;">Configure inference components for high availability: <ul> <li style="color: #333; font-size: 16px;">Use more than two copies of inference components</li> <li style="color: #333; font-size: 16px;">Set minimum instance count to two in auto-scaling policies</li> </ul> </li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Security Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Use AWS Security Hub to monitor SageMaker resource configurations</li> <li style="color: #333; font-size: 16px;">Comply with various security standards and frameworks</li> <li style="color: #333; font-size: 16px;">Regularly review and update security controls</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Low Latency Inference with AWS PrivateLink:</span></p> <ol> <li style="color: #333; font-size: 16px;">Deploy AWS PrivateLink: <ul> <li style="color: #333; font-size: 16px;">Create an interface endpoint for the VPC</li> <li style="color: #333; font-size: 16px;">Enable DNS name and select appropriate security groups and subnets</li> </ul> </li> <li style="color: #333; font-size: 16px;">Deploy SageMaker endpoint in a VPC: <ul> <li style="color: #333; font-size: 16px;">Use the same subnets as specified in AWS PrivateLink deployment</li> <li style="color: #333; font-size: 16px;">Match the Availability Zones of your client application</li> </ul> </li> <li style="color: #333; font-size: 16px;">Invoke the SageMaker endpoint using the SageMaker Runtime client</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Autoscaling:</span></p> <ul> <li style="color: #333; font-size: 16px;">Implement autoscaling to dynamically adjust capacity based on workload</li> <li style="color: #333; font-size: 16px;">Helps maintain performance while minimizing costs</li> <li style="color: #333; font-size: 16px;">Useful for both real-time and asynchronous inference</li> </ul> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Endpoint Security and Health:</span></p> <ol> <li style="color: #333; font-size: 16px;">Don't delete resources while endpoints use them: <ul> <li style="color: #333; font-size: 16px;">Model definitions, artifacts, IAM roles, inference images, VPC configurations, etc.</li> </ul> </li> <li style="color: #333; font-size: 16px;">Follow proper procedures for updating endpoints: <ul> <li style="color: #333; font-size: 16px;">Create new model definitions or endpoint configurations</li> <li style="color: #333; font-size: 16px;">Update endpoints with new configurations</li> <li style="color: #333; font-size: 16px;">Optionally delete old configurations if not in use</li> </ul> </li> <li style="color: #333; font-size: 16px;">Use unique names for new model definitions and endpoint configurations</li> <li style="color: #333; font-size: 16px;">If retaining original names, follow specific procedures for updates</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices Summary:</span></p> <ul> <li style="color: #333; font-size: 16px;">Implement multi-AZ deployments for high availability</li> <li style="color: #333; font-size: 16px;">Use AWS PrivateLink for low-latency inference within VPCs</li> <li style="color: #333; font-size: 16px;">Implement autoscaling for efficient resource utilization</li> <li style="color: #333; font-size: 16px;">Follow security best practices and use AWS Security Hub</li> <li style="color: #333; font-size: 16px;">Carefully manage endpoint resources and follow proper update procedures</li> <li style="color: #333; font-size: 16px;">Regularly monitor and optimize endpoint performance</li> </ul> <p style="color: #333; font-size: 16px;">By following these best practices, you can ensure that your SageMaker-hosted models are performant, secure, and cost-effective, providing reliable inference services for your applications.</p>
            
        

		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Features</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
        <p style="color: #333; font-size: 16px;">Amazon SageMaker offers four main options for deploying models for inference, each with its own set of supported features. This guide summarizes the core platform features supported by each inference option and provides additional details for real-time inference hosting options.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Main Inference Options:</span></p> <ol> <li style="color: #333; font-size: 16px;">Real-time inference</li> <li style="color: #333; font-size: 16px;">Batch transform</li> <li style="color: #333; font-size: 16px;">Asynchronous inference</li> <li style="color: #333; font-size: 16px;">Serverless inference</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Feature Comparison Table:</span></p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Feature</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Real-time</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Batch</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Async</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Serverless</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Autoscaling</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">N/A</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">GPU support</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><sup>1</sup></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><sup>1</sup></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><sup>1</sup></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Multi-model endpoint</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Inference Recommender</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Payload size</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">< 6 MB</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 100 MB</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 1 GB</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"> 4 MB</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Request timeout</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">< 60 seconds</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Days</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">< 1 hour</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">< 60 seconds</td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Real-time Inference Hosting Options:</span></p> <p style="color: #333; font-size: 16px;">For real-time inference, SageMaker offers various hosting options with different feature sets:</p> <table style="border-collapse: collapse; width: 100%;"> <tr style="background-color: #f2f2f2;"> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Feature</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Single model</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Multi-model</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Serial inference pipeline</th> <th style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Multi-container</th> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Autoscaling</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">GPU support</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><sup>1</sup></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Inference Recommender</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Shadow testing</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;">Model parallel serving</td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><sup>3</sup></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"><sup>3</sup></td> <td style="border: 1px solid #ddd; padding: 8px; font-size: 16px;"></td> </tr> </table> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Notes:</span></p> <ol> <li style="color: #333; font-size: 16px;">Availability of Amazon EC2 instance types depends on the AWS Region.</li> <li style="color: #333; font-size: 16px;">For frameworks other than those listed, use the SageMaker Inference toolkit to build a container supporting multi-model endpoints.</li> <li style="color: #333; font-size: 16px;">SageMaker supports deploying large models (up to 500 GB) for inference with configurable container health check and download timeout quotas.</li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Key Takeaways:</span></p> <ul> <li style="color: #333; font-size: 16px;">Real-time inference offers the most comprehensive feature set, including autoscaling, GPU support, and multiple hosting options.</li> <li style="color: #333; font-size: 16px;">Batch transform is suitable for large datasets and long-running inference jobs.</li> <li style="color: #333; font-size: 16px;">Asynchronous inference supports larger payloads and longer processing times compared to real-time inference.</li> <li style="color: #333; font-size: 16px;">Serverless inference is ideal for workloads with idle periods between traffic spurts.</li> <li style="color: #333; font-size: 16px;">Consider payload size, request timeout, and specific feature requirements when choosing an inference option.</li> </ul> <p style="color: #333; font-size: 16px;">By understanding the supported features for each inference option, you can make informed decisions when deploying your machine learning models on Amazon SageMaker, ensuring optimal performance and cost-effectiveness for your specific use case.</p>

            
		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">FAQ</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;">This FAQ guide covers common questions about Amazon SageMaker Inference Hosting, including general hosting, Real-Time Inference, Serverless Inference, Batch Transform, and Asynchronous Inference.</p> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">General Hosting FAQs:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Q: What deployment options does Amazon SageMaker provide?</strong> <p>A: SageMaker offers four deployment options:</p> <ul> <li>Real-Time Inference: For millisecond latency requirements, payloads up to 6 MB, and processing times up to 60 seconds.</li> <li>Batch Transform: For offline predictions on large batches of data.</li> <li>Asynchronous Inference: For workloads without sub-second latency requirements, payloads up to 1 GB, and processing times up to 15 minutes.</li> <li>Serverless Inference: For intermittent workloads without managing infrastructure.</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Q: How do I optimize costs with SageMaker Inference?</strong> <p>A: To optimize costs:</p> <ul> <li>Choose the right hosting option for your use case</li> <li>Use Amazon SageMaker Savings Plans</li> <li>Optimize models with SageMaker Neo</li> <li>Utilize Multi-Model Endpoints and Multi-Container Endpoints</li> <li>Implement autoscaling</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Q: What is Bring Your Own Container (BYOC) with Amazon SageMaker?</strong> <p>A: BYOC allows you to use custom Docker images for frameworks not supported by SageMaker's managed containers. You can either build a container from scratch or extend an existing SageMaker container.</p> </li> <li style="color: #333; font-size: 16px;"><strong>Q: Do I need to train models on SageMaker to host them on SageMaker endpoints?</strong> <p>A: No, you can bring your own trained models and deploy them on SageMaker endpoints. Models need to be packaged in a specific structure within a model.tar.gz file.</p> </li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Real-Time Inference FAQs:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Q: How do I create a SageMaker endpoint?</strong> <p>A: You can create endpoints using AWS SDKs, SageMaker Python SDK, AWS Management Console, AWS CloudFormation, or AWS CDK. The process involves creating a SageMaker model, an endpoint configuration, and then the endpoint itself.</p> </li> <li style="color: #333; font-size: 16px;"><strong>Q: What are the different model deployment architectures supported by Real-Time Inference?</strong> <p>A: SageMaker supports:</p> <ul> <li>Multi-Model Endpoints (MME): For deploying thousands of models on shared resources</li> <li>Multi-Container Endpoints (MCE): For deploying up to 15 containers with diverse ML frameworks</li> <li>Serial Inference Pipelines (SIP): For chaining 2-15 containers on a single endpoint</li> </ul> </li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Serverless Inference FAQs:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Q: What is Amazon SageMaker Serverless Inference?</strong> <p>A: It's a serverless model serving option that automatically manages compute resources and scaling, ideal for applications with intermittent or unpredictable traffic.</p> </li> <li style="color: #333; font-size: 16px;"><strong>Q: How do I choose the right memory size for my serverless endpoint?</strong> <p>A: Choose a memory size at least as large as your model size. Available sizes range from 1024 MB to 6144 MB. Larger memory sizes provide access to more vCPUs.</p> </li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Batch Transform FAQs:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Q: How does Batch Transform split my data?</strong> <p>A: SageMaker can split data into single-record or multi-record mini batches based on the BatchStrategy parameter.</p> </li> <li style="color: #333; font-size: 16px;"><strong>Q: What is the maximum timeout and payload limit for Batch Transform?</strong> <p>A: The maximum timeout is 3600 seconds, and the maximum payload size for a record is 100 MB.</p> </li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Asynchronous Inference FAQs:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Q: What is Amazon SageMaker Asynchronous Inference?</strong> <p>A: It's an option that queues incoming requests and processes them asynchronously, ideal for requests with large payloads or long processing times.</p> </li> <li style="color: #333; font-size: 16px;"><strong>Q: How do I scale my endpoints to 0 when there's no traffic?</strong> <p>A: SageMaker supports autoscaling for asynchronous endpoints, allowing them to scale down to zero instances when there's no traffic. Serverless Inference also automatically scales to zero.</p> </li> </ol> <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Best Practices:</span></p> <ul> <li style="color: #333; font-size: 16px;">Choose the appropriate inference option based on your specific use case and requirements</li> <li style="color: #333; font-size: 16px;">Optimize costs by leveraging features like autoscaling and multi-model endpoints</li> <li style="color: #333; font-size: 16px;">Use SageMaker Inference Recommender for optimal endpoint configurations</li> <li style="color: #333; font-size: 16px;">Properly structure your models when deploying to SageMaker</li> <li style="color: #333; font-size: 16px;">Consider using Serverless Inference for workloads with intermittent traffic</li> </ul> <p style="color: #333; font-size: 16px;">By understanding these FAQs and best practices, you can make informed decisions when deploying and managing your machine learning models on Amazon SageMaker, ensuring optimal performance and cost-effectiveness for your specific use cases.</p>
		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            <p style="color: #333; font-size: 16px;"><span style="color: #0066cc; font-weight: bold;">Comprehensive Summary of Amazon SageMaker Model Hosting:</span></p> <ol> <li style="color: #333; font-size: 16px;"><strong>Deployment Options:</strong> <ul> <li>Real-Time Inference: For low-latency requirements (milliseconds), payloads up to 6 MB, processing times up to 60 seconds.</li> <li>Batch Transform: For offline predictions on large datasets.</li> <li>Asynchronous Inference: For larger payloads (up to 1 GB) and longer processing times (up to 15 minutes).</li> <li>Serverless Inference: For intermittent workloads, automatic scaling, pay-per-use model.</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Cost Optimization:</strong> <ul> <li>Use Amazon SageMaker Savings Plans</li> <li>Optimize models with SageMaker Neo</li> <li>Implement Multi-Model Endpoints and Multi-Container Endpoints</li> <li>Utilize autoscaling features</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Bring Your Own Container (BYOC):</strong> <ul> <li>Allows use of custom Docker images for unsupported frameworks</li> <li>Option to build from scratch or extend existing SageMaker containers</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Model Deployment:</strong> <ul> <li>Models trained outside SageMaker can be deployed on SageMaker endpoints</li> <li>Specific packaging requirements (model.tar.gz file with defined structure)</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Real-Time Inference Architectures:</strong> <ul> <li>Multi-Model Endpoints (MME): For thousands of models on shared resources</li> <li>Multi-Container Endpoints (MCE): Up to 15 containers with diverse ML frameworks</li> <li>Serial Inference Pipelines (SIP): Chain 2-15 containers on a single endpoint</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Serverless Inference:</strong> <ul> <li>Automatically manages compute resources and scaling</li> <li>Ideal for intermittent or unpredictable traffic</li> <li>Memory sizes from 1024 MB to 6144 MB, affecting vCPU access</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Batch Transform:</strong> <ul> <li>Supports data splitting into single-record or multi-record mini batches</li> <li>Maximum timeout: 3600 seconds</li> <li>Maximum payload size: 100 MB per record</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Asynchronous Inference:</strong> <ul> <li>Queues requests for asynchronous processing</li> <li>Supports autoscaling to zero instances during idle periods</li> </ul> </li> <li style="color: #333; font-size: 16px;"><strong>Best Practices:</strong> <ul> <li>Choose the appropriate inference option based on specific use case requirements</li> <li>Leverage SageMaker Inference Recommender for optimal endpoint configurations</li> <li>Properly structure models when deploying to SageMaker</li> <li>Consider Serverless Inference for workloads with intermittent traffic</li> <li>Regularly review and optimize deployments for performance and cost-effectiveness</li> </ul> </li> </ol> <p style="color: #333; font-size: 16px;">This summary encapsulates the key aspects of Amazon SageMaker's model hosting capabilities, providing a comprehensive overview of the various deployment options, optimization strategies, and best practices for effectively managing machine learning models in production environments.</p>
		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>




<div class="container mt-5">
	<h3 class="text-primary h4">Services</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">
            
            
		</div>
	</div>
	
	<br/>
	
</div>





<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
