<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Machine Learning Engineer Associate (MLA)</h1>
  
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Domain 2: ML Model Development</h3>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<p></p>
	<div class="row">
		<div class="col-sm-12">

			<p style="color: blueviolet; font-size: 20px;"><stong>Task Statement 3.3: Use automated orchestration tools to set up continuous 
				integration and continuous delivery (CI/CD) pipelines. </stong></p>
			
			<p style="color: #0066cc;"><strong>Knowledge 1: Difference between on-demand and provisioned resources</strong></p> <p>On-demand and provisioned resources are two different approaches to allocating and managing cloud computing resources. Understanding their differences is crucial for optimizing cost and performance in cloud environments.</p> <ul> <li><strong>On-demand resources:</strong> <ul> <li>Allocated and billed based on actual usage</li> <li>Automatically scale up or down based on demand</li> <li>No upfront commitment or long-term contracts</li> <li>Ideal for unpredictable workloads or variable traffic</li> </ul> </li> <li><strong>Provisioned resources:</strong> <ul> <li>Pre-allocated and reserved for a specific period</li> <li>Fixed capacity regardless of actual usage</li> <li>Often require upfront commitment or long-term contracts</li> <li>Typically offer cost savings for predictable, steady-state workloads</li> </ul> </li> </ul> <p>Examples in AWS:</p> <ul> <li>On-demand: EC2 On-Demand Instances, DynamoDB On-Demand</li> <li>Provisioned: EC2 Reserved Instances, DynamoDB Provisioned Capacity</li> </ul> <p>When choosing between on-demand and provisioned resources, consider factors such as workload predictability, budget constraints, and performance requirements. On-demand resources offer flexibility but may be more expensive for constant usage, while provisioned resources can provide cost savings for stable, long-term workloads.</p> <p style="color: #0066cc;"><strong>Knowledge 2: How to compare scaling policies</strong></p> <p>Scaling policies determine how cloud resources adapt to changes in demand. Comparing scaling policies involves evaluating different approaches to ensure optimal performance and cost-efficiency.</p> <p>Common types of scaling policies:</p> <ul> <li><strong>Simple scaling:</strong> Adjusts capacity based on a single metric (e.g., CPU utilization)</li> <li><strong>Step scaling:</strong> Allows for more granular control by defining multiple step adjustments</li> <li><strong>Target tracking scaling:</strong> Automatically adjusts capacity to maintain a specific metric target</li> <li><strong>Scheduled scaling:</strong> Adjusts capacity based on predictable time-based patterns</li> </ul> <p>When comparing scaling policies, consider the following factors:</p> <ul> <li><strong>Responsiveness:</strong> How quickly does the policy react to changes in demand?</li> <li><strong>Precision:</strong> How accurately does the policy match capacity to actual needs?</li> <li><strong>Complexity:</strong> How difficult is it to set up and maintain the policy?</li> <li><strong>Cost-efficiency:</strong> How well does the policy optimize resource utilization and costs?</li> <li><strong>Workload characteristics:</strong> Does the policy align with the specific patterns and requirements of your application?</li> </ul> <p>Example comparison:</p> <ul> <li>For a web application with unpredictable traffic spikes, target tracking scaling might be more suitable than simple scaling, as it can maintain a specific metric (e.g., average CPU utilization) more precisely.</li> <li>For a batch processing job that runs nightly, scheduled scaling would be more appropriate than reactive policies like simple or step scaling.</li> </ul> <p style="color: #0066cc;"><strong>Knowledge 3: Tradeoffs and use cases of infrastructure as code (IaC) options (for example, AWS CloudFormation, AWS Cloud Development Kit [AWS CDK])</strong></p> <p>Infrastructure as Code (IaC) allows developers to manage and provision infrastructure using code and automation. AWS offers multiple IaC options, each with its own tradeoffs and use cases.</p> <p><strong>AWS CloudFormation:</strong></p> <ul> <li><strong>Tradeoffs:</strong> <ul> <li>Pros: Mature, widely adopted, extensive documentation</li> <li>Cons: YAML/JSON syntax can be verbose, limited programming constructs</li> </ul> </li> <li><strong>Use cases:</strong> <ul> <li>Large-scale infrastructure deployments</li> <li>Teams familiar with declarative languages</li> <li>Projects requiring strict version control of infrastructure</li> </ul> </li> </ul> <p><strong>AWS Cloud Development Kit (CDK):</strong></p> <ul> <li><strong>Tradeoffs:</strong> <ul> <li>Pros: Uses familiar programming languages (e.g., TypeScript, Python), enables code reuse</li> <li>Cons: Steeper learning curve, relatively newer with evolving best practices</li> </ul> </li> <li><strong>Use cases:</strong> <ul> <li>Development teams with strong programming skills</li> <li>Projects requiring complex logic or custom resource types</li> <li>Applications that benefit from integrating infrastructure and application code</li> </ul> </li> </ul> <p>When choosing between IaC options, consider factors such as:</p> <ul> <li>Team skills and preferences</li> <li>Project complexity and scale</li> <li>Need for custom logic or reusable components</li> <li>Integration with existing development workflows</li> </ul> <p>Example scenario: A startup with a small team of Python developers might prefer AWS CDK for its familiarity and ability to leverage existing coding skills. In contrast, a large enterprise with established CloudFormation templates and processes might stick with CloudFormation for consistency and to avoid retraining costs.</p>

			<p style="color: #0066cc;"><strong>Knowledge 4: Containerization concepts and AWS container services</strong></p> <p>Containerization is a lightweight virtualization technology that packages applications and their dependencies into isolated units called containers. Understanding containerization concepts and AWS container services is crucial for modern application development and deployment.</p> <p><strong>Key containerization concepts:</strong></p> <ul> <li><strong>Container:</strong> A standalone, executable package that includes everything needed to run a piece of software</li> <li><strong>Image:</strong> A lightweight, standalone, and executable software package that includes code, runtime, libraries, and settings</li> <li><strong>Dockerfile:</strong> A text file that contains instructions for building a Docker image</li> <li><strong>Container registry:</strong> A repository for storing and distributing container images</li> <li><strong>Orchestration:</strong> The process of automating the deployment, management, scaling, and networking of containers</li> </ul> <p><strong>AWS container services:</strong></p> <ul> <li><strong>Amazon Elastic Container Service (ECS):</strong> <ul> <li>Fully managed container orchestration service</li> <li>Supports both EC2 and Fargate launch types</li> <li>Ideal for applications that need deep integration with AWS services</li> </ul> </li> <li><strong>Amazon Elastic Kubernetes Service (EKS):</strong> <ul> <li>Managed Kubernetes service</li> <li>Provides compatibility with existing Kubernetes workloads</li> <li>Suitable for organizations already using Kubernetes or requiring multi-cloud support</li> </ul> </li> <li><strong>AWS Fargate:</strong> <ul> <li>Serverless compute engine for containers</li> <li>Works with both ECS and EKS</li> <li>Eliminates the need to manage underlying infrastructure</li> </ul> </li> <li><strong>Amazon Elastic Container Registry (ECR):</strong> <ul> <li>Fully managed container registry</li> <li>Integrates seamlessly with ECS and EKS</li> <li>Provides secure, scalable, and reliable storage for container images</li> </ul> </li> </ul> <p><strong>Benefits of containerization:</strong></p> <ul> <li>Consistency across development, testing, and production environments</li> <li>Improved resource utilization compared to traditional virtual machines</li> <li>Faster application deployment and scaling</li> <li>Enhanced portability across different computing environments</li> </ul> <p>Example use case: A microservices-based application could use ECS with Fargate for running containerized services, ECR for storing container images, and Application Load Balancer for routing traffic between services. This setup provides a scalable, manageable, and cost-effective solution for deploying and running microservices.</p> <p style="color: #0066cc;"><strong>Knowledge 5: How to use SageMaker endpoint auto scaling policies to meet scalability requirements (for example, based on demand, time)</strong></p> <p>Amazon SageMaker provides auto scaling capabilities for machine learning model endpoints, allowing them to automatically adjust the number of instances to handle varying workloads. Understanding how to use SageMaker endpoint auto scaling policies is essential for optimizing performance and cost in machine learning deployments.</p> <p><strong>Key concepts:</strong></p> <ul> <li><strong>Endpoint:</strong> A hosted model that can be invoked for real-time predictions</li> <li><strong>Auto scaling:</strong> Automatically adjusting the number of instances behind an endpoint based on traffic patterns</li> <li><strong>Scaling policy:</strong> A set of rules that determine when and how to scale the endpoint</li> </ul> <p><strong>Types of SageMaker auto scaling policies:</strong></p> <ul> <li><strong>Target tracking scaling:</strong> <ul> <li>Adjusts capacity to maintain a specific metric at a target value</li> <li>Common metrics: CPU utilization, memory utilization, invocations per instance</li> <li>Example: Maintain average CPU utilization at 70%</li> </ul> </li> <li><strong>Step scaling:</strong> <ul> <li>Allows for more granular control by defining multiple step adjustments</li> <li>Useful for complex scaling scenarios or when target tracking is not suitable</li> <li>Example: Add 2 instances when CPU utilization exceeds 70%, add 3 more when it exceeds 85%</li> </ul> </li> <li><strong>Scheduled scaling:</strong> <ul> <li>Adjusts capacity based on predictable time-based patterns</li> <li>Useful for known traffic patterns or batch processing jobs</li> <li>Example: Increase capacity during business hours and decrease during off-hours</li> </ul> </li> </ul> <p><strong>Implementing auto scaling for SageMaker endpoints:</strong></p> <ol> <li>Define the minimum and maximum number of instances for the endpoint</li> <li>Choose the appropriate scaling policy type based on your requirements</li> <li>Configure the scaling policy parameters (e.g., target value, cooldown periods)</li> <li>Apply the scaling policy to the SageMaker endpoint</li> <li>Monitor and adjust the policy as needed based on observed performance and costs</li> </ol> <p><strong>Best practices:</strong></p> <ul> <li>Start with conservative scaling thresholds and adjust based on observed behavior</li> <li>Use appropriate cooldown periods to prevent rapid scaling fluctuations</li> <li>Monitor endpoint metrics and logs to identify potential performance issues</li> <li>Consider using multiple scaling policies for complex workload patterns</li> <li>Regularly review and optimize your auto scaling configuration</li> </ul> <p>Example scenario: An e-commerce company uses a SageMaker endpoint for real-time product recommendations. They implement a target tracking scaling policy to maintain an average of 1000 invocations per instance, with a minimum of 2 instances and a maximum of 10. During holiday seasons, they add a scheduled scaling policy to proactively increase capacity during peak shopping hours.</p> <p>By effectively using SageMaker endpoint auto scaling policies, you can ensure that your machine learning models can handle varying workloads while optimizing costs and maintaining performance.</p>

			<p style="color: #0066cc;"><strong>Skill 1: Configuring and troubleshooting CodeBuild, CodeDeploy, and CodePipeline, including stages</strong></p> <p>This skill involves understanding and working with AWS CI/CD services to automate software delivery processes. Key components include:</p> <ul> <li><strong>AWS CodeBuild:</strong> A fully managed build service <ul> <li>Configuration: <ol> <li>Define build specifications in a buildspec.yml file</li> <li>Set up environment variables and build environment</li> <li>Configure source code location (e.g., GitHub, AWS CodeCommit)</li> <li>Specify build output artifacts and their destinations</li> </ol> </li> <li>Troubleshooting: <ul> <li>Review build logs in AWS CloudWatch</li> <li>Check buildspec.yml for syntax errors</li> <li>Verify IAM permissions for CodeBuild service role</li> </ul> </li> </ul> </li> <li><strong>AWS CodeDeploy:</strong> A service to automate application deployments <ul> <li>Configuration: <ol> <li>Create an application and deployment group</li> <li>Define deployment configuration (e.g., All at once, Blue/Green)</li> <li>Set up appspec.yml file to specify deployment actions</li> <li>Configure deployment triggers and alarms</li> </ol> </li> <li>Troubleshooting: <ul> <li>Review deployment logs on target instances</li> <li>Check instance health in deployment group</li> <li>Verify IAM roles for CodeDeploy and target instances</li> </ul> </li> </ul> </li> <li><strong>AWS CodePipeline:</strong> A continuous delivery service to model and visualize software release processes <ul> <li>Configuration: <ol> <li>Define pipeline stages (e.g., Source, Build, Test, Deploy)</li> <li>Configure actions within each stage</li> <li>Set up artifacts and their transitions between stages</li> <li>Integrate with other AWS services or third-party tools</li> </ol> </li> <li>Troubleshooting: <ul> <li>Review pipeline execution history</li> <li>Check action-specific error messages</li> <li>Verify IAM permissions for pipeline execution role</li> <li>Ensure proper configuration of integrated services</li> </ul> </li> </ul> </li> </ul> <p>Example procedure for setting up a basic CodePipeline:</p> <ol> <li>Create a pipeline and specify source provider (e.g., CodeCommit)</li> <li>Add a build stage using CodeBuild</li> <li>Configure a deployment stage with CodeDeploy</li> <li>Review and create the pipeline</li> <li>Test the pipeline by making a change in the source repository</li> </ol> <p style="color: #0066cc;"><strong>Skill 2: Applying continuous deployment flow structures to invoke pipelines (for example, Gitflow, GitHub Flow)</strong></p> <p>This skill focuses on implementing version control workflows to trigger and manage continuous deployment pipelines. Key concepts include:</p> <ul> <li><strong>Gitflow:</strong> A branching model for Git <ul> <li>Main branches: master (production) and develop (integration)</li> <li>Supporting branches: feature, release, and hotfix</li> <li>Pipeline invocation: <ol> <li>Merge feature branches into develop to trigger integration tests</li> <li>Create release branches to prepare for deployment</li> <li>Merge release branches into master to trigger production deployment</li> </ol> </li> </ul> </li> <li><strong>GitHub Flow:</strong> A simpler, feature-branch workflow <ul> <li>Single main branch (usually called main or master)</li> <li>Feature branches created for new work</li> <li>Pipeline invocation: <ol> <li>Open pull requests to trigger integration tests</li> <li>Merge pull requests into main branch to trigger deployment</li> </ol> </li> </ul> </li> </ul> <p>Implementing continuous deployment flows:</p> <ol> <li>Choose a workflow that fits your team's needs (e.g., Gitflow for larger projects, GitHub Flow for smaller teams)</li> <li>Set up branch protection rules in your repository</li> <li>Configure AWS CodePipeline to monitor specific branches or events</li> <li>Use AWS CodeBuild to run tests on pull requests or branch merges</li> <li>Implement AWS CodeDeploy stages for different environments (e.g., staging, production)</li> </ol> <p>Example: Implementing GitHub Flow with AWS services</p> <ol> <li>Developers create feature branches and work on changes</li> <li>Open pull request triggers CodePipeline</li> <li>CodeBuild runs unit and integration tests</li> <li>Upon successful tests and code review, merge pull request</li> <li>Merging to main branch triggers another CodePipeline execution</li> <li>CodeDeploy deploys the changes to production environment</li> </ol> <p style="color: #0066cc;"><strong>Skill 3: Using AWS services to automate orchestration (for example, to deploy ML models, automate model building)</strong></p> <p>This skill involves leveraging various AWS services to create automated workflows for machine learning operations. Key services and concepts include:</p> <ul> <li><strong>Amazon SageMaker:</strong> Fully managed machine learning platform <ul> <li>SageMaker Pipelines: Automate and manage ML workflows</li> <li>SageMaker Model Registry: Version and manage ML models</li> <li>SageMaker Projects: Implement MLOps practices</li> </ul> </li> <li><strong>AWS Step Functions:</strong> Coordinate multiple AWS services into serverless workflows <ul> <li>Create state machines to orchestrate ML pipelines</li> <li>Integrate with SageMaker for model training and deployment</li> </ul> </li> <li><strong>AWS Lambda:</strong> Run code without provisioning servers <ul> <li>Trigger ML workflows based on events</li> <li>Perform data preprocessing or postprocessing tasks</li> </ul> </li> <li><strong>Amazon EventBridge:</strong> Serverless event bus service <ul> <li>Create rules to trigger ML pipelines based on events</li> <li>Integrate with other AWS services to create event-driven architectures</li> </ul> </li> </ul> <p>Automating ML model deployment and building:</p> <ol> <li>Use SageMaker Pipelines to define ML workflows: <ul> <li>Data preprocessing</li> <li>Model training</li> <li>Model evaluation</li> <li>Model registration</li> </ul> </li> <li>Implement a CI/CD pipeline using AWS CodePipeline: <ul> <li>Source control integration (e.g., CodeCommit)</li> <li>Build stage for preparing training data and scripts</li> <li>SageMaker Pipeline execution stage</li> <li>Model deployment stage using SageMaker endpoints</li> </ul> </li> <li>Use Step Functions to orchestrate complex ML workflows: <ul> <li>Coordinate data processing with AWS Glue</li> <li>Trigger SageMaker training jobs</li> <li>Evaluate model performance</li> <li>Conditionally deploy models based on evaluation metrics</li> </ul> </li> <li>Set up EventBridge rules to trigger ML pipelines: <ul> <li>Schedule regular model retraining</li> <li>Initiate inference jobs based on new data arrivals</li> </ul> </li> </ol> <p>Example: Automated ML model building and deployment workflow</p> <ol> <li>New data arrives in S3 bucket, triggering an EventBridge rule</li> <li>EventBridge rule starts a Step Functions state machine</li> <li>Step Functions orchestrates: <ul> <li>Data preprocessing using AWS Glue</li> <li>Model training using SageMaker</li> <li>Model evaluation using Lambda function</li> <li>Model registration in SageMaker Model Registry</li> <li>Model deployment to SageMaker endpoint if evaluation criteria are met</li> </ul> </li> <li>CodePipeline monitors the Model Registry for new approved models</li> <li>Upon approval, CodePipeline triggers deployment to production endpoint</li> </ol>

			<p style="color: #0066cc;"><strong>Skill 4: Configuring training and inference jobs (for example, by using Amazon EventBridge rules, SageMaker Pipelines, CodePipeline)</strong></p> <p>This skill involves setting up automated processes for training machine learning models and running inference jobs. Key components include:</p> <ul> <li><strong>Amazon EventBridge rules:</strong> <ul> <li>Create rules to trigger training or inference jobs based on events or schedules</li> <li>Example: Schedule a daily retraining job at a specific time</li> </ul> </li> <li><strong>SageMaker Pipelines:</strong> <ul> <li>Define end-to-end ML workflows including data preparation, training, and model evaluation</li> <li>Automate the execution of these workflows</li> </ul> </li> <li><strong>AWS CodePipeline:</strong> <ul> <li>Orchestrate the entire ML lifecycle, including source control, build, and deployment</li> <li>Integrate with SageMaker for model training and deployment stages</li> </ul> </li> </ul> <p>Configuring training jobs:</p> <ol> <li>Prepare training data and store it in Amazon S3</li> <li>Create a SageMaker training job configuration: <ul> <li>Specify algorithm or custom training script</li> <li>Set hyperparameters</li> <li>Define input data channels</li> <li>Configure compute resources (instance type and count)</li> </ul> </li> <li>Set up EventBridge rule to trigger training job: <ul> <li>Define event pattern (e.g., S3 object creation) or schedule</li> <li>Set target as AWS Lambda function or Step Functions state machine</li> </ul> </li> <li>Implement SageMaker Pipeline for end-to-end training workflow: <ul> <li>Include data preprocessing, training, and evaluation steps</li> <li>Define pipeline parameters for flexibility</li> </ul> </li> </ol> <p>Configuring inference jobs:</p> <ol> <li>Create SageMaker model using trained model artifacts</li> <li>Configure SageMaker endpoint: <ul> <li>Specify model version</li> <li>Set instance type and count</li> <li>Enable auto-scaling if needed</li> </ul> </li> <li>Set up batch transform job for offline inference: <ul> <li>Define input and output S3 locations</li> <li>Configure instance type and count</li> </ul> </li> <li>Use CodePipeline to automate model deployment: <ul> <li>Include source stage (e.g., model artifacts in S3)</li> <li>Add deployment stage using SageMaker deployment action</li> </ul> </li> </ol> <p>Example: Automated training and deployment pipeline</p> <ol> <li>EventBridge rule detects new training data in S3</li> <li>Rule triggers SageMaker Pipeline execution</li> <li>Pipeline performs data preprocessing, model training, and evaluation</li> <li>If model meets performance criteria, it's registered in Model Registry</li> <li>CodePipeline detects new model version and initiates deployment</li> <li>Deployment stage updates SageMaker endpoint with new model</li> </ol> <p style="color: #0066cc;"><strong>Skill 5: Creating automated tests in CI/CD pipelines (for example, integration tests, unit tests, end-to-end tests)</strong></p> <p>This skill focuses on implementing various types of automated tests within CI/CD pipelines to ensure code quality and reliability. Key concepts include:</p> <ul> <li><strong>Unit tests:</strong> Test individual components or functions <ul> <li>Fast execution, typically run on every commit</li> <li>Use frameworks like pytest (Python) or JUnit (Java)</li> </ul> </li> <li><strong>Integration tests:</strong> Verify interactions between different parts of the application <ul> <li>Test API endpoints, database interactions, etc.</li> <li>May require mock services or test databases</li> </ul> </li> <li><strong>End-to-end tests:</strong> Test the entire application flow <ul> <li>Simulate real user scenarios</li> <li>Often involve UI testing (e.g., using Selenium)</li> </ul> </li> </ul> <p>Implementing automated tests in CI/CD pipelines:</p> <ol> <li>Set up test environment: <ul> <li>Configure test databases or mock services</li> <li>Prepare test data sets</li> </ul> </li> <li>Write tests: <ul> <li>Implement unit tests for individual functions</li> <li>Create integration tests for API endpoints</li> <li>Develop end-to-end test scripts</li> </ul> </li> <li>Configure AWS CodeBuild project: <ul> <li>Specify build environment (e.g., Python, Node.js)</li> <li>Define build commands to run tests</li> <li>Set up test result reporting</li> </ul> </li> <li>Integrate tests in AWS CodePipeline: <ul> <li>Add test stage after the build stage</li> <li>Configure CodeBuild action to run tests</li> <li>Set up notifications for test failures</li> </ul> </li> <li>Implement quality gates: <ul> <li>Define minimum test coverage requirements</li> <li>Set up approval stages for manual review if needed</li> </ul> </li> </ol> <p>Example: Multi-stage test pipeline in CodePipeline</p> <ol> <li>Source stage: Retrieve code from CodeCommit</li> <li>Build stage: Compile code and prepare artifacts</li> <li>Unit Test stage: Run fast, isolated tests</li> <li>Integration Test stage: Test API endpoints and database interactions</li> <li>End-to-End Test stage: Run UI tests and full application scenarios</li> <li>Deploy to Staging stage: Deploy to staging environment if all tests pass</li> <li>Manual Approval stage: Require human approval for production deployment</li> <li>Deploy to Production stage: Deploy to production if approved</li> </ol> <p style="color: #0066cc;"><strong>Skill 6: Building and integrating mechanisms to retrain models</strong></p> <p>This skill involves creating systems to automatically update machine learning models with new data, ensuring they remain accurate and relevant over time. Key components include:</p> <ul> <li><strong>Data collection and preprocessing:</strong> Gather and prepare new training data</li> <li><strong>Model performance monitoring:</strong> Track model accuracy and detect drift</li> <li><strong>Automated retraining:</strong> Trigger and execute model retraining processes</li> <li><strong>Model versioning and deployment:</strong> Manage multiple model versions and update production endpoints</li> </ul> <p>Implementing model retraining mechanisms:</p> <ol> <li>Set up data collection pipeline: <ul> <li>Use Amazon Kinesis for real-time data streaming</li> <li>Store new data in Amazon S3 data lake</li> </ul> </li> <li>Implement model monitoring: <ul> <li>Use Amazon SageMaker Model Monitor to detect data drift</li> <li>Set up CloudWatch alarms for performance metrics</li> </ul> </li> <li>Create automated retraining workflow: <ul> <li>Use Step Functions to orchestrate the retraining process</li> <li>Implement data preprocessing and feature engineering steps</li> <li>Configure SageMaker training job with updated dataset</li> </ul> </li> <li>Set up model evaluation: <ul> <li>Compare new model performance against baseline</li> <li>Use Lambda function to decide whether to deploy new model</li> </ul> </li> <li>Implement model versioning and deployment: <ul> <li>Register new model version in SageMaker Model Registry</li> <li>Use CodePipeline to automate deployment of approved models</li> </ul> </li> <li>Configure retraining triggers: <ul> <li>Set up EventBridge rules for scheduled retraining</li> <li>Create triggers based on model performance thresholds</li> </ul> </li> </ol> <p>Example: End-to-end model retraining and deployment workflow</p> <ol> <li>New data continuously streams into S3 via Kinesis Firehose</li> <li>SageMaker Model Monitor detects data drift and triggers CloudWatch alarm</li> <li>CloudWatch alarm initiates Step Functions state machine</li> <li>Step Functions orchestrates: <ul> <li>Data preprocessing using Glue ETL job</li> <li>Feature engineering using SageMaker Processing</li> <li>Model training using SageMaker training job</li> <li>Model evaluation comparing new model to current production model</li> </ul> </li> <li>If new model performs better, it's registered in Model Registry</li> <li>CodePipeline detects new approved model and initiates deployment</li> <li>New model is deployed to production SageMaker endpoint with traffic shifting</li> <li>CloudWatch alarms monitor new model performance in production</li> </ol> <p>By implementing these mechanisms, you ensure that your ML models stay up-to-date and continue to provide accurate predictions as new data becomes available or as the underlying patterns in the data change over time.</p>

		</div>
	</div>

    <hr/>

	<div class="row">
		<div class="col-sm-12">
			Topic-1: Difference between on-demand and provisioned resources

			<p style="color: goldenrod; font-size:14px;"><strong>On-demand vs Provisioned Resources</strong></p> <p>Understanding the difference between on-demand and provisioned resources is crucial for optimizing cost and performance in cloud environments:</p> <ul> <li><strong>On-demand resources:</strong> <ul> <li>Allocated and billed based on actual usage</li> <li>Automatically scale up or down based on demand</li> <li>No upfront commitment or long-term contracts</li> <li>Ideal for unpredictable workloads or variable traffic</li> </ul> </li> <li><strong>Provisioned resources:</strong> <ul> <li>Pre-allocated and reserved for a specific period</li> <li>Fixed capacity regardless of actual usage</li> <li>Often require upfront commitment or long-term contracts</li> <li>Typically offer cost savings for predictable, steady-state workloads</li> </ul> </li> </ul> <p>Examples in AWS:</p> <ul> <li>On-demand: EC2 On-Demand Instances, DynamoDB On-Demand</li> <li>Provisioned: EC2 Reserved Instances, DynamoDB Provisioned Capacity</li> </ul> <p style="color: #FF6347;"><strong>Gotchas and Insights:</strong></p> <ul> <li>On-demand resources may be more expensive for constant, predictable usage</li> <li>Provisioned resources can lead to over-provisioning and wasted resources if not properly planned</li> <li>Some services, like Amazon SageMaker, offer both on-demand (Serverless Inference) and provisioned (Endpoints) options for model hosting</li> </ul>
			Topic-2: How to compare scaling policies

			<p style="color: goldenrod; font-size:14px;"><strong>Comparing Scaling Policies</strong></p> <p>SageMaker hosting service offers multiple auto scaling options to ensure your model is scalable and cost-efficient:</p> <ul> <li><strong>Target tracking scaling:</strong> <ul> <li>Select a target value for a CloudWatch metric (e.g., average CPU utilization)</li> <li>SageMaker automatically scales in or out to achieve the target metric</li> </ul> </li> <li><strong>Step scaling:</strong> <ul> <li>Create and manage CloudWatch alarms to initiate scaling</li> <li>Define specific steps for scaling based on alarm thresholds</li> </ul> </li> <li><strong>Scheduled scaling:</strong> <ul> <li>Specify recurring schedules for scaling based on anticipated demand</li> <li>Useful for predictable traffic patterns</li> </ul> </li> </ul> <p>Best practices for comparing and implementing scaling policies:</p> <ul> <li>Combine multiple scaling options for better resilience</li> <li>Consider scaling up/down vs. scaling in/out based on traffic needs</li> <li>Use serverless inference for unpredictable traffic patterns</li> <li>Monitor key metrics: model latency, CPU utilization, invocations per instance</li> </ul> <p style="color: #FF6347;"><strong>Gotchas and Insights:</strong></p> <ul> <li>Scaling an existing endpoint requires creating a new production variant and shifting traffic</li> <li>AWS Inferentia instances can improve inference performance but may not handle sudden load surges</li> <li>SageMaker multi-model endpoints support automatic scaling for managing multiple model replicas</li> </ul>
			Topic-3: Tradeoffs and use cases of infrastructure as code (IaC) options

			<p style="color: goldenrod; font-size:14px;"><strong>Infrastructure as Code (IaC) Options</strong></p> <p>IaC tools help automate the provisioning of resources, improving maintainability and scalability:</p> <ul> <li><strong>AWS CloudFormation:</strong> <ul> <li>Uses templates to specify infrastructure definitions and configurations</li> <li>Supports version control by committing files to source code repositories</li> </ul> </li> <li><strong>AWS Cloud Development Kit (CDK):</strong> <ul> <li>Allows defining infrastructure using familiar programming languages</li> <li>Enables repeatable deployments and easier integration with existing codebases</li> </ul> </li> <li><strong>AWS Serverless Application Model (SAM):</strong> <ul> <li>Specifically designed for serverless applications</li> <li>Simplifies the deployment of Lambda functions, Step Functions, and DynamoDB tables</li> </ul> </li> </ul> <p>Use cases and considerations:</p> <ul> <li>Use CloudFormation for complex, multi-resource deployments</li> <li>Choose CDK when working with development teams familiar with specific programming languages</li> <li>Opt for SAM when focusing on serverless architectures</li> </ul> <p style="color: #FF6347;"><strong>Gotchas and Insights:</strong></p> <ul> <li>CloudFormation templates can become complex for large infrastructures</li> <li>CDK requires knowledge of both IaC concepts and programming languages</li> <li>SAM is limited to serverless resources and may not be suitable for all use cases</li> <li>Consider using CI/CD pipelines to automate the deployment of IaC templates</li> </ul>

			Topic-4: Containerization concepts and AWS container services

			<p style="color: goldenrod; font-size:14px;"><strong>Containerization and AWS Container Services</strong></p> <p>Containerization is a key concept in modern application development and deployment, especially in machine learning workflows:</p> <ul> <li><strong>Containerization basics:</strong> <ul> <li>Containers package applications and dependencies into isolated units</li> <li>Provide consistency across development, testing, and production environments</li> <li>Enable faster deployment and scaling of applications</li> </ul> </li> <li><strong>AWS container services:</strong> <ul> <li>Amazon Elastic Container Service (ECS): Fully managed container orchestration service</li> <li>Amazon Elastic Kubernetes Service (EKS): Managed Kubernetes service</li> <li>AWS Fargate: Serverless compute engine for containers</li> <li>Amazon Elastic Container Registry (ECR): Fully managed container registry</li> </ul> </li> </ul> <p>SageMaker and containers:</p> <ul> <li>SageMaker uses containers for training and hosting models</li> <li>Supports both pre-built and custom containers</li> <li>Enables portability and reproducibility of ML workflows</li> </ul> <p style="color: #FF6347;"><strong>Gotchas and Insights:</strong></p> <ul> <li>Container images can become large, impacting deployment times and resource usage</li> <li>Managing container orchestration can be complex, especially for large-scale deployments</li> <li>Consider using AWS Fargate for serverless container management to reduce operational overhead</li> <li>Ensure proper security practices when working with containers, such as scanning for vulnerabilities</li> </ul>
			Topic-5: How to use SageMaker endpoint auto scaling policies to meet scalability requirements

			<p style="color: goldenrod; font-size:14px;"><strong>SageMaker Endpoint Auto Scaling Policies</strong></p> <p>SageMaker provides various auto scaling options to ensure your model endpoints can handle varying workloads efficiently:</p> <ul> <li><strong>Target tracking scaling:</strong> <ul> <li>Set a target value for a specific metric (e.g., CPU utilization, model latency)</li> <li>SageMaker automatically adjusts the number of instances to maintain the target</li> </ul> </li> <li><strong>Step scaling:</strong> <ul> <li>Define specific scaling actions based on CloudWatch alarm thresholds</li> <li>Allows for more granular control over scaling behavior</li> </ul> </li> <li><strong>Scheduled scaling:</strong> <ul> <li>Set up recurring schedules for scaling based on known traffic patterns</li> <li>Useful for predictable workloads with time-based variations</li> </ul> </li> </ul> <p>Implementing auto scaling for SageMaker endpoints:</p> <ul> <li>Define minimum and maximum instance counts</li> <li>Choose appropriate scaling metrics (e.g., InvocationsPerInstance, CPUUtilization)</li> <li>Configure cooldown periods to prevent rapid scaling fluctuations</li> <li>Use Application Auto Scaling API or AWS Management Console to set up policies</li> </ul> <p style="color: #FF6347;"><strong>Gotchas and Insights:</strong></p> <ul> <li>Auto scaling doesn't apply to all instance types; check compatibility before implementation</li> <li>Scaling operations can take several minutes, so plan for potential latency during traffic spikes</li> <li>Consider using SageMaker Serverless Inference for highly variable or unpredictable workloads</li> <li>Multi-model endpoints require special consideration for auto scaling, as they host multiple models on a single endpoint</li> </ul> <p>Additional considerations for SageMaker scalability:</p> <ul> <li><strong>VPC connectivity:</strong> Use VPC interface endpoints for secure connections between your VPC and SageMaker API or Runtime</li> <li><strong>Security:</strong> Implement proper IAM roles and policies for SageMaker resources</li> <li><strong>Monitoring:</strong> Utilize CloudWatch metrics and alarms to track endpoint performance and trigger scaling actions</li> <li><strong>Cost optimization:</strong> Use managed spot instances for training to reduce costs, and consider the trade-offs between different instance types for inference</li> </ul> <p style="color: #FF6347;"><strong>Exam Tips:</strong></p> <ul> <li>Understand the differences between scaling options and when to use each</li> <li>Be familiar with key metrics for monitoring and scaling SageMaker endpoints</li> <li>Know how to implement secure connections for SageMaker resources within a VPC</li> <li>Recognize the importance of combining different scaling strategies for optimal performance and cost-efficiency</li> </ul>


		</div>
	</div>

    <hr/>

	<div class="row">
		<div class="col-sm-12">
            
        </div>
	</div>

	<div class="row">
		<div class="col-sm-12">
			
        </div>
	</div>

	<div class="row">
		<div class="col-sm-12">
            
        </div>
	</div>

	<div class="row">
		<div class="col-sm-12">
            
        </div>
	</div>

	<div class="row">
		<div class="col-sm-12">
            
		</div>
	</div>

    
	<div class="row">
		<div class="col-sm-12">
			
        </div>
	</div>

	<div class="row">
		<div class="col-sm-12">
			
		</div>
	</div>

	<div class="row">
		<div class="col-sm-12">
			
		</div>
	</div>

	<div class="row">
		<div class="col-sm-12">
			
		</div>
	</div>

	<div class="row">
		<div class="col-sm-12">
			
		</div>
	</div>
	<br/>

	<div class="row">
		<div class="col-sm-12">

        </div>
	</div>

	<div class="row">
		<div class="col-sm-12">

        </div>
	</div>

	<div class="row">
		<div class="col-sm-12">
			
		</div>
	</div>

	<div class="row">
		<div class="col-sm-12">
			
		</div>
	</div>

	<div class="row">
		<div class="col-sm-12">
			
		</div>
	</div>
	<br/>
	
</div>


<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
