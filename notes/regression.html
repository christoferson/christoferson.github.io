<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta charset="utf-8">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	
	<!-- Open Graph / Facebook -->
	<meta property="og:type" content="website">
	<meta property="og:locale" content="en_US">
	<meta property="og:url" content="https://christoferson.github.io/">
	<meta property="og:site_name" content="christoferson.github.io">
	<meta property="og:title" content="Meta Tags Preview, Edit and Generate">
	<meta property="og:description" content="Christoferson Chua GitHub Page">

	<!-- Twitter -->
	<meta property="twitter:card" content="summary_large_image">
	<meta property="twitter:url" content="https://christoferson.github.io/">
	<meta property="twitter:title" content="christoferson.github.io">
	<meta property="twitter:description" content="Christoferson Chua GitHub Page">
	
	<script type="application/ld+json">{
		"name": "christoferson.github.io",
		"description": "Machine Learning",
		"url": "https://christoferson.github.io/",
		"@type": "WebSite",
		"headline": "christoferson.github.io",
		"@context": "https://schema.org"
	}</script>
	
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" />
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js"></script>
  
	<title>Christoferson Chua</title>
	<meta name="title" content="Christoferson Chua | GitHub Page | Machine Learning">
	<meta name="description" content="Christoferson Chua GitHub Page - Machine Learning">
	<meta name="keywords" content="Backend,Java,Spring,Aws,Python,Machine Learning">
	
	<link rel="stylesheet" href="style.css">
	
</head>
<body>

<div class="container-fluid p-5 bg-primary text-white text-center">
  <h1>Regression - Introduction</h1>
  
</div>


<div class="container mt-5">
	<h3 class="text-primary h4">Linear Regression</h4>
	<!--<p class="lh-1" style="color:#BC8E19;">Software Developer | Cloud Architect</p>-->
	<div class="row">
		<div class="col-sm-12">
            <p>
                Linear regression is a fundamental statistical and machine learning technique used to model the relationship between a dependent variable (often called the target or outcome variable) and one or more independent variables (also known as predictors or features). The goal is to find a linear equation that best fits the data points.
            </p>
            <p>
                We predict the value of 1 variable y based on another variable x.
                x is called the independent variable and y is called the dependent variable.
                Used in various fields for prediction, forecasting, and understanding relationships between variables.
                Common metrics include R-squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE)
            </p>
		</div>
	</div>
    <br/>
	<div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Simple linear regression</p>
        <p style="color:lightseagreen;font-style: italic;font-weight: bold;">y = mx + b</p>
        <p style="color:lightseagreen;font-style: italic;font-weight: bold;">y = b0 + b1x1 + b2x2 + ... + bnxn</p>
        <ul>
        <li>y is the dependent variable</li>
        <li>x is the independent variable</li>
        <li>m is the slope (coefficient)</li>
        <li>b is the y-intercept</li>
        </ul>
		</div>
	</div>
    <br/>
	<div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Least sum of squares</p>
        <p>
            Also known as the method of least squares or ordinary least squares (OLS), is a mathematical approach used in regression analysis to find the best-fitting line or curve for a set of data points. 
            Residuals help assess how well a model fits the data. They indicate the amount of unexplained variance in the model.
            Residuals are the vertical distances between the observed data points and the fitted regression line.
            The sum of squared residuals is a measure of the overall model fit.
            Minimizing RSS is the goal of the least squares method in regression.

        </p>

		</div>
	</div>
    <br/>
	<div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Residual</p>
        <p>
            A residual is a key concept in statistics and regression analysis. It represents the difference between an observed value and the predicted value from a statistical model.
        </p>
        <p>Residual = Observed Value - Predicted Value or eᵢ = yᵢ - ŷᵢ where eᵢ is the residual, yᵢ is the observed value, and ŷᵢ is the predicted value.</p>
		</div>
	</div>
	<div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Mean Absolute Error</p>
        <p>
            A common metric used in statistics and machine learning to evaluate the accuracy of predictions, especially in regression problems. It measures the average magnitude of errors in a set of predictions, without considering their direction (positive or negative).
            <ul>
            <li>MAE is the average of the absolute differences between predicted values and actual observed values.</li>
            <li>MAE = (1/n) * Σ|yi - ŷi| Where: n = number of observations yi = actual value ŷi = predicted value</li>
            <li>A lower MAE indicates better model performance. It represents the average error in the same units as the original data.</li>
            </ul>
        </p>
        <p>
            Limitations:
            <ul>
            <li>Doesn't indicate the direction of errors (over-prediction or under-prediction)</li>
            <li>May not be suitable for applications where larger errors should be penalized more heavily</li>
            </ul>
        </p>
		</div>
	</div>

	<div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Mean Square Error</p>
        <p>
            A commonly used metric in statistics and machine learning for evaluating the accuracy of predictions, particularly in regression problems. It measures the average of the squares of the errors between predicted and actual values.
            <ul>
            <li>MSE is the average of the squared differences between predicted values and actual observed values.</li>
            <li>MSE = (1/n) * Σ(yi - ŷi)² Where: n = number of observations yi = actual value ŷi = predicted value</li>
            <li>A lower MSE indicates better model performance. The square root of MSE (RMSE) is often used to express the error in the same units as the original data.</li>
            </ul>
        </p>
        <p>
            Limitations:
            <ul>
            <li>Not as easily interpretable as MAE in terms of original units</li>
            <li>Can be disproportionately influenced by outliers</li>
            <li>Doesn't indicate the direction of errors</li>
            </ul>
        </p>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Root Mean Square Error</p>
        <p>
            A frequently used measure of the differences between predicted values and observed values in various fields, including statistics, machine learning, and forecasting. It's essentially the square root of the Mean Square Error (MSE)
            <ul>
            <li>RMSE is the square root of the average of squared differences between prediction and actual observation.</li>
            <li>RMSE = √[(1/n) * Σ(yi - ŷi)²] Where: n = number of observations yi = actual value ŷi = predicted value</li>
            <li>Lower RMSE values indicate better fit. RMSE is in the same units as the response variable, making it easy to interpret.</li>
            </ul>
        </p>
        <p>
            Limitations:
            <ul>
            <li>Can be disproportionately influenced by outliers</li>
            <li>Doesn't indicate the direction of errors (over-prediction or under-prediction)</li>
            </ul>
        </p>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Mean Absolute Percentage Error</p>
        <p>
            A popular measure of prediction accuracy in statistics, particularly in trend estimation and forecasting. It expresses accuracy as a percentage and is especially useful when you want to compare forecast performance across different datasets or models.
            <ul>
            <li>MAPE is the average of the absolute percentage errors of forecasts.</li>
            <li> MAPE = (1/n) * Σ|(Actual - Forecast) / Actual| * 100 Where: n = number of observations</li>
            <li>Lower MAPE values indicate better accuracy. It's expressed as a percentage, with 0% indicating a perfect forecast.</li>
            </ul>
        </p>
        <p>
            Limitations:
            <ul>
            <li>Cannot be used when actual values are zero</li>
            <li>Biased towards forecasts that are too low</li>
            <li>Assumes a meaningful zero point (not suitable for interval scales)</li>
            <li>Can produce infinite or undefined values if actual values are very close to zero</li>
            </ul>
        </p>
		</div>
	</div>


    <div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Mean Percentage Error</p>
        <p>
            A statistical measure used to calculate the average of percentage errors in a series of predictions or forecasts. It's similar to Mean Absolute Percentage Error (MAPE), but with a key difference: MPE takes into account the direction of the error (whether the forecast is too high or too low).
            <ul>
            <li> MPE is the average of all percentage errors in a given set of predictions.</li>
            <li>MPE = (1/n) * Σ[(Actual - Forecast) / Actual] * 100 Where: n = number of observations</li>
            <li>A negative MPE indicates that the forecasts are, on average, too high.</li>
            <li>A positive MPE indicates that the forecasts are, on average, too low.</li>
            <li>An MPE of 0% indicates unbiased forecasting.</li>
            </ul>
        </p>
        <p>
            Limitations:
            <ul>
            <li>Can be misleading if positive and negative errors cancel each other out</li>
            <li>Cannot be used when actual values are zero</li>
            <li>Sensitive to outliers and extreme values</li>
            <li>Assumes a meaningful zero point (not suitable for interval scales)</li>
            </ul>
        </p>
		</div>
	</div>


    <div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Coefficient of Determination</p>
        <p>
            Commonly known as R-squared (R²), is a statistical measure used in regression analysis to assess how well a model explains and predicts future outcomes. It provides a measure of the goodness of fit of a model.
            <ul>
            <li>R² represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).</li>
            <li>R² = 1 - (Sum of Squared Residuals / Total Sum of Squares) Or: R² = 1 - (Unexplained Variation / Total Variation)</li>
            <li>R² values range from 0 to 1 (or 0% to 100%)</li>
            <li>An R² of 0.7 means that 70% of the variance in the dependent variable is predictable from the independent variable(s).</li>
            <li>The closer R² is to 1, the better the model fits the data.</li>
            </ul>
        </p>
        <p>
            Limitations:
            <ul>
            <li>Does not indicate whether a regression model is adequate</li>
            <li>Can increase with the addition of variables, even if they're not meaningful (overfitting)</li>
            <li>Doesn't provide information about the bias of the model predictions</li>
            </ul>
        </p>
		</div>
	</div>


    <div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">Adjusted Coefficient of Determination</p>
        <p>
            Commonly known as Adjusted R-squared (Adjusted R²), is a modified version of the regular R-squared statistic that takes into account the number of predictors in a model. It's particularly useful in multiple regression analysis and addresses some limitations of the standard R-squared.
            <ul>
            <li>Adjusted R² modifies the R-squared to account for the number of predictors in the model..</li>
            <li>Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)] Where: R² = regular R-squared n = number of observations k = number of predictors</li>
            <li>Like R², it ranges from 0 to 1, but can be negative in poorly fitting models.</li>
            <li>Higher values indicate a better fit, adjusting for model complexity.</li>
            <li>Can be used to compare models with different numbers of predictors.</li>
            </ul>
        </p>
        <p>
            Limitations:
            <ul>
            <li>Still doesn't indicate whether the model's predictors are actually causing the variation in the dependent variable.</li>
            <li>Can still increase with the addition of irrelevant variables, just not as readily as R².</li>
            </ul>
        </p>
		</div>
	</div>


    <div class="row">
		<div class="col-sm-12">
        <p style="color:blue;">One Hot Encoding</p>
        <p>
            One-hot encoding is a technique used to convert categorical data into a format that can be provided to machine learning algorithms to perform better predictive modeling. Many machine learning algorithms require numerical input data, so we need to encode categorical data into numbers before feeding the data to the algorithms.
        </p>
        <p>
            In one-hot encoding, each categorical value is represented as a binary vector that is as long as the number of unique categories, with a 1 in the position of the value's category, and 0 elsewhere. This means that each categorical value is represented by a separate binary column.
        </p>
        <p>
            For example, let's say we have a categorical feature called "Color" with three possible values: "Red", "Green", and "Blue". With one-hot encoding, this feature would be transformed into three separate binary columns:
            <table style='border:1px solid blue;margin:2px;padding:2px;'>
                <tr><th width="100">Color</th><th width="50">Red</th><th width="50">Green</th><th width="50">Blue</th></tr>
                <tr><td>Red</td><td>1</td><td>0</td><td>0</td></tr>
                <tr><td>Green</td><td>0</td><td>1</td><td>0</td></tr>
                <tr><td>Blue </td><td>0</td><td>0</td><td>1</td></tr>
            </table>
        </p>
        <p>
            One-hot encoding is particularly useful when dealing with categorical variables that do not have an inherent ordering or numerical meaning. It allows the machine learning algorithms to treat each category as a separate feature, without assuming any numerical relationship between them.
        </p>
        <p>
            However, one-hot encoding can significantly increase the dimensionality of the data, especially when dealing with high-cardinality categorical features (features with many unique values). In such cases, other encoding techniques like label encoding or target encoding might be more appropriate.
        </p>
        <p>
            Overall, one-hot encoding is a widely used technique for encoding categorical data in machine learning, as it provides a straightforward way to represent categorical information in a numerical format that can be easily processed by most algorithms.
        </p>
        </div>
	</div>
    <!-- -->



<br/>
<br/>
<footer class="_fixed-bottom">
<div class="container-fluid p-2 bg-primary text-white text-center">
  <h6>christoferson.github.io 2023</h6>
  <!--<div style="font-size:8px;text-decoration:italic;">about</div>-->
</div>
</footer>

</body>
</html>
